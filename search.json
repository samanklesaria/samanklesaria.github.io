[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam’s Blog",
    "section": "",
    "text": "Air Quality and Congestion Pricing\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nDec 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnkiVec: Vector Search for Anki\n\n\n\ntools\n\n\n\n\n\n\n\n\n\nNov 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Ships with Gaussian Process Mixtures\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nNov 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Coffee Yields\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nOct 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSizecheck: Making Tensor Code Self-Documenting with Runtime Shape Validation\n\n\n\ntools\n\n\n\n\n\n\n\n\n\nAug 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Sample Size Estimation\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSo You Want to Learn [X]\n\n\n\nstatistics\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAn Opinionated Tooling Guide\n\n\n\ntools\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Power Analysis for A/B/n Tests\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Common Topics\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Controls for Texas Prison Data\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHop Lists\n\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGraph SLAM\n\n\n\nSLAM\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDiagnosing Lack of Independence in Exogenous Variables\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Basis Gaussian Processes\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Particle Approximations\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNearest Neighbor Gaussian Processes\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKrylov Methods\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMapping with Gaussian Conditioning\n\n\n\nSLAM\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nConjugate Computation\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative ODE Models are VAEs\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Variational Gaussian Processes\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDifferential Equations Refresher\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nApr 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFun with Likelihood Ratios\n\n\n\nmachine_learning\n\n\n\n\n\n\n\n\n\nJan 14, 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/graphslam/graphslam.html",
    "href": "posts/graphslam/graphslam.html",
    "title": "Graph SLAM",
    "section": "",
    "text": "using LinearAlgebra, SparseArrays, RecursiveArrayTools, Distributions, Unzip, ExtendableSparse, ForwardDiff, DiffResults, AppleAccelerate, CairoMakie\n\n\nCairoMakie.enable_only_mime!(\"svg\")\n\nFor a robot to navigate autonomously, it needs to learn both its own location, as well as the locations of any potential obsticles around it, given its sensors’ observations of the world. We’ll create a probabilistic model of our environment and get a MAP estimate of these unknown quantities.\n\nLet \\(x_t \\in \\mathbb{R}^2\\) be the robot’s location at time \\(t\\).\nLet \\(θ_t \\in \\mathbb{R}\\) be the robot’s heading angle at time \\(t\\).\nLet \\(m_i \\in \\mathbb{R}^2\\) be the position of the \\(i\\)th obstacle.\nLet \\(o_{t, i} \\in \\mathbb{R}^2\\) be our robot’s noisy observation of its distance at time \\(t\\) to obstacle \\(i\\).\n\n\nMotion Model\nWe will assume that the robot advances approximately one step forward (in its local coordiante system) at each timestep. Let \\(R(\\theta)\\) be a rotation matrix that converts coordinates in the \\(\\theta\\)-rotated coordinate frame to global coordinates\n\\[\np(x_t | x_{t-1}) = \\mathcal{N}\\left(x_{t-1} + R(\\theta_{t-1}) \\begin{bmatrix} 1  1 \\end{bmatrix}, \\sigma^2_x I\\right)\n\\]\nWe also assume that the heading angle increases by approximately \\(2\\pi/100\\) radians each step, so that \\(p(\\theta_t | \\theta_{t-1}) = \\mathcal{N}(\\theta_{t-1} + \\frac{2\\pi}{100}, \\sigma^2_θ)\\). Let\n\\[\ny_t = \\begin{bmatrix} x_t  \\theta_t \\end{bmatrix}\n\\]\ncombine the location and heading parameters, and \\(y\\) be the vector of all the \\(y_t\\). In the code that follows, Gaussian distributions will be represented by their mean and covariance parameters. For reasons we’ll see later on, it will be easiest to express the mean at time \\(t\\) by indexing into \\(y_{t-1}\\).\n\nΣ_x = Diagonal(fill(0.05, 2))\n\n2×2 Diagonal{Float64, Vector{Float64}}:\n 0.05   ⋅ \n  ⋅    0.05\n\n\n\nμ_θ(y, θ) = y[θ:θ] + fill(2π/100, 1)\n\nμ_θ (generic function with 1 method)\n\n\n\nΣ_θ = 0.008\n\n0.008\n\n\n\nrot(t) = [cos(t) -sin(t); sin(t) cos(t)]\n\nrot (generic function with 1 method)\n\n\n\nμ_x(y, x, θ) = y[x] + rot(y[θ]) * [1.0,0.0]\n\nμ_x (generic function with 1 method)\n\n\n\nstart = zeros(3)\n\n3-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n\n\n\n\nSimulating the Model\nLet’s simulate a robot trajectory that obeys this motion model for \\(T\\) timesteps. We’ll try to reconstruct this trajectory with a maximum likelihood approach in the next section.\n\nT = 100\n\n100\n\n\n\nmean_step(y) = [μ_x(y, 1:2, 3); μ_θ(y, 3)]\n\nmean_step (generic function with 1 method)\n\n\nWithout noise, the robot’s trajectory would look like this, with the color changing from black to yellow over time.\n\nyhat = VectorOfArray(accumulate((y,_)-&gt; mean_step(y), 1:T; init=start))\n\nVectorOfArray{Float64,2}:\n100-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.06283185307179587]\n [1.9980267284282716, 0.06279051952931337, 0.12566370614359174]\n [2.9901414297427493, 0.18812375309361762, 0.1884955592153876]\n [3.972428680471438, 0.3755050676793422, 0.25132741228718347]\n [4.941011841600069, 0.624194954844197, 0.3141592653589793]\n [5.892068357895223, 0.9332119492191444, 0.37699111843077515]\n [6.821844843783475, 1.3013365019038223, 0.439822971502571]\n [7.726671896249494, 1.727115793468895, 0.5026548245743668]\n [8.602978576293358, 2.20886946757061, 0.5654866776461627]\n [9.447306501795373, 2.7446962625496067, 0.6283185307179585]\n ⋮\n [-7.602978576293377, 2.208869467570511, 5.780530482605213]\n [-6.726671896249516, 1.7271157934687904, 5.843362335677009]\n [-5.8218448437834995, 1.3013365019037118, 5.906194188748804]\n [-4.89206835789525, 0.9332119492190273, 5.9690260418206]\n [-3.941011841600099, 0.6241949548440728, 6.031857894892395]\n [-2.9724286804714697, 0.3755050676792106, 6.094689747964191]\n [-1.9901414297427826, 0.18812375309347806, 6.157521601035986]\n [-0.9980267284283059, 0.06279051952916548, 6.220353454107782]\n [-3.4861002973229915e-14, -1.566524687746096e-13, 6.283185307179577]\n\n\n\nlet f1 = Figure()\n    ax1 = f1[1, 1] = Axis(f1)\n    p1 = plot!(yhat[1, :], yhat[2, :], color=1:T)\n    Colorbar(f1[1,2], p1)\n    f1\nend\n\n\n\n\n\n\n\n\nBut as we’ve assumed some noise at each step, the true trajectory is a little more wiggly.\n\nstep(y) = [rand(MultivariateNormal(μ_x(y, 1:2, 3), Σ_x)); rand(Normal(μ_θ(y, 3)[1], Σ_θ))]\n\nstep (generic function with 1 method)\n\n\n\ntrue_y = VectorOfArray(accumulate((y,_)-&gt; step(y), 1:T; init=start))\n\nVectorOfArray{Float64,2}:\n100-element Vector{Vector{Float64}}:\n [0.9948369008080734, -0.17881455800404114, 0.06813995060080176]\n [2.2627930974742254, 0.17186151904112557, 0.12975149463685665]\n [3.2577831373702573, -0.10985759933358524, 0.1829090984073673]\n [4.400490027838276, 0.025835244482602535, 0.2454353075657262]\n [5.287719367549745, 0.33742069041111256, 0.33541549600906045]\n [6.189569042596519, 0.3940744827551949, 0.4010285171119553]\n [6.9437488500664895, 0.4715841172952981, 0.45651471287207906]\n [7.972843771064869, 1.3090087381507867, 0.5219706310461045]\n [8.80959728633984, 1.6739948567368281, 0.6084779835161396]\n [10.08790868778338, 2.3364002859826782, 0.6766190544277169]\n ⋮\n [-8.286488651409167, 4.725160020949448, 5.858871709439354]\n [-7.462292377502831, 4.310271190958247, 5.92407834074762]\n [-6.299206837132079, 3.9592901447036657, 5.989198227579204]\n [-5.0048292897616315, 3.031484767839803, 6.051276214969418]\n [-4.101678423979381, 2.951706786370477, 6.115026977729722]\n [-3.174101632699504, 2.518573445277533, 6.184476993977584]\n [-2.2918628875042364, 2.3265372408595124, 6.242626128695027]\n [-1.1511843642951767, 2.8874967492862282, 6.313806498956315]\n [0.26107996338269074, 2.8672711545305583, 6.374629526124709]\n\n\n\nlet f1 = Figure()\n    ax1 = f1[1, 1] = Axis(f1)\n    p1 = plot!(true_y[1, :], true_y[2, :], color=1:T)\n    Colorbar(f1[1,2], p1)\n    f1\nend\n\n\n\n\n\n\n\n\n\n\nObstacle Location Prior\nWe will assume an uninformative prior over the locations of our \\(k\\) potential obstacles \\(m\\): \\(m_i \\sim \\mathcal{N}(0, \\Sigma_m)\\). For consistency with the earlier mean method defined for positions \\(x\\), the mean function for obstacles will take a dummy argument.\n\nk = 10\n\n10\n\n\n\nμ_m(_) = fill(10, 2)\n\nμ_m (generic function with 1 method)\n\n\n\nΣ_m = Diagonal(fill(100, 2))\n\n2×2 Diagonal{Int64, Vector{Int64}}:\n 100    ⋅\n   ⋅  100\n\n\n\ntrue_m = rand(MultivariateNormal(μ_m(nothing), Σ_m), k)\n\n2×10 Matrix{Float64}:\n 6.55633  25.9119   10.7696  -3.08722  …  19.4976    3.42704  -6.70656\n 8.74785   4.83715  20.3582  19.8423       4.63834  -3.50851   7.83388\n\n\n\nlet f1 = Figure()\n    ax1 = f1[1, 1] = Axis(f1)\n    p1 = plot!(true_y[1, :], true_y[2, :], color=1:T, label=\"robot trajectory\")\n    plot!(true_m[1, :], true_m[2,:], color=:red, label=\"map points\")\n    f1[1,2] = Legend(f1, ax1)\n    f1\nend\n\n\n\n\n\n\n\n\nLet \\[\nz = \\begin{bmatrix} y  m \\end{bmatrix}\n\\] combine the ego and obstacle parameters.\n\ntrue_z = [vec(true_y); vec(true_m)]\n\n320-element Vector{Float64}:\n  0.9948369008080734\n -0.17881455800404114\n  0.06813995060080176\n  2.2627930974742254\n  0.17186151904112557\n  0.12975149463685665\n  3.2577831373702573\n -0.10985759933358524\n  0.1829090984073673\n  4.400490027838276\n  ⋮\n 20.087161874125705\n 16.98673666207452\n 26.761324025474323\n 19.497590609511285\n  4.638343755581045\n  3.427042183951291\n -3.5085063520697517\n -6.706558668864389\n  7.833875387182863\n\n\n\n\nObservation Model\nLet \\(p(o_{t, i} | x_t, θ_t, m_i) = \\mathcal{N}(R(-θ_t)(m_i - x_t), Σ_o)\\), so that the robot’s sensor tells it approximately how close each map point is in its local coordinate frame.\n\nμ_obs(z, x, θ, m) = rot(-z[θ]) * (z[m] - z[x])\n\nμ_obs (generic function with 1 method)\n\n\n\nΣ_obs = Diagonal(fill(0.1, 2))\n\n2×2 Diagonal{Float64, Vector{Float64}}:\n 0.1   ⋅ \n  ⋅   0.1\n\n\n\nobs = VectorOfArray([VectorOfArray([\n    rand(MultivariateNormal(μ_obs(true_z, 3t+1:3t+2, 3t+3, (3T+2m+1):(3T+2m+2)), Σ_obs)) for m in 0:(k-1)]) for t in 0:(T-1)])\n\nVectorOfArray{Float64,3}:\n100-element Vector{VectorOfArray{Float64, 2, Vector{Vector{Float64}}}}:\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[6.013700145086878, 8.368164266295134], [24.568021744453933, 3.6157091049669656], [11.23778228240714, 19.222792500273055], [-2.7040300052210795, 19.792162726658354], [11.250628280481457, -1.6887668931377868], [16.027211901569483, 19.032850002087653], [17.778324286305484, 25.8465606216527], [19.441293019621643, 3.8879295067523945], [1.8809069271553998, -3.251827387382284], [-7.432381353009933, 8.645019960045108]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[5.641985228875032, 7.924150333390872], [24.474762791025697, 1.5399668441475096], [10.896505350585308, 19.14184066164264], [-2.8898071407213397, 19.79983657318185], [9.749145325474837, -3.2938891127184835], [15.803316027431402, 18.16650234550198], [17.878814119398438, 24.86620332021086], [17.671848850537003, 2.078932762216099], [0.5769536724566005, -4.355619037474067], [-8.207967964723636, 8.985428046601001]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[4.851631808665426, 8.188085448607072], [22.948895587595022, 0.43775846135747726], [11.005179454038947, 18.59536299257642], [-2.805042873203761, 20.84842826750874], [8.505294699591524, -2.6997322648621034], [15.70012657408447, 17.572421151893366], [18.128916953436377, 23.834553381464215], [16.780208160819445, 2.1368667977760416], [-0.7400891037317023, -3.417356665861245], [-7.912720993040339, 9.534565325810163]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[4.048231394317695, 7.721987916874405], [21.71834476032371, -0.5659096212098226], [10.88881162981954, 17.926602832838835], [-2.042645731965736, 21.06910413938711], [7.006996582086496, -2.8700125188555052], [16.18457477434436, 16.97570115207072], [18.80072063154177, 23.462534798530292], [16.021686390387767, 0.5784537452527012], [-1.6102837184467977, -3.530878884865166], [-8.539929896505848, 10.424353745640826]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[3.63090613235886, 7.550792587951964], [21.19886142760244, -2.899881179137308], [11.445282614446857, 16.829170336602218], [-1.5711220247939073, 21.216618198260317], [5.9946431651363925, -4.22151707487656], [16.84201289628381, 15.135162281477065], [19.897419903902616, 21.509982311505958], [14.812395117449746, -0.30171320319528583], [-2.9705849900207686, -3.0753253031980003], [-9.351472933012097, 11.256286706657121]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[4.1567292874234045, 7.217876989269879], [20.031530259147335, -3.805016511339554], [11.805343841699973, 17.15847024793535], [-0.9494894059627843, 21.411926277598948], [5.269723616963123, -4.028733124369017], [16.758682983305825, 14.14696720211139], [19.959509571468296, 19.99039322133718], [13.224448152122887, -1.0309267036091796], [-3.5222001411460484, -2.6497070522637056], [-8.954298046447112, 11.774280435000888]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[3.1741880442516948, 7.278860356736828], [18.87521422440231, -4.472497094503896], [11.800325404846303, 16.295369409287265], [-0.7171437115541555, 21.53254662271214], [3.2856046187068175, -3.370560611170286], [16.310533683090117, 13.617389923037623], [20.89472387664705, 18.705680477004066], [13.365791953957965, -1.495587523708471], [-4.7323986077301425, -2.4427779029098704], [-8.882759283217794, 12.533620977684915]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[2.0425556099387574, 7.066129125669452], [17.598079572324643, -5.502543567288933], [12.075143487583459, 15.85152134744959], [-0.5393243431996848, 21.469407020903695], [2.561477443413501, -4.446991446491485], [16.082759754634296, 11.83419562578087], [20.822299827955526, 17.644956151132828], [11.718433710022705, -3.14677933192268], [-6.66235093454659, -1.8072160990252237], [-9.750416360283822, 12.359686401888242]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[2.2816570565727656, 7.383041195033532], [15.808228099440342, -7.550027191654502], [11.507667669820282, 13.967166455229231], [0.5884347133759719, 20.86134703635186], [0.6756153569732424, -4.340809047211794], [16.47356421296756, 11.108958142749307], [20.95613108347853, 15.883372806979661], [10.563130717995497, -3.468325849068952], [-7.146941348087052, -0.4041440353753609], [-9.24822537609671, 13.702260824489597]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[1.6793343967549705, 7.036795779767694], [13.360779749114423, -7.9647506399299255], [11.236228718586213, 13.969091821537623], [1.0391408118453676, 21.900876838648326], [-0.10684530555668281, -4.395463121679125], [15.140171375134607, 9.934715867253184], [20.712718472833604, 15.052217576577188], [9.145744140662826, -4.081860230417136], [-8.923938796838996, -0.11272085563531176], [-9.413796486221301, 14.94540366680314]])\n ⋮\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[11.895435519097228, 10.096250080587007], [30.975050523300183, 14.45443709898133], [11.135232297052879, 22.64729705590045], [-1.71080986823068, 15.406779695838383], [20.658265969190452, 3.838088694804081], [15.233762630418745, 23.734209497280688], [13.698979348814278, 30.149769005198372], [24.84130959192779, 11.402516186176626], [13.913990328907596, -3.308575062072246], [0.2700971701232535, 3.3249075529248797]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[11.544045405312193, 9.06021770042106], [31.029513316602163, 13.163344230056122], [11.58933285912982, 21.653482692182195], [-1.3537117605130766, 16.097935654330694], [20.358796399141443, 1.6629414343363025], [15.91514387742586, 23.2570196053237], [14.649109298607984, 29.916763958813313], [25.45425630262209, 10.045359892816323], [13.294676732727366, -2.963936492473572], [-0.25029973900567865, 3.1924000111842177]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[11.045652254232264, 8.619982301082722], [30.477238913040956, 10.674471999285572], [11.291262593919816, 20.782892126283105], [-1.9471137886124064, 15.816602347816609], [19.128166048454982, 0.45362865668916885], [16.40449799707704, 21.75446187297804], [15.68190718601276, 28.832763883748935], [23.68335834622055, 8.045926400443612], [11.841980930405585, -4.5323125356999405], [-1.4837503894146615, 4.237739513970863]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[9.864512396788715, 8.174355969610168], [29.64109550471643, 8.94464529078981], [11.74909722916652, 21.083713859941614], [-2.1196222870955834, 17.16313205691649], [17.584766293952026, -0.26954654517960586], [16.328243760948826, 21.16344789096051], [16.16578259093745, 27.811516254603013], [23.541726969840862, 6.837205505173892], [9.827960898326841, -4.52230863369103], [-3.685379588831624, 4.64562622457701]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[9.335691826213935, 7.607209026495554], [29.45240430760495, 6.21606512830997], [11.90890616348594, 19.66088439355682], [-2.0329608490032243, 16.705853074573707], [17.288944713096615, -1.16570411797972], [16.891583909028103, 20.100892575353356], [17.403731679617888, 27.02663611787579], [23.672802301167607, 5.45259331883669], [8.604313192351126, -5.224359270684223], [-3.247508519731462, 4.092589703568361]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[8.919742708155354, 7.245067771309988], [28.36992146744046, 5.723139469478966], [12.390020064954982, 18.64410819705427], [-1.151716565306933, 17.289944341814728], [15.000327482057703, -1.7008403954528681], [16.959186573295256, 19.620865492493625], [17.7002432735754, 26.018009499617758], [22.862309288611925, 4.881388901683297], [7.233238624850517, -5.320660824135512], [-3.636429232574335, 5.438673018619582]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[8.279106326855326, 7.149463899354719], [28.091268735912287, 3.6646525160383265], [12.207777477591296, 18.55169023921717], [-1.0393831156839681, 17.730881363829212], [14.462496539724393, -2.9068150158810058], [17.29448503387057, 18.55333657403531], [18.116942128466697, 25.877334959834457], [21.801692311606093, 2.6985815160041837], [5.865153526042226, -5.294348183788221], [-4.383202346059136, 5.140777560712807]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[7.871969763556479, 5.693281179819593], [27.079311433478527, 1.0222543664256254], [12.829613327905994, 17.009600131285378], [-0.9388450513507693, 16.854425667022767], [13.432432475396185, -4.764050539289802], [17.64484260642448, 16.789186893496602], [18.992102332827866, 23.22148939098534], [21.136826483389267, 0.9763542938789962], [4.451279536187137, -6.384753657386503], [-5.609142471465136, 4.931707956247374]])\n VectorOfArray{Float64, 2, Vector{Vector{Float64}}}([[6.739357850166455, 5.314592282275178], [25.824003013561565, -0.44346792418659314], [12.613184509356499, 16.829002244521998], [-1.9804481682466397, 17.148558489846486], [11.331549703798766, -5.239902906708373], [16.671274647209334, 15.523578816652083], [18.808836993675946, 21.73238749095739], [18.961881301347855, 0.29836699013508333], [2.304305953590278, -6.099487027629671], [-6.608484484768158, 5.967973441682578]])\n\n\nThe following plots our observations over time, from black at the start to yellow at time \\(T\\). The swirling shape comes from the fact that the robot spins as it moves.\n\nlet f = Figure()\n    ax = f[1, 1] = Axis(f)\n    for i in 1:k\n        p1 = plot!(obs[1,i,:], obs[2, i,:], color=1:T)\n        if i == 1\n            Colorbar(f[1,2], p1)\n        end\n    end\n    f\nend\n\n\n\n\n\n\n\n\n\n\nGuessing an Initial Trajectory\nWe’ll start out with our guess \\(\\hat{y}\\) as the mean of the motion model. We’ll guess \\(\\hat{m}\\) by just sampling from the prior.\n\nmhat = rand(MultivariateNormal(μ_m(nothing), Σ_m), k)\n\n2×10 Matrix{Float64}:\n  9.77342   3.92698  14.6207    …  27.3042  -4.2041   8.7704   10.2939\n -0.432306  3.51201  -0.757796      4.7226   8.80291  1.73778  25.5637\n\n\n\nzhat = [vec(yhat); vec(mhat)]\n\n320-element Vector{Float64}:\n  1.0\n  0.0\n  0.06283185307179587\n  1.9980267284282716\n  0.06279051952931337\n  0.12566370614359174\n  2.9901414297427493\n  0.18812375309361762\n  0.1884955592153876\n  3.972428680471438\n  ⋮\n  4.6246080767284585\n 27.30417657249438\n  4.722597230875135\n -4.204101557016603\n  8.80290626371208\n  8.770396239663455\n  1.7377838203908151\n 10.293861660079147\n 25.5637364438413\n\n\n\n\nAssembling Trajectory Probability\nThe negative joint log density of our guess and observations \\(L(y, m)\\) is a sum of factors \\(L_i = (v_i - μ_i(z))^TΛ_i(v_i - μ_i(z))\\) for different variables \\(v_i\\) with means \\(\\mu_i\\) and precisions \\(\\Lambda_i\\). The following code assembles these factors for the observation model above.\n\nstart_x(_) = [0., 1.0]\n\nstart_x (generic function with 1 method)\n\n\n\nstart_θ(_) = fill(2π/100, 1)\n\nstart_θ (generic function with 1 method)\n\n\n\n\nMaximizing Trajectory Probability\nWe know \\(L_i(z) = -f_i(z)^T\\Lambda_i f(z)\\) where \\(f_i\\) is a potentially nonlinear transformation of \\(z\\). Taylor expand \\(f_i(z) \\approx f_i(\\hat{z}) + J_i\\Delta\\) where \\(\\Delta = z - \\hat{z}\\) and \\(J_i = \\nabla_z f_i(\\hat{z})\\). Substitute this expression in definition of \\(L_i\\) giving \\(L_i(z) \\approx \\Delta^T H_i\\Delta + 2 b_i^T\\Delta + c\\), where \\(H_i = J_i^T\\Lambda_i J_i\\) and \\(b_i = J_i^T\\Lambda_i f_i(\\hat{z})\\). When add up all these factors to get the full log probability, we get \\(\\Delta^T H \\Delta + 2 b^T \\Delta + c\\) where \\(b\\) is the sum of the \\(b_i\\) and \\(H\\) is the sum of the \\(H_i\\).\nTo minimize this quantity, take the derivative. We find that \\(L\\) will be approximately minimized when \\(H\\Delta = -b\\) or \\(\\Delta = -H^{-1}b\\). It remains to solve for \\(\\Delta\\) and modify \\(z\\) appropriately. Repeating this gives the Gauss Newton algorithm.\nWhen \\(f\\) is not well approximated by its first order Taylor expansion, instead of solving \\(H^{-1}b\\), it works better to solve the smoothed version \\((H + \\lambda \\text{Diag}(H))^{-1}b\\), where \\(\\lambda\\) is a factor that gets slowly lowered to zero as \\(z\\) nears its optimal value. This is the Levenberg-Marquardt algorithm.\n\nUnderstanding log_prob_builder\nThe log_prob_builder function above has two forms, depending on its jac argument. If jac=true, the function builds a vector of terms \\(H_i\\) and \\(b_i\\) described above. Othewise, it builds a vector of the negative log probabilities \\(L_i\\).\n\nconst QuadformBuilder = Vector{Tuple{&lt;: ExtendableSparseMatrix, Vector}}\n\n\nVector{Tuple{ExtendableSparseMatrixCSC, Vector}} (alias for Array{Tuple{ExtendableSparseMatrixCSC, Array{T, 1} where T}, 1})\n\n\n\nThese terms are computed by the factor function, which assembles \\(H\\) and \\(b\\) out of \\(f_i(z)\\) and its jacobian \\(J\\).\n\nfactor(Λ, (fval,_)::Tuple{&lt;:Any, Nothing}) = fval' * (Λ * fval)\n\nfactor (generic function with 1 method)\n\n\n\nfunction factor(Λ, (fval, J))\n    ΛJ = Λ * J\n    b = (ΛJ)' * fval\n    H = J' * ΛJ\n    (ExtendableSparseMatrix(H), b)\nend\n\nfactor (generic function with 2 methods)\n\n\nFinally, we compute \\(f_i(z)\\) and its Jacobian using the following wrappers around the sparse_jac function, which computes a function and its sparse Jacobian.\n\n\n\nHandling Sparsity\nTo define the sparse_jac function, we’ll wrap the jacobian function from the ForwardDiff library.\n\nfunction sparse_jac(f, z, support, outs; jac=true)\n    jac || return f(z), nothing\n    M = nothing\n    fz = nothing\n    function f_wrapper(z_sup)\n        newz = collect(eltype(z_sup), z)\n        newz[support] .= z_sup\n        f(newz)\n    end\n    z_sup = z[support]\n    res = DiffResults.JacobianResult(zeros(outs), z_sup)\n    res = ForwardDiff.jacobian!(res, f_wrapper, z_sup)\n    J = DiffResults.jacobian(res)\n    M = ExtendableSparseMatrix(outs, length(z))\n    M[:, support] .= J\n    DiffResults.value(res), M\nend\n\nsparse_jac (generic function with 1 method)\n\n\n\nfunction term(z, target, μ, outs, ixs...; jac=true)\n    support = [target; reduce(vcat, ixs; init=Int[])]\n    f(z) = z[target] - μ(z, ixs...)\n    sparse_jac(f, z, support, outs; jac)\nend\n\nterm (generic function with 1 method)\n\n\n\nfunction obs_term(z, obs, ixs...; jac=true)\n    support = reduce(vcat, ixs; init=Int[])\n    f(z) = obs - μ_obs(z, ixs...)\n    sparse_jac(f, z, support, 2; jac)\nend\n\nobs_term (generic function with 1 method)\n\n\n\nfunction log_prob_builder(z; jac=true)\n    bld = jac ? QuadformBuilder() : Vector{Float64}()\n\n    # We know the location distribution at time 1\n    push!(bld, factor(inv(Σ_x), term(z, 1:2, start_x, 2; jac)))\n    push!(bld, factor(inv(Σ_θ), term(z, 3:3, start_θ, 1; jac)))\n\n    # Add the Markovian jump probabilities at each step\n    for t in 1:(T-1)\n        ix = 3t+1\n        pix = 3(t-1)+1\n        push!(bld, factor(inv(Σ_x), term(z, ix:ix+1, μ_x, 2, pix:pix+1, pix+2; jac)))\n        push!(bld, factor(inv(Σ_θ), term(z, ix+2:ix+2, μ_θ, 1, pix+2; jac)))\n    end\n\n    # Add the prior on map components\n    for i in 0:(k-1)\n        ix = 3T+2i+1\n        push!(bld, factor(inv(Σ_m), term(z, ix:(ix+1), μ_m, 2; jac)))\n    end\n\n    # Add the observations\n    for t in 0:(T-1)\n        for i in 1:k\n            m = 3T+2(i-1)\n            ix = 3t+1\n            push!(bld, factor(inv(Σ_obs), obs_term(z, obs[:, i, t+1], ix:(ix+1), ix+2,(m+1):(m+2); jac)))\n        end\n    end\n    jac ? sum.(unzip(bld)) : sum(bld)\nend\n\nlog_prob_builder (generic function with 1 method)\n\n\n\nfunction maximize_logprob(z; λ=1e-3, α=2, β=3, maxiter=100, eps=1e-4)\n    Δ = ones(length(z))\n    i = 0\n    prevL = log_prob_builder(z; jac=false)\n    L = 0.0\n    while any(abs.(Δ) .&gt; eps)  && i &lt; maxiter\n        H, b = log_prob_builder(z; jac=true)\n        while true\n            Δ = (H + λ * Diagonal(H)) \\ b\n            L = log_prob_builder(z - Δ; jac=false)\n            L &gt;= prevL || break\n            λ *= β\n        end\n        z[:] .-= Δ\n        λ /= α\n        prevL = L\n        i += 1\n    end\n    println(\"Concluded after $i iterations\")\n    z\nend\n\nmaximize_logprob (generic function with 1 method)\n\n\n\n\nRunning the Optimization\nHere’s the negative log probability of the robot’s true trajectory.\n\nlog_prob_builder(true_z; jac=false)\n\n2082.4291845741\n\n\nWereas here’s how likely our prior mean would be:\n\nlog_prob_builder(zhat; jac=false)\n\n4.293105704799155e6\n\n\nRunning Levenberg-Marquardt gives us a solution that isn’t our true trajectory, but is technically more likely under the generative model above.\n\nz_guess = maximize_logprob(copy(zhat))\n\nConcluded after 23 iterations\n\n\n320-element Vector{Float64}:\n  0.0034378099328569015\n  0.9914065354044322\n  0.04412767144911623\n  1.1616831781275363\n  1.2383383769126906\n  0.10251482279741249\n  2.26578807824746\n  1.006135883145371\n  0.15915735291351707\n  3.2835135446382417\n  ⋮\n 20.80105806876392\n 16.461071406617254\n 27.51403449819456\n 18.585176469681425\n  5.309729223046708\n  2.28449550904925\n -2.4762014943557835\n -7.58331169017551\n  9.055115769823518\n\n\n\nlog_prob_builder(z_guess; jac=false)\n\n1765.1322400328486\n\n\nBelow, we plot the guessed trajectory as a line, with the true trajectory represented as a scatter plot.\n\ny_guess = reshape(z_guess[1:3T], (3, T))\n\n3×100 Matrix{Float64}:\n 0.00343781  1.16168   2.26579   3.28351   …  -3.27478  -2.14573  -0.667408\n 0.991407    1.23834   1.00614   1.03379       3.46532   3.84416   3.83187\n 0.0441277   0.102515  0.159157  0.223708      6.22054   6.30426   6.36082\n\n\n\nlet f1 = Figure()\n    ax1 = f1[1, 1] = Axis(f1)\n    p1 = plot!(true_y[1, :], true_y[2, :], color=1:T)\n    lines!(y_guess[1,:], y_guess[2,:], color=1:T+1)\n    Colorbar(f1[1,2], p1)\n    f1\nend\n\n\n\n\n\n\n\n\nWe ended up with a pretty good estiamte of the map points’ locations as well.\n\nm_guess = reshape(z_guess[3T+1:end], (2, k))\n\n2×10 Matrix{Float64}:\n 5.74172  24.964    10.1238  -3.69083  …  18.5852    2.2845  -7.58331\n 9.74611   5.44817  21.2426  20.9662       5.30973  -2.4762   9.05512\n\n\n\nlet f1 = Figure()\n    ax1 = f1[1, 1] = Axis(f1)\n    plot!(true_m[1, :], true_m[2,:], color=:red, label=\"map points\")\n    plot!(m_guess[1, :], m_guess[2,:], color=:black, label=\"guesses\")\n    f1[1,2] = Legend(f1, ax1)\n    f1\nend"
  },
  {
    "objectID": "posts/bayesian_logistic_regression/bayesian_logistic_regression.html",
    "href": "posts/bayesian_logistic_regression/bayesian_logistic_regression.html",
    "title": "Bayesian Power Analysis for A/B/n Tests",
    "section": "",
    "text": "using LogExpFunctions, LinearAlgebra, Distributions, QuasiMonteCarlo, CairoMakie, StatsModels, PythonCall\nCairoMakie.enable_only_mime!(\"png\")\nThis post highlights a Bayesian approach to sample size estimation in A/B/n testing. Say we’re trying to test which variant of an email message generates the highest response rate from a population. We consider \\(k\\) different messages and send out \\(n\\) emails for each message. After we wait for responses, we should be able to tell which message yielded the highest response rate as long as we set \\(n\\) high enough. But we generally can’t send out too many messages: say we’re capped at \\(N\\) total. How do we choose the highest \\(k\\) that still allows us to confidently pick which message got the highest response rate?\nWe’ll start with a prior over the response rates of each message. Choosing an independent prior over the rates for each message would be overly naive: all the messages will likely give very similar response rates, so if we know the rate for one message, we’ll have a good guess about the rates for other messages too. Instead, we’ll choose one message to be a baseline, giving it a response rate prior centered at 7%. We’ll sample independent multiples of this baseline to give the response rates for the other messages.\npriors = [Normal(logit(0.07), 0.5), Normal(0, log(1.3))]\n\n2-element Vector{Normal{Float64}}:\n Distributions.Normal{Float64}(μ=-2.5866893440979424, σ=0.5)\n Distributions.Normal{Float64}(μ=0.0, σ=0.26236426446749106)\nfunction plot_prior(p, transform, title)\n    f = Figure()\n    ax = Axis(f[1, 1], title=title)\n    hist!(ax, transform.(rand(p, 1000)))\n    qs = transform.(quantile.(p, [0.05, 0.5, 0.95]))\n    vlines!(ax, qs, color=:orange, linestyle = :dash)\n    f\nend\n\nplot_prior (generic function with 1 method)\nplot_prior(priors[1], logistic, \"Control success probability\")\nplot_prior(priors[2], exp, \"Odds ratio\")\nIf the posterior odds ratios of the non-baseline messages all have 95% of their mass below 1.1, we’ll stick with the baseline message. If, instead, any of the messages has 95% of its posterior odds ratio mass above 1.1, we’ll choose one of those. If neither of these conditions occur, our experiment will have been a failure: we’ll remain too uncertain to make a decision of whether or not to switch from the baseline message. Our task is to choose the number of messages to ensure that we have high probability of running a successful experiment.\nTo find the posterior odds ratio for each message, we’ll use the Laplace approximation for Bayesian logistic regression. We’ll use Newton’s method to find the MAP.\nfunction newton(f, x; maxiter=1000)\n    for _ in 1:maxiter\n        g, H = f(x)\n        if norm(g) &lt; 1e-4\n            return (x, H)\n        else\n            x .-= H \\ g\n        end\n    end\n    error(\"Maximum iterations reached\")\nend\n\nnewton (generic function with 1 method)\nThe following code averages the decision probability over quasi Monte Carlo samples of possible datasets we might observe if the true parameters are given by θ.\nfunction decision_prob(θ, n, k, prior_means, prior_precs; m=80)\n    X = [ones(k,1) StatsModels.ContrastsMatrix(DummyCoding(), 1:k).matrix]\n    dists = Binomial.(n, logistic.(X * θ))\n    samples = QuasiMonteCarlo.sample(m, k, SobolSample())\n    succs = stack([quantile.(d, s) for (d,s) in zip(dists, eachrow(samples))])\n    mean(eachrow(succs)) do succs\n        function gh(t)\n            y = logistic.(X * t)\n            vals = succs .* (y .- 1) .+ (n .- succs) .* y\n            g = [sum(vals); vals[2:end]] .+ (t .- prior_means) .* prior_precs\n            H = Diagonal(prior_precs) + sum(zip(y, eachrow(X))) do (yi, r)\n                n * yi * (1 - yi) * Symmetric(r * r')\n            end\n            g, H\n        end\n        mode, H = newton(gh, Vector(θ))\n        covar = inv(H)\n        c = cdf.(Normal.(mode[2:end], diag(covar)[2:end]), log(1.1))\n        Float32(all(c .&gt; 0.95) | any(c .&lt; 0.05))\n    end\nend\n\ndecision_prob (generic function with 1 method)\nWe also need to average the decision probability over our prior for θ.\nfunction avg_decision_prob(total_n, k, m=80)\n    n = div(total_n, k)\n    samples = QuasiMonteCarlo.sample(m, k, SobolSample())\n    full_priors = [priors; fill(priors[2], k-2)]\n    prior_means = mean.(full_priors)\n    prior_precs = 1 ./ var.(full_priors)\n    prior_samples = stack([quantile.(p, c) for (p,c) in\n                                  zip(full_priors, eachrow(samples))])\n    mean(eachrow(prior_samples)) do θ\n        decision_prob(θ, n, k, prior_means, prior_precs)\n    end\nend\n\navg_decision_prob (generic function with 2 methods)\nNow, we can use these quasi Monte Carlo estimates to approximate how many messages we’ll be able to test for a given sample size.\nlet\nf = Figure()\nax = Axis(f[1, 1],\n          yminorgridvisible=true,\n          yminorticks = IntervalsBetween(5),\n          title=\"Sample Size Requirements for Correlated Priors\",\n          xlabel=\"samples\", ylabel=\"decision probability\")\nns = 100:100:5000\nfor k in 2:5\n    probs = avg_decision_prob.(ns, k)\n    lines!(ax, ns, probs, label=\"$k arms\")\nend\nLegend(f[1, 2], ax)\nf\nend"
  },
  {
    "objectID": "posts/bayesian_logistic_regression/bayesian_logistic_regression.html#estimates-from-independent-priors",
    "href": "posts/bayesian_logistic_regression/bayesian_logistic_regression.html#estimates-from-independent-priors",
    "title": "Bayesian Power Analysis for A/B/n Tests",
    "section": "Estimates From Independent Priors",
    "text": "Estimates From Independent Priors\nIf we had used a simple Beta-Bernoulli model assuming the each of the message rate priors were independent, our estimates of required sample size would be slightly inflated.\nPreviously, we used a Normal prior over the log odds of getting a response. But with a Beta distribution, we’ll want to model the probability of getting a response directly. Applying the logistic transformation to samples from a Normal distribution produces samples from a logit-normal distribution, which gives us the response probabilities we had in the first section.\n\nqs = quantile(LogitNormal(params(priors[1])...), [0.05, 0.95])\n\n2-element Vector{Float64}:\n 0.032011773807213074\n 0.1462572912990306\n\n\nWe’ll use the preliz python package to find a similar Beta distribution to use for our independent Beta prior.\n\npz = pyimport(\"preliz\")\n\n/Users/sam/myblog/posts/bayesian_logistic_regression/.venv/lib/python3.13/site-packages/preliz/ppls/pymc_io.py:16: UserWarning: PyMC not installed. PyMC related functions will not work.\n  warnings.warn(\"PyMC not installed. PyMC related functions will not work.\")\n/Users/sam/myblog/posts/bayesian_logistic_regression/.venv/lib/python3.13/site-packages/preliz/ppls/agnostic.py:33: UserWarning: PyMC not installed. PyMC related functions will not work.\n  warnings.warn(\"PyMC not installed. PyMC related functions will not work.\")\n\n\nPython: &lt;module 'preliz' from '/Users/sam/myblog/posts/bayesian_logistic_regression/.venv/lib/python3.13/site-packages/preliz/__init__.py'&gt;\n\n\n\nlet\n    dist = pz.Beta()\n    pz.maxent(dist, lower=qs[1], upper=qs[2], mass=0.9)\n    dist\nend\n\n\nPython: Beta(alpha=5.49, beta=55.4)\n\n\n\nIt seems like the Beta distribution parameters must comparable to our original setting are 5.49 and 55.4.\n\nbeta_prior = Beta(5.49, 55.4)\n\nDistributions.Beta{Float64}(α=5.49, β=55.4)\n\n\nOnce again, we can simulate the decision probability for different numbers of samples. The curve looks similar to what we observed above, but while 4k samples got us to 95% previously, it now gets us below 80%.\n\nfunction indep_decision_prob(p, n, k, prior_a, prior_b; m=80)\n    dists = Binomial.(n, p)\n    samples = QuasiMonteCarlo.sample(m, k, SobolSample())\n    succs = stack([quantile.(d, s) for (d,s) in zip(dists, eachrow(samples))])\n    mean(eachrow(succs)) do succs\n        posts = Beta.(prior_a .+ succs, prior_b .+ (n .- succs))\n        ps = stack(rand.(posts, m))\n        c = mean(Float64.((ps[:, 2:end] ./ ps[:, 1]) .&lt; 1.1); dims=1)\n        Float64(all(c .&gt; 0.95) | any(c .&lt; 0.05))\n    end\nend\n\nindep_decision_prob (generic function with 1 method)\n\n\n\nfunction avg_indep_decision_prob(total_n, k, m=80)\n    n = div(total_n, k)\n    samples = QuasiMonteCarlo.sample(m, k, SobolSample())\n    prior_params = params(beta_prior)\n    prior_samples = stack(quantile.(beta_prior, eachrow(samples)))\n    mean(eachrow(prior_samples)) do θ\n        indep_decision_prob(θ, n, k, prior_params[1], prior_params[1])\n    end\nend\n\navg_indep_decision_prob (generic function with 2 methods)\n\n\n\nlet\nf = Figure()\nax = Axis(f[1, 1],\n          yminorgridvisible=true,\n          yminorticks = IntervalsBetween(5),\n          title=\"Sample Size Requirements For Independent Beta Priors\",\n          xlabel=\"samples\", ylabel=\"decision probability\")\nns = 100:100:5000\nfor k in 2:5\n    probs = avg_indep_decision_prob.(ns, k)\n    lines!(ax, ns, probs, label=\"$k arms\")\nend\nLegend(f[1, 2], ax)\nf\nend"
  },
  {
    "objectID": "posts/differential-equations.html",
    "href": "posts/differential-equations.html",
    "title": "Differential Equations Refresher",
    "section": "",
    "text": "In my freshman year of college, I took an introductory differential equations class. That was nine years ago. I’ve forgotten pretty much everything, so I thought I’d review a little, trying to generalize the techniques along the way. I’ll use summation notation throughout, and write \\(\\frac{\\partial^n}{\\partial x^n}\\) as \\(\\partial^n_x\\)."
  },
  {
    "objectID": "posts/differential-equations.html#ordinary-differential-equations",
    "href": "posts/differential-equations.html#ordinary-differential-equations",
    "title": "Differential Equations Refresher",
    "section": "Ordinary Differential Equations",
    "text": "Ordinary Differential Equations\nSolving ordinary differential equations is mostly an exercise in linear algebra.\n\nInitial Value Problems\nLet \\(u : \\mathbb{R} \\to \\mathbb{R}^n\\). It’s a vector function of time. In an initial value problem, we know that \\(u'(t) = Au(t)\\) for some linear operator \\(A\\), and we know \\(u(0)\\). If \\(A\\) were diagonal, finding \\(u(t)\\) would be easy. Each element would obey \\(u_i'(t) = A_{ii} u_i(t)\\), which means \\(u_i(t) = e^{A_{ii}t}u_i(0)\\). The trick to these problems, therefore, is to express \\(u\\) in a basis where \\(A\\) is diagonal: its eigenvector basis. Say \\(A\\) has eigenvectors \\(v_i\\), and \\(u(t) =\\sum_i c_i(t)v_i\\) for some \\(c_i\\). Then \\(Au(t) = \\lambda_i c_i(t) v_i\\), so \\[\nu(t) = \\sum_i e^{\\lambda_i t}c_i(0) v_i= e^{At}u(0)\n\\] This can also be used for constraints of the form \\(u''(t) = Au'(t) + Bu(t)\\), when we’re given \\(u(0)\\) and \\(u'(0)\\). Just expand the system into \\[\n\\begin{pmatrix} u'' \\\\ u' \\end{pmatrix} = \\begin{bmatrix} A & B \\\\ I & 0 \\end{bmatrix} \\begin{pmatrix} u' \\\\ u \\end{pmatrix}\n\\] We’re left with the same form we had before.\n\n\nCharacteristic Equations\nMore generally, say your differential equation is described by the following linear system: \\[\n\\begin{pmatrix}\nu'''' \\\\\nu''' \\\\\nu'' \\\\\nu' \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na_1 & a_2 & a_3 & a_4 \\\\\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nu''' \\\\\nu'' \\\\\nu' \\\\\nu\n\\end{pmatrix}\n\\] Let the matrix in the middle be \\(M\\). We want to find \\(\\lambda\\) such that \\(|M-\\lambda I| = 0\\). Calculating the determinant with a cofactor expansion looks like this: \\[\n(a_1 - \\lambda) \\begin{vmatrix}\n-\\lambda & 0 & 0 \\\\\n1 & -\\lambda & 0 \\\\\n0 & 1 & -\\lambda\n\\end{vmatrix}\n-\na_2 \\begin{vmatrix}\n1 & 0 & 0 \\\\\n0 & -\\lambda & 0 \\\\\n0 & 1 & -\\lambda\n\\end{vmatrix}\n+ \\dots\n\\] Each of these are cofactors are triangular. The determinant of a triangular matrix is the product of pivots. We get \\((a_1 - \\lambda)(-\\lambda)^3 - a_2 (-\\lambda)^2 + a_3(-\\lambda) - a_4 =0\\).\nThis simplifies to the characteristic equation \\(\\lambda^4 = a_1 \\lambda^3 + a_2 \\lambda^2 + a_3 \\lambda + a_4\\), which can be easily read off the top row of the matrix.\nWhen differential equations are written in the form \\(au’’ + bu’ + cu = 0\\), this means we’re solving a system like this: \\[\n\\begin{pmatrix}\nu'' \\\\\nu' \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-b/a & -c/a \\\\\n1 & 0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nu' \\\\\nu\n\\end{pmatrix}\n\\] We get \\((-b/a - \\lambda)(-\\lambda) + c/a =0\\). Multiply by \\(a\\) to get \\(a\\lambda^2 + b\\lambda + c = 0\\).\n\n\nInhomogenous Differential Equations\nSo far, we’ve seen differential equations of the form \\(u' - Au = 0\\). But what if the right hand side is some \\(q(t)\\) instead? We can use integrating factors to solve this. Multiply both sides by \\(e^{-At}\\): \\[\ne^{-At}u' - e^{-At}Au = e^{-At}q(t)\n\\] By the product rule, the left hand side is just the derivative of \\(e^{-At}u\\). This lets its integrate both sides from \\(0\\) to \\(T\\).\n\\[\ne^{-AT}u - u(0) = \\int_0^T e^{-At}q(t)\\, dt\n\\] Multiply by \\(e^{AT}\\) to get \\(u = e^{AT}u(0) + \\int_0^T e^{A(T-t)}q(t)\\, dt\\).\nConceptually, we’re adding up a bunch of different copies of our homogenous equation \\(y'=Ay\\), but with different starting conditions \\(q(t)\\) and shifted forward in time."
  },
  {
    "objectID": "posts/differential-equations.html#partial-differential-equations",
    "href": "posts/differential-equations.html#partial-differential-equations",
    "title": "Differential Equations Refresher",
    "section": "Partial Differential Equations",
    "text": "Partial Differential Equations\nWith partial differential equations (where we can be differentiating with respect to multiple different variables), the simple finite dimensional vector spaces we’ve been using won’t be as useful. We’ll have to use an infinite dimensional basis.\n\nFunctions are vectors\nFunctions \\(f : Y \\to Z\\) form a vector space when \\(Z\\) is a field. For example, we could form a basis from the delta functions for different values of \\(Y\\), or we could use the Fourier basis. It’s also pretty easy to see how they form a field. This means that functions \\(g : X \\to Y \\to Z\\) are also vector spaces, but over the field of functions in \\(Y \\to Z\\). If we have a function \\(u(x,y) : (X,Y) \\to Z\\), therefore, we can express it both as \\(X \\to Y \\to Z\\) (making it a vector space over functions \\(Y \\to Z\\)) and as \\(Y \\to X \\to Z\\) (making it a vector space over functions \\(X \\to Z\\)). To be more concrete, we can both think about \\(u(x,y)\\) as a linear combination of basis functions \\(u_i(x)\\) where the coefficients \\(c_i(y)\\) depend on \\(y\\), as well as a linear combination of basis functions \\(u_i(y)\\) where the coefficients \\(c_i(x)\\) depend on \\(x\\). In other words, any function \\(u(x,y)\\) can be written as \\(c_{ij}u_i(x)u_j(y)\\).\n\n\nEigenfunctions of derivatives are exponentials\nSay \\(f : A \\to B\\) is a linear function and \\(A\\) is a vector space. Then, for any \\(u \\in A\\), \\(f(u) = f(c_iu_i) = c_if(u_i)\\), where the \\(u_i\\) are a basis for \\(A\\). Specifically, you can choose \\(u_i\\) to be an eigenvector basis for \\(f\\), so that \\(f(c_iu_i) = c_i \\lambda_i u_i\\) for eigenvalues \\(\\lambda_i\\). If the vector space \\(A\\) we’re looking at consists of functions of a single argument, then when \\(\\frac{d^n}{dx^n}v_i=\\lambda^n v_i\\), \\(v_i(x)=e^{\\lambda x}\\). If the vector space contains functions \\(u(x,y)\\) of multiple arguments, when \\(\\frac{\\partial^n u_i}{\\partial x^n} = \\lambda^n u_i\\), \\(u_i(x,y) = e^{\\lambda x}u_i(y)\\).\n\n\nSolving homogeneous equations\nSay we know that \\(A\\frac{d^2 u}{dx} + B \\frac{du}{dx} + Cu = 0\\). We can express \\(u\\) in the derivative’s eigenfunction basis to get \\((A\\lambda_i^2 + B \\lambda_i + C)c_i e^{\\lambda_i t}=0\\), which simplifies to \\(A\\lambda_i^2 + B\\lambda_i + C = 0\\) (once again giving us the characteristic equation). This means that the only eigenfunctions that make up \\(u\\) are those with eigenvalues that are the roots of the characteristic equation.\nWe can solve a system of linear equations to get the coefficients \\(c_i\\). If we know the initial conditions \\(u(0)\\) and \\(\\partial_x u(0)\\), when \\(x=0\\), we get that the sum \\(c_i = u(0)\\), and \\(c_i \\lambda_i = \\partial_x u(0)\\). If, alternately, we know \\(u(0)\\) and \\(u(X)\\) for some \\(X\\), the second equation in the system becomes \\(c_ie^{\\lambda_i T} = u(T)\\), which is still linear in the \\(c_i\\).\n\n\nSeparation of Variables\nPartial differential equations of the form \\(\\partial^n_x u = k^m \\partial^m_y u\\) are called “separable”. To find what \\(u(x,y)\\) is, we express it in the basis \\(c_i u_i(x) u_i(y)\\). The differential equation tells us that \\(c_{ij} u_i(y) \\partial^n_x u_j(x) = k^m c_{ij} u_j(x) \\partial^m_y u_i(y)\\). Considering each component of the sum separately, we get \\(\\frac{\\partial^n_x u_i(x)}{u_i(x)} = k^m \\frac{\\partial^m_y u_i(y)}{u_i(y)}\\). As one side of the equation is only a function of \\(x\\) and the other is only a function of \\(y\\), both must equal a constant. Call this \\(\\lambda^n\\). We get \\(\\partial_x^n u_i(x) = \\lambda^n u_i(x)\\), which means \\(u_i(x)=e^{\\lambda x}\\). For the other side, \\(\\partial_y^m u_i(y) = \\frac{\\lambda^m}{k^m} u_i(y)\\), which means \\(u_i(y) = e^{\\frac{\\lambda y}{k}}\\). Together, we find that \\(u_i(x,y)=e^{\\lambda (x + \\frac{\\lambda y}{k})}\\)."
  },
  {
    "objectID": "posts/ankivec/ankivec.html",
    "href": "posts/ankivec/ankivec.html",
    "title": "AnkiVec: Vector Search for Anki",
    "section": "",
    "text": "I just released AnkiVec, an Anki addon that creates vector embeddings for cards using Ollama and enables hybrid semantic search with ChromaDB."
  },
  {
    "objectID": "posts/ankivec/ankivec.html#features",
    "href": "posts/ankivec/ankivec.html#features",
    "title": "AnkiVec: Vector Search for Anki",
    "section": "Features",
    "text": "Features\n\nVector Embeddings: Generate embeddings for all cards using local Ollama models\nSemantic Search: Find cards by meaning, not just keywords\nFast Local Processing: Uses lightweight embedding models (nomic-embed-text by default)\nPersistent Storage: ChromaDB stores embeddings for quick retrieval"
  },
  {
    "objectID": "posts/ankivec/ankivec.html#how-to-use-it",
    "href": "posts/ankivec/ankivec.html#how-to-use-it",
    "title": "AnkiVec: Vector Search for Anki",
    "section": "How to Use It",
    "text": "How to Use It\nWhen you restart Anki after installing the add-on, a vector-database will be initialized for your deck. This process may take a few minute- you should see a loading dialog with a progress bar. From this point on, any changes to your deck will be automatically indexed.\nTo search the vector database, use the form “vec: [your query here]” in Anki’s usual search bar. You can combine keyword searches with vector searches by putting the “vec” section at the end of the query: e.g. “keyword1 keyword2 vec: [natural language description]”."
  },
  {
    "objectID": "posts/ankivec/ankivec.html#why-chromadb",
    "href": "posts/ankivec/ankivec.html#why-chromadb",
    "title": "AnkiVec: Vector Search for Anki",
    "section": "Why ChromaDB?",
    "text": "Why ChromaDB?\nI started trying to use sqlite-vec, modifying Anki’s internal sqlite database. Unfortunately, Anki doesn’t like other processes modifying its database, and frequently warns mistakenly about corruption issues. Eventually, I figured it was easiest to keep the embedding database separate, and switched to ChromaDB."
  },
  {
    "objectID": "posts/ankivec/ankivec.html#why-ollama",
    "href": "posts/ankivec/ankivec.html#why-ollama",
    "title": "AnkiVec: Vector Search for Anki",
    "section": "Why Ollama?",
    "text": "Why Ollama?\nIf I’m already using ChromaDB, why use Ollama? ChromaDB has its own embedding models built in! Unfortunately, they’re super slow compared to Ollama’s. For the all-minilm model that ChromaDB uses by default, Ollama’s version is about 2.72x faster on my Mac. Even if you jump through the necessary hoops to get MPS acceleration for ChromaDB, Ollama is still over twice as fast. Yes, this makes installation a bit more complicated, but it’s worth it, and makes it easier to swap in other embedding choices. You can run the benchmarks yourself from the repository."
  },
  {
    "objectID": "posts/ankivec/ankivec.html#how-do-dependencies-get-installed",
    "href": "posts/ankivec/ankivec.html#how-do-dependencies-get-installed",
    "title": "AnkiVec: Vector Search for Anki",
    "section": "How do dependencies get installed?",
    "text": "How do dependencies get installed?\nAnki’s launcher uses a bundled version of uv to download its dependencies. I piggybacked on this to install my own dependencies (ollama and chromadb).\n\nWhen the addon is initialized, it checks where Anki’s copy of uv is located. On a Mac, this is within the application bundle itself.\nNext, it uv syncs its own directory using this bundled uv instance.\nFinally, it adds the addon directory to the Python path before importing any modules.\n\nThis avoids the usual headaches of vendoring in all the transitive dependencies of an Anki addon. Just let uv handle it for you!"
  },
  {
    "objectID": "posts/sizecheck.html",
    "href": "posts/sizecheck.html",
    "title": "Sizecheck: Making Tensor Code Self-Documenting with Runtime Shape Validation",
    "section": "",
    "text": "Writing neural networks often feels like juggling tensors in the dark. You know that attention_weights should be 4-dimensional, but PyTorch won’t tell you until your matrix multiplication explodes at runtime. What if your variable names could automatically validate tensor shapes?\nMeet sizecheck – a Python decorator that for automatic runtime validation."
  },
  {
    "objectID": "posts/sizecheck.html#the-shape-suffix-convention",
    "href": "posts/sizecheck.html#the-shape-suffix-convention",
    "title": "Sizecheck: Making Tensor Code Self-Documenting with Runtime Shape Validation",
    "section": "The Shape Suffix Convention",
    "text": "The Shape Suffix Convention\nWhen writing PyTorch or NumPy code, it’s common to use naming conventions that indicate tensor shapes, as Noam Shazeer explains in his Medium post:\n\n“When known, the name of a tensor should end in a dimension-suffix composed of those letters, e.g. input_token_id_BL for a two-dimensional tensor with batch and length dimensions.”\n\nThis makes code self-documenting. Looking at query_BLHK, you immediately know it’s a 4D tensor with batch, length, heads, and key dimensions."
  },
  {
    "objectID": "posts/sizecheck.html#from-convention-to-validation",
    "href": "posts/sizecheck.html#from-convention-to-validation",
    "title": "Sizecheck: Making Tensor Code Self-Documenting with Runtime Shape Validation",
    "section": "From Convention to Validation",
    "text": "From Convention to Validation\nSizecheck verifies that your tensors actually have the shapes you expect them to have. By analyzing your function’s syntax tree, it automatically injects shape checks wherever you use suffixed variable names. Just prefix your function with @shapecheck:\nimport torch\nfrom shapecheck import shapecheck\n\n@shapecheck\ndef attention(query_BLH, key_BLH, value_BLH):\n    # Automatic validation: all tensors must be 3D with matching B,L dimensions\n    scores_BLL = torch.matmul(query_BLH, key_BLH.transpose(-2, -1))\n    weights_BLL = torch.softmax(scores_BLL, dim=-1)\n    output_BLH = torch.matmul(weights_BLL, value_BLH)\n    return output_BLH\nWhen shapes don’t match, ShapeCheck produces a clear error message describing the discrepency. For example:\nq = torch.randn(2, 10, 64)\nk = torch.randn(2, 12, 64)\nv = torch.randn(2, 10, 64)\nresult = attention(q, k, v)\nAssertionError: Shape mismatch for key_BLH dimension L: expected 10 (from query_BLH), got 12\nThe magic happens through AST transformation. ShapeCheck parses your function, identifies shape-annotated variables, and injects validation code automatically. You write clean, readable code with meaningful names, and get bulletproof shape checking for free.\nAdditionally, shape dimensions are stored as local variables within each function. If local variable is named score_BLL, for example, then the variables B and L will automatically be assigned its first and second shape indices."
  },
  {
    "objectID": "posts/sizecheck.html#available-in-julia-too",
    "href": "posts/sizecheck.html#available-in-julia-too",
    "title": "Sizecheck: Making Tensor Code Self-Documenting with Runtime Shape Validation",
    "section": "Available in Julia too!",
    "text": "Available in Julia too!\nThe Julia version is called SizeCheck.jl. It’s available on GitHub and can be installed via Pkg.add(\"SizeCheck\")."
  },
  {
    "objectID": "posts/kalman/kalman.html",
    "href": "posts/kalman/kalman.html",
    "title": "Mapping with Gaussian Conditioning",
    "section": "",
    "text": "using LinearAlgebra, SparseArrays, FillArrays, Rotations, RecursiveArrayTools, ForwardDiff, DiffResults, LazyArrays, CairoMakie\nCairoMakie.enable_only_mime!(\"png\")\nFor a robot to navigate autonomously, it needs to learn the locations of any potential obsticles around it. One of the standard ways to do this is with an algorithm known as EKF-Slam. Slam stands for “simultaneous localization and mapping”, as the algorithm must simultaneously find out where the robot is (localization) and where the obstacles are (mapping). The “EKF” part refers to the “extended Kalman filter”, which is just a fancy name for Gaussian conditioning with Taylor approximations. The idea goes as follows:\nDescribe the robot by its position coordinates \\(u \\in \\mathbb{R}^2\\). Assume it has a sensor that gives noisy measurements \\(Y\\) of the displacement to an obstacle at location \\(v \\in \\mathbb{R}^2\\). Specifically, assume \\(Y \\sim \\mathcal{N}(v - u, \\Sigma_2)\\).\nLet our uncertainty about the locations \\(u\\) and \\(v\\) be jointly be described by the random variable \\(X \\sim \\mathcal{N}(\\mu, \\Sigma_1)\\) in \\(\\mathbb{R}^4\\). Given the observation \\(Y\\), we’d like to find the posterior distribution over \\(X\\).\nConditioning Gaussians is easier in the natural paramteterization. Instead of describing distributions with a mean \\(\\mu\\) and covariance \\(\\Sigma\\), we describe them with a precision \\(\\Lambda\\) and information vector \\(\\Lambda \\mu\\). In this parameterization, say \\(X \\sim \\mathcal{N}(\\Lambda_1, \\Lambda_1\\mu_1)\\) and \\(Y \\sim \\mathcal{N}(\\Lambda_2, \\Lambda_2 (Ax  + b))\\) where \\(A\\) is a linear transformation. Then by Bayes’ rule, a little algebra tells us that \\[\nX | y \\sim \\mathcal{N}(\\Lambda_1\\mu + A^T\\Lambda_2(y - b), \\Lambda_1 + A^T\\Lambda_2A)\n\\]\nWhen \\(X\\) describes robot and obstacle locations and \\(Y\\) describes noisy displacement observations as above, we get the posterior natural parameters \\[\n(\\Sigma_1^{-1} \\mu + A^T\\Sigma_2^{-1}Y, \\Sigma_1^{-1} + A^T\\Sigma_2^{-1}A)\n\\] where \\(A = \\begin{bmatrix} -I & I \\end{bmatrix}\\).\nWe also need to update the distribution for \\(X\\) when the robot moves. Say we know that the robot’s position was displaced by some \\(\\Delta \\sim \\mathcal{N}(\\delta, \\Sigma_3)\\). After this displacement\n\\[\nX \\sim \\mathcal{N}\\left(\\mu + \\begin{bmatrix} \\delta  0 \\end{bmatrix}, \\Sigma_1 + \\begin{bmatrix} \\Sigma_3 &   & 0 \\end{bmatrix}\\right)\n\\]\nBy repeatedly updating our distribution over \\(X\\) in response to observations \\(Y\\) and movements \\(\\Delta\\), we can track the likely position of the robot and the obstacle over time."
  },
  {
    "objectID": "posts/kalman/kalman.html#adding-a-heading",
    "href": "posts/kalman/kalman.html#adding-a-heading",
    "title": "Mapping with Gaussian Conditioning",
    "section": "Adding a Heading",
    "text": "Adding a Heading\nTo make this toy example slightly more realistic, let us also give the robot a heading angle \\(\\theta\\). Assume now that the sensor measurement above is rotated \\(-\\theta\\) degrees, so that \\(Y = f(X) + \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma_2)\\), \\(f(x) = R_x(x_2 - x_1)\\), \\(x_1\\) and \\(x_2\\) refer to the robot and obstacle coordinate components of \\(x\\) respectively, and \\(R_x\\) is the corresponding rotation matrix. Although \\(f\\) isn’t linear, we can use the Taylor approximation of \\(f\\) about \\(\\mu\\) to pretend it is. \\[\nf(x) = f(\\mu + (x - \\mu)) \\approx f(\\mu) + \\nabla f(\\mu)(x - \\mu)\n\\] This makes \\(Y\\) a linear transformation of \\(X\\), so we can continue to use the standard Gaussian conditioning formula. We get natural parameters\n\\[\n(\\Sigma_1^{-1} \\mu + J^T\\Sigma_2^{-1}(Y - b), \\Sigma_1^{-1} + J^T\\Sigma_2^{-1}J)\n\\] where \\(J = \\nabla f(\\mu)\\) and \\(b = f(\\mu) - J \\mu\\). This expression naturally extends to handling observations for multiple obstacles.\nThis is the EKF-Slam algorithm in a nutshell. With the math out of the way, let’s get to some code."
  },
  {
    "objectID": "posts/Frequentist_Sample_Size_Estimation.html",
    "href": "posts/Frequentist_Sample_Size_Estimation.html",
    "title": "Frequentist Sample Size Estimation",
    "section": "",
    "text": "In the previous post, I showed a Bayesian method of sample size estimation for A/B/n testing. This post goes over the more conventional frequentist method.\nAs before, here’s the context. Say we’re trying to test which variant of an email message generates the highest response rate. We consider \\(k\\) different messages and send out \\(n\\) emails for each message. After we wait for responses, we should be able to tell which message yielded the highest response rate as long as we set \\(n\\) high enough. But we generally can’t send out too many messages: say we’re capped at \\(N\\) total. How do we choose the highest \\(k\\) that still allows us to confidently pick which message got the highest response rate?"
  },
  {
    "objectID": "posts/Frequentist_Sample_Size_Estimation.html#frequentist-two-arm-estimation",
    "href": "posts/Frequentist_Sample_Size_Estimation.html#frequentist-two-arm-estimation",
    "title": "Frequentist Sample Size Estimation",
    "section": "Frequentist Two-Arm Estimation",
    "text": "Frequentist Two-Arm Estimation\nIf we only have two messages (or ‘arms’ in the standard terminology), we might naively use a t-test to see if the observed difference in mean response rates \\(\\hat{\\delta}\\) was statistically significant. Let \\(\\hat{\\sigma}\\) be the pooled standard deviation. Then if the true difference in mean response rates \\(\\delta\\) is zero (the null hypothesis), \\(t = \\frac{\\sqrt{n}\\hat{\\delta}}{\\sqrt{2}\\hat{\\sigma}}\\) will be T distributed with \\(2n-2\\) dof. To get a false positive rate of \\(\\alpha\\), we can reject the null hypothesis when \\(t &gt; t_{1-\\alpha}\\) where \\(t_{1-\\alpha}\\) gives the \\(1-\\alpha\\) quantile of the Student’s t distribution. Under the alternative hypothesis that \\(\\delta = \\delta_0 &gt; 0\\), \\(t\\) will actually come from a non-central t distribution (ignoring the differences in arm variance). We can see this because \\(\\sqrt{n}(\\hat{\\delta} - \\delta_0)/\\sqrt{2}\\sigma\\) comes from a standard normal distribution, so \\(\\sqrt{n}\\hat{\\delta}/\\sqrt{2}\\sigma\\) is normal with mean \\(\\delta_0 \\sqrt{n}\\). This means that the power of our test will be the survival function of a \\(2n-2\\) degree non-central T distribution with noncentrality parameter \\(\\delta_0 \\sqrt{n}\\) evaluated at \\(t_{1-\\alpha}\\). We can find the number of samples \\(n\\) that produces a desired power \\(\\beta\\) with a simple binary search. This calculation is implemented within the statsmodels library as follows:\n\nfrom statsmodels.stats import power\nfrom scipy.optimize import brentq\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_theme()\n\n\npowers = [0.8, 0.85, 0.9]\n\n\nfreq_df_t = pd.concat([pd.DataFrame({\n    'samples': [2 * power.tt_ind_solve_power(effect_size=delta, alpha=alpha, power=p, alternative='larger') for p in powers],\n    'power': powers, 'delta': delta, 'alpha': alpha})\n    for delta in [0.01, 0.015, 0.02]\n    for alpha in [0.2, 0.15, 0.1]], ignore_index=True)\n\n\nfor ax in sns.relplot(data=freq_df_t, x='power', y='samples', hue='delta', col='alpha', kind='line').axes.ravel():\n    ax.set_xticks(powers)\n\n\n\n\n\n\n\n\nThis suggests we’d need a truly massive number of samples! Fortunately, things aren’t as bad as they seem. We can use a Z-test instead.\nSay response rates are \\(\\theta_0\\) and \\(\\theta_1\\) with difference \\(\\delta\\). The difference of observed arm means \\(\\hat{\\delta}\\) will be approximately normal around \\(\\delta\\) with variance \\(\\frac{\\theta_0(1-\\theta_0) + \\theta_1(1-\\theta_1)}{n}\\). Let \\(\\sigma_0^2 =2\\theta_0(1-\\theta_0)\\) and \\(\\sigma_1^2 =\\theta_0(1-\\theta_0) + \\theta_1(1-\\theta_1)\\). if \\(\\delta = 0\\) and we reject when \\(\\hat{\\delta} &gt; \\frac{\\sigma_0}{\\sqrt{n}} z_{1 - \\alpha}\\), we’ll have a false positive rate of \\(\\alpha\\). If \\(\\delta = \\delta_0\\), on the other hand, then \\(\\hat{\\delta} - \\delta_0\\) will be normal with variance \\(\\sigma_1^2/n\\), so rejecting if \\(\\hat{\\delta} &gt; \\delta_0 + \\frac{\\sigma_1}{\\sqrt{n}} z_{1 - \\beta}\\) would give us power \\(\\beta\\). We can choose \\(n\\) so that these critical values are the same.\nA little algebra tells us that \\(n = \\frac{(\\sigma_0 z_{1 - \\alpha} - \\sigma_1 z_{1 - \\beta})^2}{\\delta_0^2}\\). Once again, this is already implemented in statsmodels.\n\nt = 0.04\n\n\nfreq_df_z = pd.concat([pd.DataFrame({\n    'samples': [2 * power.normal_sample_size_one_tail(d, p, alpha,\n                        std_null=np.sqrt(2 * t * (1-t)),\n                        std_alternative=np.sqrt(t * (1-t) + (t+d)*(1 - (t+d))))\n                for p in powers],\n    'power': powers, 'delta': d, 'alpha': alpha})\n    for d in [0.01, 0.015, 0.02]\n    for alpha in [0.2, 0.15, 0.1]], ignore_index=True)\n\n\nfor ax in sns.relplot(data=freq_df_z, x='power', y='samples', hue='delta', col='alpha', kind='line').axes.ravel():\n    ax.set_xticks(powers)"
  },
  {
    "objectID": "posts/Frequentist_Sample_Size_Estimation.html#frequentist-multi-arm-estimation",
    "href": "posts/Frequentist_Sample_Size_Estimation.html#frequentist-multi-arm-estimation",
    "title": "Frequentist Sample Size Estimation",
    "section": "Frequentist Multi-Arm Estimation",
    "text": "Frequentist Multi-Arm Estimation\nWith more than two arms, we end up testing more than one null hypothesis. Specifically, we’ll want to test the null hypothesis that arm \\(i\\) has a higher mean than arm \\(j\\) for every pair arms \\(i,j\\). We’ll reject this hypothesis when \\(\\hat{\\delta_{i,j}} &gt; c\\) for a specific value of \\(c\\). Our family-wise false positive rate \\(1 - \\alpha\\) must now cover the probability that we mistakenly reject any of these tests. If \\(\\hat{\\delta}_{i,j} &gt; c\\) for any \\(i,j\\), it will also be true that \\(\\delta^\\star &gt; c\\) where \\(\\delta^\\star\\) is the largest observed difference in means between any pair of arms. We can show that \\(\\delta^\\star\\) follows a known distribution.\nIn general, if \\(F\\) is a distribution’s cdf, the joint probability that each of \\(k\\) independent draws from the distribution are between \\(a\\) and \\(a+\\delta\\) is given by \\((F(a + \\delta) - F(a))^k\\). This makes the density at \\(a\\) given by \\(k f(a)(F(a + \\delta) - F(a))^{k-1}\\). We want to integrate over all \\(a\\): \\(\\int_{-\\infty}^\\infty k f(a)(F(a + \\delta) - F(a))^{k-1}\\, da\\).\nThis distribution over the largest difference within \\(k\\) samples is known as the Studentized range distribution when \\(F\\) is the cdf of a Student’s t distribution. This is built into scipy! And because a normal distribution is just a Student’s t distribution with infinite degrees of freedom, this covers a largest difference between normal samples as well, which I’ll refer to as a range distribution.\nSay each of the per-arm differences are normal with approximately the same variance \\(\\sigma^2\\). We can reject the null hypothesis that all the inter-arm differences are zero when \\(\\delta^\\star &gt; \\frac{\\sigma}{\\sqrt{n}} q_{1-\\alpha}\\) to get a family-wise false positive rate of \\(\\alpha\\), where \\(q_{1-\\alpha}\\) gives the \\(1-\\alpha\\) quantile of the range distribution. If we also reject when \\(\\delta^\\star &gt; \\delta_0 +  \\frac{\\sigma}{\\sqrt{n}} q_{1-\\beta}\\), we’d have power \\(\\beta\\) when the true maximum difference is \\(\\delta_0\\). Once again, we can solve for when these critical values are equal to find the necessary number of samples; unfortunately the algebra here isn’t built into statsmodels, so we’ll have to do it ourselves.\n\ndef range_sample_size(k, theta=0.06, delta=0.015, alpha=0.90, beta=0.8):\n    q0 = stats.studentized_range.ppf(1 - alpha, k, np.inf)\n    q1 = stats.studentized_range.ppf(1 - beta, k, np.inf)\n    theta1 = theta + delta\n    return k * theta1 * (1 - theta1) * ((q1 - q0) / delta)**2\n\n\nks = range(2, 5)\n\n\nfreq_df_q = pd.concat([pd.DataFrame({\n    'samples': [range_sample_size(k, delta=delta, theta=theta, alpha=alpha) for k in ks],\n    'k': ks, 'delta': delta, 'theta': theta, 'alpha': alpha})\n    for delta in [0.01, 0.015, 0.02]\n    for theta in [0.04, 0.05]\n    for alpha in [0.2, 0.15, 0.1]], ignore_index=True)\n\n\nfor ax in sns.relplot(data=freq_df_q, x='k', y='samples', hue='delta', col='theta', row='alpha', kind='line').axes.ravel():\n    ax.set_xticks(list(ks))"
  },
  {
    "objectID": "posts/clustering/Clustering.html",
    "href": "posts/clustering/Clustering.html",
    "title": "Finding Common Topics",
    "section": "",
    "text": "How do you find thematic clusters in a large corpus of text documents? The techniques baked into sklearn (e.g. nonnegative matrix factorization, LDA) give you some intuition about common themes. But contemporary NLP has largely moved on from bag-of-words representations. We can do better with some transformer models!\nFor demonstration purposes, I’ll use a few categories from the standard 20-newsgroups dataset. Ideally, we should be able to recover the four categories in the dataset (atheism, computer graphics, space and religion).\n\nimport numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.cluster import KMeans\nfrom pydantic import BaseModel, Field\nfrom transformers import AutoTokenizer, AutoModel\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.messages import HumanMessage, SystemMessage\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom IPython.display import Markdown\n\n\ncategories = [\n    \"alt.atheism\",\n    \"talk.religion.misc\",\n    \"comp.graphics\",\n    \"sci.space\",\n]\n\ndataset = fetch_20newsgroups(\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    subset=\"all\",\n    categories=categories,\n    shuffle=True,\n    random_state=42,\n)\n\nSome of the documents in the dataset are only a few words; I only want to deal with documents that are least a couple hundred characters.\n\nX = np.array(list(filter(lambda x: len(x) &gt; 200, (d.strip() for d in dataset.data))))\n\nFirst, I’ll map each document to its embedding using the all-MiniLM BERT variant.\n\nminilm_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nminilm = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('mps')\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element contains all embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n        token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded,\n                     1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\ndef get_embeddings(X):\n    loader = DataLoader(X, batch_size=16)\n    embeddings = []\n    for batch in loader:\n        toks = minilm_tokenizer(batch, padding=True, truncation=True,\n                                return_tensors='pt')\n        with torch.no_grad():\n            model_output = minilm(**toks.to('mps'))\n            result = F.normalize(mean_pooling(model_output,\n                                              toks['attention_mask']), p=2, dim=1)\n            embeddings.append(result.cpu())\n    return torch.cat(embeddings)\n\nNext, I’ll cluster the embeddings with the standard k-means algorithm. There’s far more sophisticated clustering techniques in sklearn, but this should be sufficient for the toy problem.\n\ndef get_clusters(embeddings):\n    neural_kmeans = KMeans(n_clusters=4, n_init=25)\n    neural_kmeans.fit(embeddings)\n    docs_per_label = pd.DataFrame({'labels': neural_kmeans.labels_}).value_counts()\n    return neural_kmeans, docs_per_label\n\nFinally, I’ll take a random set of documents closest to the center of each cluster and ask Qwen to find a title for the collection.\n\ndef top_per_cluster(X, embeddings, kmeans, k=25, m=8):\n    return [np.random.choice(\n        X[kmeans.labels_ == i][np.argsort(((embeddings[kmeans.labels_ == i]\n                                                - c)**2).sum(axis=-1))[:k]], m)\n        for i, c in enumerate(kmeans.cluster_centers_)]\n\n\nllama = ChatOllama(model=\"mistral:7b\", temperature=0)\n\nI’ll let the LLM contemplate common themes to itself before deciding on a title. We can require that the results get packaged together in a structured output format.\n\nclass SampleAnalysis(BaseModel):\n    analysis: str = Field(description='Analysis of the texts.')\n    category: str = Field(description='Category of the cluster.')\n\n\ndef llama_summarize(strs):\n    prompt = [SystemMessage(\"\"\"\nYour task is to understand why the given documents were assigned to the same cluster.\n- First analyze the documents in the cluster for common topics.\n- Then, propose a short category name for the cluster containing these documents based on the analysis.\"\"\")]\n    prompt.extend(strs)\n    return llama.with_structured_output(SampleAnalysis).invoke(prompt)\n\nLet’s try it out!\n\ndef get_topics(X):\n    embeddings = get_embeddings(X)\n    neural_kmeans, docs_per_label = get_clusters(embeddings)\n    top_embeddings = top_per_cluster(X, embeddings, neural_kmeans)\n    results = [llama_summarize([a for a in t]) for t in top_embeddings]\n    return pd.DataFrame({\n        'category': [r.category for r in results],\n        'n_docs': [int(docs_per_label[a]) for a in range(len(docs_per_label))]\n    }).sort_values(by='n_docs', ascending=False)\n\n\nMarkdown(get_topics(X).to_markdown())\n\n\n\n\n\ncategory\nn_docs\n\n\n\n\n3\nSpace Industry News\n710\n\n\n0\nSoftware Development & Graphics\n707\n\n\n2\nAtheism and Religion Debate\n631\n\n\n1\nSocial Transformation and Ethics\n609\n\n\n\n\n\nSounds about right!"
  },
  {
    "objectID": "posts/finite_basis_gps/finite_basis_gps.html",
    "href": "posts/finite_basis_gps/finite_basis_gps.html",
    "title": "Finite Basis Gaussian Processes",
    "section": "",
    "text": "using KernelFunctions,LinearAlgebra, AbstractGPs, Random\nusing BenchmarkTools\n\n\nPrecompiling packages...\n    705.7 ms  ✓ JSON\n    856.7 ms  ✓ BenchmarkTools\n  2 dependencies successfully precompiled in 2 seconds. 13 already precompiled.\nPrecompiling packages...\n    557.9 ms  ✓ QuartoNotebookWorkerJSONExt (serial)\n  1 dependency successfully precompiled in 1 seconds\nBy Mercer’s theorem, every positive definite kernel \\(k(x, y) : \\mathcal{X} \\to \\mathcal{X} \\to \\mathbb{R}\\) that we might want to use in a Gaussian Process corresponds to some inner product \\(\\langle \\phi(x), \\phi(y) \\rangle\\), where \\(\\phi : \\mathcal{X} \\to \\mathcal{V}\\) maps our inputs into some other space. For many kernels (like the venerable RBF), this space is infinite dimensional, and we can’t work with it directly. But when it’s finite dimensional (in say \\(d\\) dimensions), we can! This lets us avoid the usual \\(O(n^3)\\) scaling for Gaussian process regression, getting \\(O(nd+d^3)\\) instead.\nimport AbstractGPs: AbstractGP, FiniteGP\nimport Statistics\nstruct FiniteBasis &lt;: KernelFunctions.SimpleKernel end\nWe can define a finite dimensional kernel in Julia using the KernelFunctions library. The library assumes our kernel k has the form k(x,y) = kappa(metric(x,y)), and lets us fill in the definitions for kappa and metric.\nKernelFunctions.kappa(::FiniteBasis, d::Real) = d\nKernelFunctions.metric(::FiniteBasis) = KernelFunctions.DotProduct()\nWe will use the weight space view of Gaussian processes, which interprets GP regression as Bayesian linear regression. We assume that there is a weight vector \\(w : \\mathcal{V}\\) with prior \\(\\mathcal{N}(0, I)\\), and that \\(y \\sim \\mathcal{N}(X w, I)\\), where \\(X\\) is the matrix for which row \\(i\\) is given by \\(\\phi(x_i)\\). The posterior over \\(w\\) remains Gaussian with precision \\(\\Lambda = I + X^T X\\) and mean \\(\\mu = \\Lambda^{-1} X^T y\\). To make a prediction at \\(x_*\\), we simply find \\(\\langle \\phi(x_*), w \\rangle\\).\nOn the face of it, this seems like a very different generative model than the traditional depiction of Gaussian processes in which the observations \\(y\\) are noisy versions of the function values \\(f\\), which are all jointly Gaussian with a covariance matrix given by the associated kernel. But with a little algebra, one can show that the posterior over \\(f(x_*) = \\langle \\phi(x_*), w \\rangle\\) in the weight space view is the same as the posterior over \\(f(x_*)\\) is the traditional function-space view.\nFirst, we can marginalize out \\(w\\) to find that\n\\[\nf(x_*) | y \\sim \\mathcal{N}(X_* \\mu, X_* \\Lambda^{-1} X_*^T)\n\\] The mean expands to \\(X_*(I + X^T X)^{-1} X^T y\\) and the variance expands to \\(X_*(I + X^T X)^{-1}X_*^T\\).\nNow, we can use the Woodbury Matrix Identity, which says that \\[\n(I + X^TX)^{-1} = I - X^T(I + XX^T)^{-1}X\n\\] This lets the mean simplify to \\(X_*X^T (XX^T + I)^{-1}y\\) and the variance simplify to \\(X_*X_*^T -X_*X^T(XX^T + I)^{-1}XX_*^T\\). Letting \\(XX^T = K\\), we recover the familiar function space representation of Gaussian process. See the first chapter of the Rasmussen book for a more detailed derivation.\nstruct DegeneratePosterior{P,T,C} &lt;: AbstractGP\n    prior::P\n    w_mean::T\n    w_prec::C\nend\nweight_form(A::KernelFunctions.ColVecs) = A.X'\n\nweight_form (generic function with 1 method)\nweight_form(A::KernelFunctions.RowVecs) = A.X\n\nweight_form (generic function with 2 methods)\nfunction Statistics.mean(f::DegeneratePosterior, x::AbstractVector)\n    w = f.w_mean\n    X = weight_form(x)\n    X * w\nend\nfunction AbstractGPs.posterior(fx::FiniteGP{GP{M, B}}, y::AbstractVector{&lt;:Real}) where {M, B &lt;: FiniteBasis}\n    kern = fx.f.kernel\n    δ = y - mean(fx)\n    X = weight_form(fx.x)\n    X_prec = X' * inv(fx.Σy)\n    Λμ = X_prec * y\n    prec = cholesky(I + Symmetric(X_prec * X))\n    w = prec \\ Λμ\n    DegeneratePosterior(fx.f, w, prec)\nend\nfunction Statistics.cov(f::DegeneratePosterior, x::AbstractVector)\n    X = weight_form(x)\n    AbstractGPs.Xt_invA_X(f.w_prec, X')\nend\nfunction Statistics.cov(f::DegeneratePosterior, x::AbstractVector, y::AbstractVector)\n    X = weight_form(x)\n    Y = weight_form(y)\n    AbstractGPs.Xt_invA_Y(X', f.w_prec, Y')\nend\nfunction Statistics.var(f::DegeneratePosterior, x::AbstractVector)\n    X = weight_form(x)\n    AbstractGPs.diag_Xt_invA_X(f.w_prec, X')\nend\nfunction Statistics.rand(rng::AbstractRNG, f::DegeneratePosterior, x::AbstractVector)\n    w = f.w_mean\n    X = weight_form(x)\n    X * (f.w_prec.U \\ randn(rng, length(x)))\nend\nWe can compare the results of this optimized implementation with the standard posterior implementation to ensure that the two agree on the output.\nx = rand(2, 2000)\n\n2×2000 Matrix{Float64}:\n 0.351496   0.201416  0.437436  0.666158  …  0.0625915  0.731095  0.382785\n 0.0576307  0.698233  0.582991  0.465895     0.899785   0.260989  0.554423\ny = sin.(norm.(eachcol(x)))\n\n2000-element Vector{Float64}:\n 0.3487055324991339\n 0.6644094915138241\n 0.66601590805138\n 0.7262912901139835\n 0.3997721826491429\n 0.8130975274946121\n 0.3867715265954484\n 0.6915444950745628\n 0.7064190858519713\n 0.687646536276883\n ⋮\n 0.7421488774224382\n 0.6884915867547636\n 0.8886306385116652\n 0.6716576846196286\n 0.8250049592784389\n 0.5242250412111538\n 0.7845434358940825\n 0.7006320687866278\n 0.6239034098291798\nkern = FiniteBasis()\n\nFiniteBasis()\nf = GP(kern)\n\nGP{ZeroMean{Float64}, FiniteBasis}(ZeroMean{Float64}(), FiniteBasis())\nfx = f(x, 0.001)\n\nAbstractGPs.FiniteGP{AbstractGPs.GP{AbstractGPs.ZeroMean{Float64}, Main.Notebook.FiniteBasis}, KernelFunctions.ColVecs{Float64, Matrix{Float64}, SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}, LinearAlgebra.Diagonal{Float64, FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}}}}(\nf: GP{ZeroMean{Float64}, FiniteBasis}(ZeroMean{Float64}(), FiniteBasis())\nx: SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}[[0.35149641083221217, 0.05763068729679277], [0.2014163045582935, 0.6982330113172243], [0.43743631452871634, 0.5829913842710026], [0.6661578075039322, 0.465895058162053], [0.29424765897527083, 0.28733242404477044], [0.6455059374287724, 0.6962644211821701], [0.24798696009517884, 0.3101824940598763], [0.6601860876479605, 0.3837676112030852], [0.28756466003278713, 0.7298156296435593], [0.4234423824785205, 0.628989944148036]  …  [0.7721172196470107, 0.3562414055654409], [0.6620141097416459, 0.5109659959662884], [0.5696028513670078, 0.5022467251213828], [0.9082232376802148, 0.6105194028256506], [0.27720475817422985, 0.6822809807126444], [0.9668046822131043, 0.08122834020244951], [0.3188134615509236, 0.4503848915058404], [0.0625915040682905, 0.8997850871147113], [0.7310949402838798, 0.2609892975778647], [0.38278458559490836, 0.5544229330341326]]\nΣy: Diagonal(Fill(0.001, 2000))\n)\nx2 = ColVecs(rand(2, 2000))\n\n2000-element ColVecs{Float64, Matrix{Float64}, SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}:\n [0.7714753598208173, 0.17451404189323316]\n [0.42425908263840695, 0.2124601416173687]\n [0.07272271000919628, 0.5482454543064469]\n [0.7059731358753187, 0.7259727573183643]\n [0.11460956633870045, 0.24344830224835445]\n [0.8300383940970001, 0.2790143015230281]\n [0.4477432799750962, 0.4551113837179096]\n [0.058584388428317924, 0.17827802331726972]\n [0.7793071586655852, 0.21791492416561375]\n [0.13358837960391345, 0.833479653115069]\n ⋮\n [0.7989244260415876, 0.6185838381828523]\n [0.7023053355322493, 0.7796220515234427]\n [0.8638621277598659, 0.061635729084563606]\n [0.8171989565611498, 0.8240352874225825]\n [0.6105743997173582, 0.10404049957766726]\n [0.3990949140503428, 0.9542567830909576]\n [0.47200770576798967, 0.922900672462661]\n [0.19074503951635113, 0.3108299131058804]\n [0.1447563133315517, 0.7373452805826761]\nopt_m, opt_C = @btime mean_and_cov(posterior($fx, $y)($x2))\n\n  5.156 ms (28 allocations: 61.16 MiB)\n\n\n([0.6059480706502862, 0.40803777403564323, 0.3986292483384698, 0.9181528858346273, 0.22970844859000528, 0.7105468977081874, 0.5788983199916785, 0.15199134833403666, 0.6388331041719346, 0.6207664866757735  …  0.8070315642474819, 0.9086944609323611, 0.9502575404488182, 0.5926030564142932, 1.0523308290593398, 0.4576866093448145, 0.8682977878068852, 0.8948382509092377, 0.3217197069238477, 0.566180008870297], [1.4379522182192942e-6 6.316651977316091e-7 … -1.0093037669316541e-8 -6.925915889297121e-7; 6.316651977316091e-7 3.0593138536360454e-7 … 6.094832850339773e-8 -1.321418708611357e-7; … ; -1.0093037669316541e-8 6.094832850339773e-8 … 1.5031280378064394e-7 4.003327712855696e-7; -6.925915889297121e-7 -1.321418708611357e-7 … 4.003327712855696e-7 1.3745595946685778e-6])\nTo compare against the implementation that uses a function-space perspective, we’ll use a bit of a hack: by adding a ZeroKernel to our FiniteBasis kernel, we get a kernel for which our custom posterior method won’t be called.\nfx2 = GP(kern + ZeroKernel())(x, 0.001)\n\nAbstractGPs.FiniteGP{AbstractGPs.GP{AbstractGPs.ZeroMean{Float64}, KernelFunctions.KernelSum{Tuple{Main.Notebook.FiniteBasis, KernelFunctions.ZeroKernel}}}, KernelFunctions.ColVecs{Float64, Matrix{Float64}, SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}, LinearAlgebra.Diagonal{Float64, FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}}}}(\nf: GP{ZeroMean{Float64}, KernelSum{Tuple{FiniteBasis, ZeroKernel}}}(ZeroMean{Float64}(), Sum of 2 kernels:\n    FiniteBasis()\n    Zero Kernel)\nx: SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}[[0.35149641083221217, 0.05763068729679277], [0.2014163045582935, 0.6982330113172243], [0.43743631452871634, 0.5829913842710026], [0.6661578075039322, 0.465895058162053], [0.29424765897527083, 0.28733242404477044], [0.6455059374287724, 0.6962644211821701], [0.24798696009517884, 0.3101824940598763], [0.6601860876479605, 0.3837676112030852], [0.28756466003278713, 0.7298156296435593], [0.4234423824785205, 0.628989944148036]  …  [0.7721172196470107, 0.3562414055654409], [0.6620141097416459, 0.5109659959662884], [0.5696028513670078, 0.5022467251213828], [0.9082232376802148, 0.6105194028256506], [0.27720475817422985, 0.6822809807126444], [0.9668046822131043, 0.08122834020244951], [0.3188134615509236, 0.4503848915058404], [0.0625915040682905, 0.8997850871147113], [0.7310949402838798, 0.2609892975778647], [0.38278458559490836, 0.5544229330341326]]\nΣy: Diagonal(Fill(0.001, 2000))\n)\nm, C = @btime mean_and_cov(posterior($fx2, $y)($x2))\n\n  263.622 ms (71 allocations: 458.03 MiB)\n\n\n([0.6059480706526301, 0.4080377740373251, 0.39862924833721536, 0.9181528858364345, 0.22970844858986084, 0.7105468977106284, 0.5788983199925042, 0.15199134833355288, 0.6388331041741822, 0.620766486675052  …  0.807031564248291, 0.9086944609343846, 0.9502575404517302, 0.5926030564144185, 1.0523308290598834, 0.4576866093462968, 0.8682977878042948, 0.894838250912926, 0.3217197069247959, 0.5661800088709015], [1.4379522189643675e-6 6.31665197814435e-7 … -1.00930376278896e-8 -6.925915888300604e-7; 6.31665197814435e-7 3.059313857255449e-7 … 6.094832855074728e-8 -1.321418708621902e-7; … ; -1.00930376278896e-8 6.094832855074728e-8 … 1.5031280348287792e-7 4.003327714330318e-7; -6.925915888300604e-7 -1.321418708621902e-7 … 4.003327714330318e-7 1.3745595951474295e-6])\nmax(maximum(abs.(opt_C .- C)), maximum(abs.(opt_m .- m)))\n\n6.212141911987601e-12\nOur optimized technique produces the same results!"
  },
  {
    "objectID": "posts/finite_basis_gps/finite_basis_gps.html#random-fourier-features",
    "href": "posts/finite_basis_gps/finite_basis_gps.html#random-fourier-features",
    "title": "Finite Basis Gaussian Processes",
    "section": "Random Fourier Features",
    "text": "Random Fourier Features\nOne application of this technique is the Random Fourier Features approximation. By Bochner’s theorem, every kernel of the form \\(k(x,y) = f(x-y)\\) for some \\(f\\) can be expressed in the Fourier basis as \\(f(x-y) = E e^{i\\omega (x-y)}\\), where the distribution from which \\(\\omega\\) is sampled determines the kernel. A Monte Carlo estimate of this expectation is just \\(\\sum_{w_j} e^{i w_j x}e^{-i w_j y}\\), which is an inner product of features of the form \\(\\phi_j(x) = e^{i w_j x}\\). With some algebraic simplifications (see here for a good derivation) we can ignore the imaginary parts and express this as \\(\\phi_j(x)=(\\cos(w_j x), \\sin(w_j x))\\).\n\nbegin\nstruct RandomFourierFeature\n    ws::Vector{Float64}\nend\nRandomFourierFeature(kern::SqExponentialKernel, k::Int) = RandomFourierFeature(randn(k))\nfunction (f::RandomFourierFeature)(x)\n    Float64[cos.(f.ws .* x); sin.(f.ws .* x)] .* sqrt(2/length(f.ws))\nend\nend\n\n\nbegin\nFFApprox(kern::Kernel, k::Int) = FiniteBasis() ∘ FunctionTransform(RandomFourierFeature(kern, k))\nFFApprox(rng::AbstractRNG, kern::Kernel, k::Int) = FiniteBasis() ∘ FunctionTransform(RandomFourierFeature(rng, kern, k))\nend\n\nFFApprox (generic function with 2 methods)\n\n\nTo support other spectral densities besides the RBF, we could add constructors for RandomFourierFeature.\n\nrbf = SqExponentialKernel()\n\nSquared Exponential Kernel (metric = Distances.Euclidean(0.0))\n\n\n\nflat_x = rand(2000)\n\n2000-element Vector{Float64}:\n 0.8262485689043089\n 0.428468984864\n 0.345564372312792\n 0.6376102893364441\n 0.25288608513089594\n 0.42337735430117496\n 0.6965653681089846\n 0.6387935649325237\n 0.5341863343340341\n 0.8086038706781346\n ⋮\n 0.9640717549579014\n 0.1402547421925595\n 0.20175123213064217\n 0.22658244315688736\n 0.16899866224516824\n 0.3786632396773453\n 0.7167636570113126\n 0.4158869625613356\n 0.6282980646795796\n\n\n\nflat_x2 = rand(100)\n\n100-element Vector{Float64}:\n 0.8765314982311043\n 0.436856034220347\n 0.8668468831498979\n 0.9134469399631979\n 0.4870653556247635\n 0.6596646711472544\n 0.31423023225547797\n 0.6780304136075354\n 0.18625992923928625\n 0.9051183709542147\n ⋮\n 0.6289144203449463\n 0.9998425990601125\n 0.9487253381196158\n 0.8139396150194459\n 0.513503852026044\n 0.6373971066414789\n 0.706102962208211\n 0.32186760637075507\n 0.5846464209932682\n\n\n\nffkern = FFApprox(rbf, 100)\n\nFiniteBasis()\n    - Function Transform: Main.Notebook.RandomFourierFeature([-0.9605425137782779, -0.4556580662486187, -0.0462526551078979, -1.3515990868234025, -1.1252083933432082, 0.43463448941753513, 1.700977151608801, 0.9116832996013912, 1.6391244777495146, 0.29478489882492276, 0.26310896178970455, 0.16536976449367444, 0.41311033576224077, 0.8021076108404168, -0.36224564234194234, -0.45140419700107354, -0.8404017324605675, -0.32575945151579616, -0.980445461745031, 1.1698215477932925, 0.5842088364572025, 0.5384055093260036, 0.05178873825595889, -0.6626447165057271, 0.8679493446281221, 1.349329080293985, -0.16802647757149256, 0.4710928234668019, -1.8247581525220078, -0.8597030133670049, -1.333558768499329, 0.580130059459092, 0.21509456524203416, 0.7054271578607293, -1.2379096528957472, -1.2014813367066064, -1.0410065900406815, -1.9226600346721625, -1.8514215521777038, -0.8180452155965721, -0.10190309416921497, 0.562650195374764, 0.8293657133221493, -1.2990338378436075, -0.5170154571788326, -0.9151706246010864, 1.483232320473878, 0.9048628480763394, -0.4017464202857643, -0.2240300957866955, 0.6098831636271618, -0.7606263017808166, -0.3746306045271828, -0.6625287549558049, -0.5831191968088915, 0.22422978158669804, -0.8904231441360309, -0.18624629873664772, 0.23746264724234228, 0.6564796280956496, -1.6226828341283277, 1.4493913670528968, 0.3359940295548781, -0.3683601416301013, 0.3014031635517683, 0.6877853582263126, 1.4380518243642266, -1.4638617310812916, 0.02025369338300898, 0.2192953880487077, 0.012093451637238975, -1.3428510181223463, 1.6005651034910295, -0.12585405343545533, 0.29018716162710834, 1.0245781887241046, -0.1545578835682928, -2.8782130742163887, -2.415730601110451, -0.19570567975859263, 0.3526448080108851, 0.46849487818920266, 1.2904901273290066, 0.005491063031410227, 1.1388679799042067, -0.25914475868047765, -0.2906027587870364, 0.16352409586397848, -2.4151073557100693, 0.5233804354699256, -0.3073352202906987, 0.7708555590835786, 1.17631703035191, -0.2407438030463325, -0.9776252136075437, 0.9427453274536897, 1.060523393924405, -0.5703270787509798, -0.4411228783126022, 0.06212947146234058])\n\n\n\nff_m, ff_C = mean_and_cov(posterior(GP(ffkern)(flat_x, 0.001), y)(flat_x2))\n\n([0.6685153185817398, 0.6674623265753326, 0.6689004704276158, 0.6668306862111422, 0.6687821690838973, 0.6719040257021334, 0.6644888551700205, 0.6719995286557605, 0.6631313700981991, 0.6672408172789801  …  0.6720328747069289, 0.6716173079512373, 0.6615481461990385, 0.6648991489923901, 0.6705943634497089, 0.6694427157613063, 0.6717114496368595, 0.6720259681410425, 0.6646426842835353, 0.6709597278850197], [2.0694806650577443e-6 -4.316087447442385e-7 … -2.1791949644800468e-7 5.8330127616912364e-8; -4.316087447442385e-7 1.547964608939912e-6 … 1.1850965961901494e-6 1.1310200658254388e-6; … ; -2.1791949644800468e-7 1.1850965961901494e-6 … 1.5535339965576663e-6 3.601289015886522e-7; 5.8330127616912364e-8 1.1310200658254388e-6 … 3.601289015886522e-7 1.5508166835164213e-6])\n\n\n\nm2, C2 = mean_and_cov(posterior(GP(rbf)(flat_x, 0.001), y)(flat_x2))\n\n([0.6685490108977774, 0.6673803029543706, 0.6689414161301102, 0.6668293718448695, 0.6687040195592999, 0.6719280128240825, 0.6644774650667387, 0.6720350848340786, 0.6632388504062874, 0.6672484082791925  …  0.6720775015810432, 0.6716201964375159, 0.6614222732557664, 0.6648538281624496, 0.6706607135324703, 0.6693735452645342, 0.6717203031507779, 0.67207682201024, 0.6646240488616968, 0.6709313820106217], [2.0496280453984335e-6 -4.031442002450092e-7 … -2.077162950531175e-7 6.837665633696588e-8; -4.031442002450092e-7 1.5371062037649049e-6 … 1.1907212842388049e-6 1.112839862149606e-6; … ; -2.077162950531175e-7 1.1907212842388049e-6 … 1.5617916862008537e-6 3.5271860387986465e-7; 6.837665633696588e-8 1.112839862149606e-6 … 3.5271860387986465e-7 1.5458090074648164e-6])\n\n\n\nmax(maximum(abs.(m2 .- ff_m)), maximum(abs.(C2 .- ff_C)))\n\n0.0002836960175045533\n\n\nEven with only 100 samples, we get a pretty close approximation!"
  },
  {
    "objectID": "posts/regression_diagnostics.html",
    "href": "posts/regression_diagnostics.html",
    "title": "Diagnosing Lack of Independence in Exogenous Variables",
    "section": "",
    "text": "While performing linear regression with statsmodels, you might occasionally find that your exogenous variables aren’t independent, giving you a error about a singular matrix.\nTo figure out exactly which variables are colinear, I tend to use the following recipe:\n\nTake the SVD of the design matrix \\(X = QSV^T\\).\nFind a column of \\(V\\) that corresponds to a zero singular value.\nCheck which terms in our original formula correspond to the nonzero elements of \\(V\\). Usually there’s only a couple nonzero terms.\n\nFor posterity, I’ve reproduced the workflow below.\nm = dmatrix(formula, df)\nu, s, vh = np.linalg.svd(m)\nmisfits = (np.abs(vh[s &lt; 1e-8]) &gt; 1e-5)\nnp.array(m.design_info.column_names)[misfits[0]]"
  },
  {
    "objectID": "posts/ships/ships.html",
    "href": "posts/ships/ships.html",
    "title": "Classifying Ships with Gaussian Process Mixtures",
    "section": "",
    "text": "using DataFrames, PythonCall, CSV, Dates, DataFramesMeta, RecursiveArrayTools, StatsBase, CairoMakie, Distributions, SpecialFunctions, PDMats, LinearAlgebra, SizeCheck, KernelFunctions, AbstractGPs, LogExpFunctions, Random\nCairoMakie.enable_only_mime!(\"png\")\nI recently came across a dataset of container ship movement between Tallinn and Helsinki on Kaggle. In this notebook, we’ll try to classify whether a given ship’s trajectory seems similar to those of the container ships, or whether we’re looking at something else (perhaps a pirate).\nI’ll start by loading the ship tracking data, taking zscores of the latitude and longitude, and normalizing timing information for each trajectory. We can identify rows belonging to the same ship’s trajectory by the ship’s International Maritime Organization code (IMO) and its actual arrival time (ATA). For simplicity, we’ll just consider 2k samples of trajectories leaving Helsinki for the time being.\nfunction load_ship_data()\n    kagglehub = pyimport(\"kagglehub\")\n    ships_path = pyconvert(String, kagglehub.dataset_download(\"bobaaayoung/container-ship-data-collection\"))\n    raw_df = dropmissing(\n        CSV.read(joinpath(ships_path, \"tracking_db.csv\"), DataFrame,\n            dateformat=\"mm/dd/yyyy HH:MM\",\n            types=Dict(:updated =&gt; DateTime, :ata =&gt; DateTime), silencewarnings=true))\n\n    df = @chain raw_df begin\n        @subset(:long .&gt; 24)\n        @subset(:arrPort .== \"FIHEL\")\n        @select(:updated, :ata, :long, :lat, :imo)\n        @transform(:t = Minute.(:ata .- :updated), :long = zscore(:long), :lat = zscore(:lat))\n        @groupby :imo :ata\n        @transform(:nt = (:t .- minimum(:t)) ./ (maximum(:t) - minimum(:t)))\n        @subset((:nt .&gt; 0) .& (:nt .&lt; 1))\n    end\n\n    df, groupby(df, [:imo, :ata])\nend\n\nload_ship_data (generic function with 1 method)\ndf, groups = load_ship_data()\n\n\n(94944×7 DataFrame\n   Row │ updated              ata                  long       lat       imo    ⋯\n       │ DateTime             DateTime             Float64    Float64   Int64  ⋯\n───────┼────────────────────────────────────────────────────────────────────────\n     1 │ 2018-05-04T19:26:00  2018-04-05T21:26:00  -0.545055  -1.52547  936472 ⋯\n     2 │ 2018-05-04T19:29:00  2018-04-05T21:24:00  -0.615397  -1.47017  936472\n     3 │ 2018-05-04T19:33:00  2018-04-05T21:28:00  -0.806885  -1.34498  936472\n     4 │ 2018-05-04T19:36:00  2018-04-05T21:26:00  -0.918912  -1.28835  936472\n     5 │ 2018-05-04T19:37:00  2018-04-05T21:31:00  -0.974925  -1.25871  936472 ⋯\n     6 │ 2018-05-04T19:39:00  2018-04-05T21:30:00  -1.09086   -1.20032  936472\n     7 │ 2018-05-04T19:39:00  2018-04-05T21:26:00  -1.10128   -1.19501  936472\n     8 │ 2018-05-04T19:41:00  2018-04-05T21:26:00  -1.22112   -1.13661  936472\n   ⋮   │          ⋮                    ⋮               ⋮         ⋮         ⋮   ⋱\n 94938 │ 2019-12-03T20:06:00  2019-03-12T20:35:00   1.06891    1.07842  936472 ⋯\n 94939 │ 2019-12-03T20:07:00  2019-03-12T20:29:00   1.03374    1.1054   936472\n 94940 │ 2019-12-03T20:12:00  2019-03-12T20:28:00   1.05458    1.24564  936472\n 94941 │ 2019-12-03T20:14:00  2019-03-12T20:26:00   1.09106    1.29386  936472\n 94942 │ 2019-12-03T20:16:00  2019-03-12T20:26:00   1.14447    1.33854  936472 ⋯\n 94943 │ 2019-12-03T20:19:00  2019-03-12T20:24:00   1.1627     1.3965   936472\n 94944 │ 2019-12-03T20:23:00  2019-03-12T20:22:00   1.15358    1.48011  936472\n                                                3 columns and 94929 rows omitted, GroupedDataFrame with 18432 groups based on keys: imo, ata\nFirst Group (14 rows): imo = 9364722, ata = 2018-04-05T21:26:00\n Row │ updated              ata                  long       lat        imo     ⋯\n     │ DateTime             DateTime             Float64    Float64    Int64   ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ 2018-05-04T19:26:00  2018-04-05T21:26:00  -0.545055  -1.52547   9364722 ⋯\n   2 │ 2018-05-04T19:36:00  2018-04-05T21:26:00  -0.918912  -1.28835   9364722\n   3 │ 2018-05-04T19:39:00  2018-04-05T21:26:00  -1.10128   -1.19501   9364722\n   4 │ 2018-05-04T19:41:00  2018-04-05T21:26:00  -1.22112   -1.13661   9364722\n  ⋮  │          ⋮                    ⋮               ⋮          ⋮         ⋮    ⋱\n  11 │ 2018-05-04T20:40:00  2018-04-05T21:26:00   0.945162   0.413511  9364722 ⋯\n  12 │ 2018-05-04T20:47:00  2018-04-05T21:26:00   0.955583   0.609489  9364722\n  13 │ 2018-05-04T20:49:00  2018-04-05T21:26:00   0.984241   0.669211  9364722\n  14 │ 2018-05-04T20:55:00  2018-04-05T21:26:00   1.10539    0.856341  9364722\n                                                    2 columns and 6 rows omitted\n⋮\nLast Group (1 row): imo = 9364722, ata = 2019-03-12T20:22:00\n Row │ updated              ata                  long     lat      imo      t  ⋯\n     │ DateTime             DateTime             Float64  Float64  Int64    Mi ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │ 2019-12-03T20:23:00  2019-03-12T20:22:00  1.15358  1.48011  9364722  -3 ⋯\n                                                               2 columns omitted)\nBelow, I’ve plotted the trajectories with color marking the passage of time.\nscatter(df[!, :long], df[!, :lat], color=df[!, :nt], markersize=2, alpha=0.4)"
  },
  {
    "objectID": "posts/ships/ships.html#the-model",
    "href": "posts/ships/ships.html#the-model",
    "title": "Classifying Ships with Gaussian Process Mixtures",
    "section": "The Model",
    "text": "The Model\nIt seems like there’s more than one standard way of moving between these ports. I make out three different routes; trajectories seem to be scattered around a central curve for each route.\nWe can model this behavior with a mixture of Gaussian processes. For each of the three different routes above (\\(i=1,2,3\\)), we’ll assume a function \\(f_i\\) mapping time to lattitude and longitude values was sampled from a Gaussian Process prior. Container ships will trajectories will always be close to one of these routes- we just don’t know which one. We’ll also assume that other (non-container) ships follow trajectories sampled independently from the same Gaussian Process prior. To tell if a given trajectory seems to be that of a container ship, we just need to check whether it more closely resembles the posterior mixture of container routes or prior over all possible ship trajectories.\nSpecifically, let \\(y\\) refer to the observed trajectories and \\(y'\\) refer to the new trajectory we’re trying to classify. Say \\(B=0\\) if \\(y'\\) is a container ship and \\(B=1\\) otherwise. We want to learn the posterior odds that \\(B=1\\) given \\(y\\) and \\(y'\\), which is just \\(\\frac{P(y' | y, B=1)P(B=1)}{P(y' | y, B=O)P(B=O)}\\). When \\(B=1\\), we can get the likelihood of \\(y'\\) by integrating over samples \\(f\\) from the posterior GP mixture \\(\\int P(y' | f)P(f | y) \\, df\\). When \\(B=0\\), we can get the likelihood by integrating over samples \\(g\\) from the prior GP: \\(\\int P(y' | g)P(g) \\, dg\\).\nWe don’t know a priori what fraction of the ships in the region are container ships, but to be conservative, we’ll give equal prior probabiliy to \\(B=0\\) and \\(B=1\\)."
  },
  {
    "objectID": "posts/ships/ships.html#modeling-sub-trajectories",
    "href": "posts/ships/ships.html#modeling-sub-trajectories",
    "title": "Classifying Ships with Gaussian Process Mixtures",
    "section": "Modeling Sub-Trajectories",
    "text": "Modeling Sub-Trajectories\nIdeally, we’d like to identify out-of-distribution ships without having to observe their full port-to-port trajectories. To do this, we can marginalize the likelihood of a trajectory snippet over possible offsets in time.\n\nfunction marginal_logprob(dist, val, t, σ2)\n    starts = LinRange(0, 1 - t[end], 200)\n    logsumexp([logpdf(dist(s .+ t, σ2), val) for s in starts])\nend\n\nmarginal_logprob (generic function with 1 method)\n\n\n\nfunction ok_prob(t, gp, y1p, y2p, y1, y2)\n    \"\"\"Find the posterior probability that a ship with longitude/latitude trajectory pairs (`y1`, `y2`) at times `t` is a container ship (with trajectories coming from posterior GPs `y1p` and `y2p`) rather than some other kind of ship (with trajectories coming from prior `gp`).\n    \"\"\"\n    logistic(marginal_logprob(y1p, y1, t) + marginal_logprob(y2p, y2, t) -\n             marginal_logprob(gp, y1, t) - marginal_logprob(gp, y2, t))\nend\n\nok_prob (generic function with 1 method)"
  },
  {
    "objectID": "posts/ships/ships.html#approximating-posterior-gp-mixtures",
    "href": "posts/ships/ships.html#approximating-posterior-gp-mixtures",
    "title": "Classifying Ships with Gaussian Process Mixtures",
    "section": "Approximating Posterior GP Mixtures",
    "text": "Approximating Posterior GP Mixtures\nWe can’t compute the posterior of a mixture of Gaussian processes analytically. But we can approximate it with mean field variational inference.\nLet \\(z_i\\) be a 1-hot vector giving the route chosen in trajetory \\(i\\). I assume \\(z_i \\sim \\text{Categorical}(\\pi)\\), and \\(\\pi \\sim \\text{Dirichlet}(\\alpha_0)\\). The variational posterior for \\(\\pi\\) will be Dirichlet with parameters \\(\\alpha\\).\nFor the likelihood, I’ll introduce inducing inputs \\(c\\) and outputs \\(u_k\\) for each route and assume that \\(u_k = f_k(c)\\) and \\(p(y_i | u_k, z_{ik})  = \\mathcal{N}(K_{yu} K_{uu}^{-1}u, Q_{yy})\\) where \\(Q_{yy} = \\text{diag}(K_{yy} - K_{yu}K_{uu}^{-1}K_{uy}) + \\sigma^2I\\) (the Fully Indepenent Training Conditional assumption). Let \\(A = K_{yu}K_{uu}^{-1}\\) and \\(Q = \\Lambda^{-1}\\). We can pre-calculate the kernel matrices and store them in a separate KernelMats struct for each trajectory. If we were more concered with performance, we would avoid finding \\(K_{uu}^{-1}\\) explicitly, but this is good enough for a blog post.\n\nσ = 0.03\n\n0.03\n\n\n\nstruct KernelMats\n    Λ::PDiagMat{Float64,Vector{Float64}}\n    A::Matrix{Float64}\nend\n\n\n@sizecheck function kernel_mats(kern, x_N, c_U)\n    K_UU_inv = inv(PDMat(kern.(c_U', c_U)))\n    k = kern(1, 1) # Assuming stationarity\n    K_N = [kern.(c_U, reshape(x, (1, :))) for x in x_N]\n    c = KernelMats[\n        let\n            K_UT = K_N[n]\n            KernelMats(\n                inv(PDiagMat(k .- diag(K_UT' * (K_UU_inv * K_UT)) .+ σ^2)),\n                K_UT' * K_UU_inv)\n        end for n in 1:N\n    ]\n    (K_UU_inv, c)\nend\n\nkernel_mats (generic function with 1 method)\n\n\nThe variational posterior for inducing points \\(u_k\\) will be normal with mean \\(m_k\\) and covariance matrix \\(S_k\\).\n\nstruct VComp\n    S_inv::PDMat{Float64,Matrix{Float64}}\n    m1::Vector{Float64}\n    m2::Vector{Float64}\nend\n\nNow to figure out the variational updates. We’ll start by deriving the component update for \\(q(z_i)\\). From the mean field assumption, we know the ELBO is maximized when \\(\\log q(z_{ik}) = E_q \\log p(y_i | u_k) + E_q \\log p(z_{ik}) + \\text{const}\\). As the approximate posterior over \\(\\pi\\) is Dirichlet, \\(E_q \\log p(z_{ik}) = E_q \\pi_k = \\psi(\\alpha_k) - \\psi(\\sum_j \\alpha_j)\\). It remains to find\n\\(E_q \\log p(y_i | u_k) =-\\frac{1}{2} E_q (y-Au_k)\\Lambda (y-Au_k) + \\frac{1}{2} \\log |\\Lambda|\\)\nExpanding the quadratic term lets us compute the expectation:\n\\(-2 y^T\\Lambda A m_k + y^T\\Lambda y + \\text{Tr}(E[u_ku_k^T])A^T \\Lambda A\\) \\(= -2 y^T\\Lambda A m_k + y^T\\Lambda y + \\text{Tr}(m_km_k^T + S_k)A^T \\Lambda A\\) \\(= -2 y^T\\Lambda A m_k + y^T\\Lambda y + \\text{Tr}(S_kA^T \\Lambda A) + m_k^TA^T\\Lambda A m_k\\) \\(= (y - Am_k)^T\\Lambda (y - A m_k) + \\text{Tr}(S_kA^T \\Lambda A)\\)\nWe can eliminate terms like \\(\\psi(\\sum_j \\alpha _j)\\) and \\(\\log | \\Lambda |\\) which are the same for all components. The result is an expression for \\(q(z_i = k)\\), which I’ll write as \\(r_{ik}\\).\n\nfunction responsibilities(y1, y2, c::Vector{KernelMats}, o, alpha)\n    \"Calculate r_ik for seeing `(y1, y2)` for each component in `o`\"\n    Vector{Float64}[\n        softmax(Float64[isnothing(og) ? -Inf64 : let\n            μ1 = cn.A * og.m1\n            μ2 = cn.A * og.m2\n            V = og.S_inv \\ (cn.A' * cn.Λ * cn.A)\n            q = quad(cn.Λ, y1n - μ1) + quad(cn.Λ, y2n - μ2)\n            -0.5 * q - tr(V) + digamma(alpha_g)\n        end for (og, alpha_g) in zip(o, alpha)])\n        for (cn, y1n, y2n) in zip(c, y1, y2)]\nend\n\nresponsibilities (generic function with 1 method)\n\n\nNext, let’s calculate the inducing point variational parameters. From the mean field assumption, the ELBO is maximized when\n\\(\\log q(u_k) = \\sum_i E_q 1_{z_i = k} \\log  p(y_i | u_k, z_i = k) + \\log p(u_k) + \\text{const}\\) \\(= \\sum_i r_{ik} (\\langle u_ku_k^T, A^T\\Lambda A \\rangle + \\langle \\Lambda A u_k, y_i \\rangle) + \\langle u_ku_k^T, K_{uu}^{-1} \\rangle + \\text{const}\\)\nBy combining like terms, we can see that \\(S_k^{-1} = K_{uu}^{-1} + \\sum_i r_{ik} A^T \\Lambda A\\) and \\(S_k^{-1}m_k = \\sum_i r_{ik} A^T \\Lambda y_i\\).\n\nfunction fit_gp(g, K_UU_inv::PDMat{Float64,Matrix{Float64}}, r, y1, y2, c)\n    if sum(rn -&gt; rn[g], r) &lt; 1e-8\n        return nothing\n    end\n    S_inv = K_UU_inv + PDMat(sum(rn[g] * Xt_A_X(cn.Λ, cn.A) for (rn, cn) in zip(r, c) if rn[g] &gt; 0))\n    m1 = S_inv \\ sum(rn[g] * cn.A' * (cn.Λ * yn) for (rn, cn, yn) in zip(r, c, y1))\n    m2 = S_inv \\ sum(rn[g] * cn.A' * (cn.Λ * yn) for (rn, cn, yn) in zip(r, c, y2))\n    VComp(S_inv, m1, m2)\nend\n\nfit_gp (generic function with 1 method)\n\n\n\nfunction fit_gps(K_UU_inv::PDMat{Float64,Matrix{Float64}}, r, y1, y2, c)::Vector{Union{Nothing,VComp}}\n    Union{VComp,Nothing}[fit_gp(g, K_UU_inv, r, y1, y2, c) for g in 1:length(r[1])]\nend\n\nfit_gps (generic function with 1 method)\n\n\nFinally, we get to the mixture weight paramers. As before,\n\\(\\log q(\\pi) = \\log p(\\pi) + E \\log p(z_i | \\pi) + \\text{const}\\) \\(= \\langle \\alpha_0 - 1, \\pi \\rangle + \\sum_i \\langle r_i, \\pi \\rangle\\)\nThis gives \\(\\alpha = \\alpha_0 + \\sum_i r_i\\).\nPutting everything together gives the following variational inference algorithm.\nThis uses an auxiliary function to compute fixed points.\n\nfunction fixedpoint(f, arg::T; iters=500) where {T}\n    for _ in 1:iters\n        result = f(arg)::T\n        max_diff = maximum(abs, result - arg)\n        if max_diff &lt; 1e-5\n            return result\n        end\n        arg = result\n    end\n    println(\"DID NOT CONVERGE\")\n    arg\nend\n\nfixedpoint (generic function with 1 method)\n\n\n\n@sizecheck function fit_mixture(kern, alpha0, x_N, y1_N, y2_N, z_M, o; iters=50)\n    K_MM_inv, c = kernel_mats(kern, x_N, z_M)\n    alpha = fixedpoint(alpha0; iters) do alpha\n        r = responsibilities(y1_N, y2_N, c, o, alpha)\n        o = fit_gps(K_MM_inv, r, y1_N, y2_N, c)\n        alpha0 + sum(r)\n    end\n    alpha, c, o\nend\n\nfit_mixture (generic function with 1 method)"
  },
  {
    "objectID": "posts/ships/ships.html#example-on-synthetic-data",
    "href": "posts/ships/ships.html#example-on-synthetic-data",
    "title": "Classifying Ships with Gaussian Process Mixtures",
    "section": "Example on Synthetic Data",
    "text": "Example on Synthetic Data\n\nkern = with_lengthscale(Matern52Kernel(), 0.1)\n\nMatern 5/2 Kernel (metric = Distances.Euclidean(0.0))\n    - Scale Transform (s = 10.0)\n\n\n\nT = 100\n\n100\n\n\n\nN = 100\n\n100\n\n\n\nπ_prior = Dirichlet(5 * ones(3))\n\nDistributions.Dirichlet{Float64, Vector{Float64}, Float64}(alpha=[5.0, 5.0, 5.0])\n\n\n\ngp = GP(kern)\n\nGP{ZeroMean{Float64}, TransformedKernel{Matern52Kernel{Distances.Euclidean}, ScaleTransform{Float64}}}(ZeroMean{Float64}(), Matern 5/2 Kernel (metric = Distances.Euclidean(0.0))\n    - Scale Transform (s = 10.0))\n\n\n\nrng = Xoshiro(9)\n\nXoshiro(0x31c0fdb77e6b079f, 0xf8bbbedb8c20d31c, 0x1c19355ea0d34d01, 0x402b368a357b9496, 0x69a0c2eabd4f1212)\n\n\n\npi = rand(π_prior)\n\n3-element Vector{Float64}:\n 0.36892846174002913\n 0.38153355876999534\n 0.24953797948997547\n\n\n\n(true_f, y1_N, y2_N, x_N) = let\n    x_T = LinRange(0, 1, T)\n    true_f = [rand(rng, gp(x_T), 2) for _ in 1:3]\n    noise = [σ .* randn(T, 2) for _ in 1:N]\n    z = rand(rng, Distributions.Categorical(pi), N)\n    y_T2N = stack(true_f[z] .+ noise)\n    y1_N = eachcol(y_T2N[:, 1, :])\n    y2_N = eachcol(y_T2N[:, 2, :])\n    x_N = [x_T for _ in 1:N]\n    (true_f, y1_N, y2_N, x_N)\nend\n\n([[0.39940744485764307 0.781570857858878; 0.512664898939086 0.9316931016687168; … ; 1.5971325077761558 0.384910967509904; 1.582860813305773 0.3279274081244852], [-0.472291406780798 -0.31707101959464334; -0.5771870148169773 -0.151971382440848; … ; 0.956562978197726 -0.04504026546424312; 0.8338497795432375 -0.07039399921429594], [1.7847815717824167 0.2774961462911457; 1.884786093600304 0.24311245888699062; … ; 0.4102229023901559 0.45159679562935084; 0.49950707039911835 0.5780682109962041]], SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}[[0.3700879617376666, 0.47788108272999624, 0.5525052930328936, 0.5676365000476474, 0.600580781383119, 0.656945621953472, 0.6775837278595784, 0.6881661800554684, 0.5653046086360503, 0.4880848580064354  …  1.7852554741039721, 1.7631940807466238, 1.7866280687546399, 1.6855743542791397, 1.633295886410844, 1.5593140529511285, 1.5809315473099828, 1.5850692917905742, 1.5749665860994762, 1.5334497225603498], [-0.4211210630553008, -0.6285051132552908, -0.6227617039329616, -0.8708783039278554, -0.9826350021129571, -1.1425221312002887, -1.2456052275455118, -1.339157460801622, -1.3290334978511795, -1.2908546745738376  …  1.7440585177057981, 1.642796584537638, 1.5471585069379599, 1.4042550243933754, 1.273922733201116, 1.2097211725456494, 1.1401774243829077, 1.0330166219774146, 0.9774660463719848, 0.8694560989261199], [-0.44252855724920725, -0.5767173309396507, -0.7518329940704416, -0.8264096060580315, -0.9813279633871161, -1.0735124841508843, -1.2444212804907984, -1.3061050624555488, -1.3969219450012467, -1.303032688942919  …  1.7683495188197973, 1.6419402128662237, 1.5761334309833228, 1.3606435788261928, 1.224679012415514, 1.1808031839489388, 1.1440982686727479, 1.038392382871553, 0.9647433487335163, 0.8530716930658208], [-0.47473269079831315, -0.5615750444633624, -0.7183606354693861, -0.8867156543723269, -0.9749576028015003, -1.1542788066262792, -1.189686860068581, -1.2503907540431287, -1.3637508843511998, -1.2417064682862893  …  1.7444528889141617, 1.6338139567309706, 1.5221667857459085, 1.3851123960074552, 1.2918327829204133, 1.1962757171128644, 1.1486952037078746, 1.034241253576947, 0.9431906285244502, 0.8771208457572213], [0.36494672244837023, 0.505601088156253, 0.5419318915414362, 0.6390307602149727, 0.6699930478719739, 0.6380325617871504, 0.7000116125595375, 0.619618434365539, 0.6466087167325566, 0.4645811083735421  …  1.7772023639080774, 1.7879075341332262, 1.8234619091398676, 1.624647272462175, 1.595123486550642, 1.5823015283466466, 1.6018578158912113, 1.5445446904908762, 1.5804156908057965, 1.5874978731660736], [-0.45829387216263934, -0.5845683157765975, -0.7693263706843804, -0.8577174415492629, -0.8834134626113572, -1.0496103351531525, -1.2095788920673365, -1.3298190726797585, -1.3324800229270173, -1.2100700907652808  …  1.8267688543625462, 1.6884903757111873, 1.4990146493192344, 1.3954822052838494, 1.2625187306115169, 1.2072624049503318, 1.1245814546757065, 1.0427897663355867, 0.9501295786797349, 0.8557545395659737], [0.3911978212505599, 0.47390431099975877, 0.5172472721731246, 0.6005713954613187, 0.6564877516019645, 0.7428362817154499, 0.6849315350068836, 0.6893463560608264, 0.6005205136139856, 0.516664551994273  …  1.742633293786382, 1.783338177103919, 1.7520259940736869, 1.6961588834929628, 1.6145302208050327, 1.5889162342065626, 1.5491919318064642, 1.6088224008108758, 1.6238059225322634, 1.6468580894377165], [-0.4991119968068169, -0.6049268868041415, -0.7045121931024152, -0.8772407092667843, -1.0096857469337286, -1.125173412712578, -1.2041341694080048, -1.3813047339822273, -1.3517693973120835, -1.2406432132068899  …  1.7890062258542998, 1.688037354400058, 1.55065835081178, 1.373678310091426, 1.2882506966993172, 1.1985096174552177, 1.170421838865118, 1.073173545610662, 0.9302561089260525, 0.8343017204831965], [1.7786400304805892, 1.932975938351668, 2.027516057090291, 2.0033981296517247, 2.0128784398831536, 1.9631861753128486, 1.88461564254782, 1.6353303715807372, 1.4095268547439055, 1.1729328342054135  …  0.07080188064212697, 0.012460862800967806, -0.003860472673403812, -0.027792527897081892, 0.041006054523241325, 0.07233487412832193, 0.13337486155989425, 0.2976129483379553, 0.38809138003591453, 0.4717183474687341], [-0.44791149133422614, -0.5810326796431938, -0.7487286401181614, -0.7742755300049207, -0.9809947843833565, -1.0816424737325367, -1.2030786640396176, -1.3559847041836666, -1.3741457632277978, -1.2648118142953142  …  1.7811394294186476, 1.631426421683647, 1.5579500986578252, 1.4672048846444257, 1.2712351531963988, 1.1810396356652466, 1.1468560415074445, 1.0999079643321066, 0.9236696931355612, 0.7942863307502518]  …  [-0.44594578234679483, -0.5964229389200998, -0.7157235648306179, -0.8420088977395895, -0.9939481689585593, -1.0712170449386966, -1.1831352801992991, -1.3793831458908126, -1.3668583344846437, -1.2624021712766755  …  1.7030451330999403, 1.6588017752536302, 1.512757674357593, 1.4013957525922651, 1.250702904939713, 1.2121393136416265, 1.0744324286427764, 1.0620879318473355, 0.9283018855601111, 0.8801629156596172], [-0.5047967910591429, -0.5466362008552932, -0.6642314745302432, -0.8665718388130023, -0.9770433234279923, -1.082748196037504, -1.2445321491565609, -1.3428682755147545, -1.37176085369222, -1.2222335484676774  …  1.7362298140257617, 1.6248308007159942, 1.5876133909496646, 1.3805173346437243, 1.2983694600460338, 1.1636778226273061, 1.1590428012588874, 1.0828843878659404, 0.9812034152139966, 0.8281145968431958], [0.3864372145405262, 0.47506544453546246, 0.5790311319771552, 0.605001195666856, 0.6642869684695955, 0.6572596859911383, 0.6458185110626002, 0.6932787719655424, 0.5834768194374723, 0.4210018183812101  …  1.825359709436106, 1.7637870052307099, 1.7882698835578315, 1.6336417412945545, 1.5968288216523252, 1.5397504369174955, 1.5972296799671601, 1.5732473167653882, 1.5750034773116124, 1.6086170561222364], [1.7652791784827582, 1.8992601889650986, 1.952025741075971, 1.975228312896439, 2.0285127685927513, 1.9170691010660572, 1.7523626215932344, 1.588547282576697, 1.4568152531165184, 1.11160332392917  …  0.05493937081390281, 0.049141727865529075, 0.00161747946027661, -0.04965613318054553, -0.03803713447194219, 0.10903602769992818, 0.1927704678808896, 0.2850016776623056, 0.4188130112268084, 0.4810023570547357], [0.4568436560110791, 0.48120454594583395, 0.571062776775225, 0.5952367679609637, 0.67959854911344, 0.6639152098144613, 0.6926944303221002, 0.6707664681350729, 0.624327313587915, 0.49367900246722796  …  1.7248769135622033, 1.7988909358878185, 1.7055663411581536, 1.7091054868956446, 1.5445615111365127, 1.6105424278886316, 1.6073818339426091, 1.558477019335945, 1.6083236847122333, 1.6061616139075827], [-0.4835813560919496, -0.5942356617704319, -0.6934005695853287, -0.8737173243718228, -0.9896088391321894, -1.1117975171234642, -1.1945413835901322, -1.319212417107135, -1.3414057714347458, -1.2392607356840089  …  1.7528161969646778, 1.6924694705607917, 1.5367129548531426, 1.3953555090582246, 1.2449245129761877, 1.2448055522759653, 1.1313157873096107, 1.0911204298246533, 0.9820455857015086, 0.8369014421300336], [-0.47315145964374994, -0.6072952347887302, -0.701880445688005, -0.8913588489475074, -0.9723306840541012, -1.1048488413578572, -1.2281168567356953, -1.3370998069464055, -1.345697912763073, -1.2529732938260187  …  1.8102388083417889, 1.6157632299169338, 1.5118503776166272, 1.4049830864638395, 1.2451653591009686, 1.2040886523321568, 1.1649931662817816, 1.1058982705777327, 0.9798737421629538, 0.8557887184426698], [1.7816870699107052, 1.8537112521580037, 1.976950190984634, 1.986261857263395, 2.021509615944646, 2.005696075937408, 1.8123978064589177, 1.6048856037735737, 1.41076537382959, 1.1644426850695577  …  0.057427651456858686, 0.028476859108208852, 0.0058752637035537035, -0.019816606578055063, -0.05569232459204474, 0.14836624405061954, 0.18946405785437778, 0.3061126113781384, 0.4227092896828641, 0.49661083058849514], [-0.5233359997666176, -0.5645079977227427, -0.6811099898375295, -0.8130532288194817, -0.9635666569689258, -1.1034908626409008, -1.2013648271891462, -1.3153184932607274, -1.3492353730582967, -1.325160707906475  …  1.7799405339485965, 1.6420547516008175, 1.5807903035712507, 1.4081405505375408, 1.2825112685705002, 1.244780947058204, 1.0927230918227466, 1.148161555289747, 0.9339069566655267, 0.8397286373318037], [-0.480714146455318, -0.5902724390160647, -0.6741421716500787, -0.8094161433217102, -0.9780975130073811, -1.0542478765511951, -1.213235664986352, -1.3681628231299032, -1.267384648323656, -1.2456436388976502  …  1.762646039608105, 1.6014173787887998, 1.563066220405517, 1.3719650753744121, 1.2539148216557845, 1.1904511189212659, 1.1052689281117576, 1.124542482409055, 0.9413495117813608, 0.8523346180358521]], SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}[[0.8026667343773839, 0.9272374973895329, 1.1127739414618427, 1.1850772393095752, 1.3755161827643632, 1.5145110873441292, 1.5451907301923051, 1.6238314911980516, 1.7107084933276706, 1.6891266615139446  …  0.37705151045419133, 0.3970301134461245, 0.5406376063300915, 0.571561218068671, 0.6800069366790182, 0.6246300248434518, 0.5549571657924839, 0.4845750398315211, 0.4444632386688463, 0.3065966944421674], [-0.3511202590582168, -0.1394202595527906, -0.01592128257509561, 0.11456141792709228, 0.20546059628586333, 0.22156494914935, 0.22615040939683548, 0.09025338096872909, 0.182187630160484, -0.005985131923862824  …  0.9453481027904888, 0.840024046255369, 0.7933974937600053, 0.6098280496150628, 0.45261882673612297, 0.28363201452859776, 0.1761962510005486, 0.044366780231433076, -0.042824779486057486, -0.07321518270210282], [-0.3316590172562221, -0.12882551009709525, 0.014541497404227253, 0.07032042159061222, 0.2176387514998076, 0.1811778304305366, 0.16815037466155341, 0.1407701910457701, 0.0996079066868828, -0.02930677781917293  …  0.9493178932539263, 0.8902953306505575, 0.7558815763551197, 0.6232956457134926, 0.4624333195116825, 0.27285330599692004, 0.13070915954548842, 0.06284622293823146, -0.044645324121477765, -0.08316048793996679], [-0.3061132070781288, -0.18746915091063898, -0.0037156318009084863, 0.11477712248293126, 0.14178840135194523, 0.19457909414717062, 0.1955899805257978, 0.11467999852483013, 0.11595901240894115, 0.022845111590977667  …  0.9469114758188042, 0.833763050840117, 0.7458581706309779, 0.5551756175360192, 0.4739637556591287, 0.29179344911648075, 0.16481176509486664, 0.06225125282634908, -0.04991623542449028, -0.05926327822357183], [0.7542643532313039, 0.9381647159092591, 1.0572712378171865, 1.230254771632905, 1.4011319305296153, 1.450573926965493, 1.52631013974342, 1.5834123511215288, 1.6286189371679543, 1.6881582903149  …  0.42886155919053387, 0.43128489920801094, 0.4842915816963676, 0.5582992436597269, 0.6702241932135719, 0.6383303005842197, 0.6041011943044065, 0.45254403753246025, 0.40008280432257515, 0.3345193631040302], [-0.33971839622565775, -0.18497161908851448, 0.012290318908119165, 0.03893645767118309, 0.1462335973747323, 0.2486427659633012, 0.18025218264555226, 0.15180563377655565, 0.129006648407872, 0.04482222095069195  …  0.9809957348161344, 0.8749881080800541, 0.746274443026718, 0.5541425643246481, 0.4625891124459483, 0.32162274339713603, 0.1872853342094712, 0.06036664390304627, -0.03680231544771183, -0.049510137247818095], [0.8070002191587647, 0.9442913066249085, 1.1302769386697824, 1.2081967521746397, 1.3356914974758014, 1.5223766554137672, 1.5909753470138102, 1.627331547618399, 1.6511501976480922, 1.6689231102993827  …  0.40538984256155297, 0.42861193633673994, 0.5015969281072589, 0.5311856631469392, 0.6674954615231076, 0.5965903598553703, 0.5566369513333743, 0.48324093467948537, 0.4311549502042953, 0.39076973414700705], [-0.28480705916843696, -0.10625315182533043, 0.024785261879050544, 0.07641654517612127, 0.1552347330393129, 0.21467197524252887, 0.214388936776746, 0.17167367047308044, 0.08858284863844713, -0.003576561190596985  …  1.0149570354303759, 0.9116976342059242, 0.8239318219863443, 0.6096597989327066, 0.48176526860875346, 0.32314738554439126, 0.16730003837914675, 0.08077671698648625, -0.07390351923490435, -0.05204669294629327], [0.28648197660419655, 0.20155607654827692, 0.26083569673153795, 0.36486042972579297, 0.47696577741860846, 0.5728627677913581, 0.7118394193366427, 0.8165500345643881, 0.8503989847697979, 0.8896777697420456  …  -1.4056923559021057, -1.3446808615941683, -1.1687235246679424, -0.8896726010400444, -0.5328105301816746, -0.21166465673454016, 0.032554311108303965, 0.2512420002094242, 0.40613003452988733, 0.5554062118802982], [-0.31588359240943087, -0.17187403815816005, -0.02484289353469079, 0.08707994300917316, 0.18861032983524167, 0.23601727846431259, 0.1828979770636666, 0.1949405573686691, 0.0929478161486642, 0.020004547787449693  …  0.9620080749546955, 0.8869241610481053, 0.7259735088196013, 0.5693908342304073, 0.4573272244950232, 0.29148148550445485, 0.1222081972384219, 0.02596855804006122, -0.04126537469085773, -0.08653472746798775]  …  [-0.35705120321208633, -0.20901635372498298, 0.02321892186912369, 0.11576829504557906, 0.14650371450550775, 0.20303895815636624, 0.24571110676730512, 0.11440248690470649, 0.07691455144487752, -0.010704949465951612  …  1.0079343494149848, 0.8639592791616786, 0.6989091242862185, 0.5875322078162816, 0.4186520837850273, 0.2654176591290303, 0.1682257940773366, 0.06676302288135211, -0.044769623955030355, -0.0788874095146267], [-0.279904108877613, -0.12295700233805448, 0.0071024562964638795, 0.08351957396009985, 0.1917727970633079, 0.1922179072599698, 0.19007885524486368, 0.11830436120149257, 0.11410074376829943, 0.01998378576031322  …  0.9492839117247923, 0.8589391035144648, 0.7319842854719563, 0.6128710414693419, 0.4817199565533437, 0.3135326814014038, 0.16696709064644785, 0.09259594733545032, -0.059887950660281526, -0.06310514982217307], [0.7485885481532815, 0.9496278327723022, 1.1355736085922767, 1.1970508579770986, 1.3664900305228709, 1.4789190585922352, 1.5562589096163317, 1.6778939629468463, 1.6972443353143774, 1.6889220543113057  …  0.37479976199667936, 0.47511857681857794, 0.5105806160436273, 0.5383790314420341, 0.6217977166256058, 0.6870037921444523, 0.5882077526873921, 0.4640753689514396, 0.3691364274267584, 0.32346671203108546], [0.28218547921788467, 0.25852326560035777, 0.24804168349375058, 0.34035215679584213, 0.48332091491032664, 0.6810745307736549, 0.7397047752201192, 0.8227716054680719, 0.9051939537338954, 0.9101865057623911  …  -1.412996682530946, -1.3325410232299726, -1.182947548555859, -0.8412079171641019, -0.5041422949557393, -0.16769397982582562, 0.10509389309414877, 0.33998724966109944, 0.5023973081720233, 0.5688581438571654], [0.7101443419242278, 0.92537579827458, 1.0610646275627824, 1.208902483146923, 1.4000777462059244, 1.4894610567046778, 1.5644128089198555, 1.621089004697623, 1.6888913837865571, 1.6464733511232061  …  0.3456885315155104, 0.4596129940542724, 0.491276304574797, 0.5198203237315869, 0.655724814227794, 0.693713107801062, 0.5775682053959397, 0.5010802378190798, 0.35285324550080654, 0.3119044310686243], [-0.36306238962978576, -0.12389784539665918, -0.03267561160049834, 0.07582850937766375, 0.19309351607550818, 0.22632673439955503, 0.22787902916734107, 0.18175550402040694, 0.06540796199682547, 0.04787935917086694  …  0.9164575597690792, 0.8931848866429338, 0.7148653413134657, 0.6040060300793832, 0.5191080406576657, 0.30390468880288385, 0.16387312554123587, 0.06095543112877319, -0.07994270150037766, -0.05720146267407376], [-0.32410490682718823, -0.1417689843419464, 0.04179619775600474, 0.16226839463733644, 0.21713888548744123, 0.2649215807550171, 0.20120446679707107, 0.17468738952420917, 0.10792588917593836, -0.01579405559071982  …  0.9321369006252102, 0.8769783256543988, 0.7465564760762197, 0.6192597526582085, 0.49699952114827506, 0.3324812875687691, 0.1674177074005299, 0.08781208984408945, -0.09395454139673826, -0.12054932874179738], [0.29020548407997343, 0.24367041136655856, 0.3140377175581981, 0.326030813256892, 0.4922290250390344, 0.69727486316066, 0.6715321969219319, 0.8220146448200999, 0.8571048995855263, 0.9569098957853442  …  -1.4085410061936572, -1.3139630581712771, -1.144940546367433, -0.8649584777180653, -0.5340106625375828, -0.15799823192358955, 0.10376369727576212, 0.3235305289744173, 0.4729629138433807, 0.6080658146372319], [-0.31922898415091433, -0.1277049942711259, -0.03651236916001316, 0.13697856902367483, 0.17011585663299533, 0.21585601992290315, 0.23131615828652724, 0.18219613085112296, 0.06861156243141214, 0.03269045900839065  …  0.9650255147908333, 0.9013952849286737, 0.7903142815081935, 0.602727673045968, 0.4284457361412203, 0.2691222965999577, 0.17383127894830167, 0.02768397406416313, -0.0550293602895119, -0.06250077994127747], [-0.30763151538794775, -0.11112087217424535, -0.04011655123221196, 0.1299296495772406, 0.15642780439202808, 0.20332251240533103, 0.23625236030045538, 0.14377374782200086, 0.06136200192502247, 0.009558288951302969  …  0.9637535656666396, 0.895903950309292, 0.7115397197932328, 0.6059992603639028, 0.40487763604619487, 0.29276415532274447, 0.17868614362400462, 0.07885875761328787, -0.044904922656329685, -0.07785071961322676]], LinRange{Float64, Int64}[LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100)  …  LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100), LinRange{Float64}(0.0, 1.0, 100)])\n\n\n\nc_U = LinRange(0, 1, 25)\n\n25-element LinRange{Float64, Int64}:\n 0.0, 0.0416667, 0.0833333, 0.125, …, 0.875, 0.916667, 0.958333, 1.0\n\n\nTo kick off variational inference, we’ll need to guess starting parameters for \\(S_k\\) and \\(m_k\\).\n\no_guess = let\n    f = gp(c_U, σ^2)\n    K_UU_inv = inv(PDMat(kern.(c_U', c_U)))\n    Union{Nothing,VComp}[VComp(K_UU_inv, rand(rng, f), rand(rng, f)) for _ in 1:3]\nend\n\n3-element Vector{Union{Nothing, VComp}}:\n VComp([7.454870343138737 -12.085564351326695 … -4.395025867196253e-8 1.0695053577639308e-8; -12.085564351326695 27.047547206323763 … 1.8060921559003263e-7 -4.395025867240513e-8; … ; -4.395025867196253e-8 1.8060921559003263e-7 … 27.04754720632348 -12.085564351326603; 1.0695053577639308e-8 -4.395025867240513e-8 … -12.085564351326603 7.454870343138712], [-0.32979921047879956, -0.5897305605112932, 0.11447706895399364, 1.029017128451943, 1.0599601362252413, 0.5111666085710849, 0.07907917386581927, 0.2877821470816543, 0.6470354741278084, 1.4784262694762869  …  -0.5740153604150344, -0.5371827926948518, -0.7993928322576462, -0.5568548810814099, -1.1369227588787811, -1.798383746675905, -1.2631768279691984, -0.7743985404177847, -0.5390795325431714, 0.46438075009635527], [0.43883495716887116, 0.9848746247385155, 0.9515814521504622, 0.7643492693358623, 0.5271321278211968, 0.4894869429738167, 0.8325602734008992, 1.3088984166300248, 1.8516330253814923, 1.8097893768525444  …  -1.2254495200013993, -0.9601158375877539, -0.46840313165530256, -0.46897044443131936, -0.48417534075567253, -0.5578469644611623, -1.0491654256770087, -1.4257330683614824, -0.8617948360702556, 0.017421172919114353])\n VComp([7.454870343138737 -12.085564351326695 … -4.395025867196253e-8 1.0695053577639308e-8; -12.085564351326695 27.047547206323763 … 1.8060921559003263e-7 -4.395025867240513e-8; … ; -4.395025867196253e-8 1.8060921559003263e-7 … 27.04754720632348 -12.085564351326603; 1.0695053577639308e-8 -4.395025867240513e-8 … -12.085564351326603 7.454870343138712], [-0.7744159166454945, -1.2601807812167034, -0.9958548872815728, -0.17653467402440767, 0.9430594849897121, 2.0105500724949295, 2.033124578960417, 1.8565564339339036, 1.8221137639819547, 1.1363083966179663  …  0.15448540477257527, 0.41259110078244066, -0.0027348811068554422, 0.19620549642147278, 0.8763233073663659, 1.3773182347415502, 1.2869812975779953, 0.7746707958819704, -0.022812002763165848, -0.5813990084339119], [0.7846997214502921, 0.9282255890427066, 0.9645535603585261, 0.6532360444039843, 0.3532747775171625, -0.06511150394942444, 0.38443523877237284, 0.9039526689577165, 0.8965788941631468, 0.3359702471738463  …  -0.34629481463918804, -0.8349062772444377, -0.5182156620975424, -0.044966742684170016, 0.5312907147853932, 0.5535393563291215, 0.9294317565413944, 1.479226205158952, 1.5475181642383982, 0.8477215849890636])\n VComp([7.454870343138737 -12.085564351326695 … -4.395025867196253e-8 1.0695053577639308e-8; -12.085564351326695 27.047547206323763 … 1.8060921559003263e-7 -4.395025867240513e-8; … ; -4.395025867196253e-8 1.8060921559003263e-7 … 27.04754720632348 -12.085564351326603; 1.0695053577639308e-8 -4.395025867240513e-8 … -12.085564351326603 7.454870343138712], [1.7486689273216505, 2.4259674782795706, 2.23590574897882, 1.3197002380068752, 0.15255298167106107, -0.13640229167983647, -0.2978069479132432, -0.2357749372929084, -0.480667401701669, -0.3200016176917847  …  0.30610173054220047, 0.7773459629412118, 0.6983570030055796, 0.3619794114301518, 0.23999594112184505, -0.1174427074074989, -0.750398403767938, -1.5117006503169192, -1.4472670583060792, -1.0947027562950333], [0.6884887829721742, 1.0012706667133673, 1.4795170415409875, 1.1715523761660434, 1.059851802972681, 0.47428421889003575, 0.5799475934607595, 1.2958003220887966, 1.4223610253305643, 1.4728308154550735  …  -0.37139044454526404, 0.48014979684391895, 0.39591942033965993, -0.49328143269907426, -0.18136582469922452, 0.2431903591430569, 0.4586852603345645, -0.24386642954148235, -0.6263680839326332, -0.6205153515060421])\n\n\n\n_, c, o = fit_mixture(kern, π_prior.alpha, x_N, y1_N, y2_N, c_U, o_guess; iters=50)\n\n([33.0, 42.0, 40.0], KernelMats[KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964])  …  KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964]), KernelMats([1111.111111111111 0.0 … 0.0 0.0; 0.0 338.3030875290355 … 0.0 0.0; … ; 0.0 0.0 … 338.30308752883224 0.0; 0.0 0.0 … 0.0 1111.1111111071364], [0.9999999999999987 -5.1769817281390405e-15 … -2.0285485224130183e-19 5.1985138724645184e-20; 0.7242230000833083 0.38463921309029714 … 5.850640797546697e-10 -1.4237212403844214e-10; … ; -1.423721240887343e-10 5.850640799372934e-10 … 0.38463921309030774 0.7242230000833062; 6.253485430900889e-22 -1.036291871406433e-20 … 3.552713678800501e-15 0.9999999999999964])], Union{Nothing, VComp}[VComp([38046.26445509075 5325.464176322597 … 8.006537288460481e-5 -2.1740008405797687e-5; 5325.464176322597 62727.21482930112 … -0.00029091409508395566 8.006537289784688e-5; … ; 8.006537288460481e-5 -0.00029091409508395566 … 62727.214828988595 5325.464176318087; -2.1740008405797687e-5 8.006537289784688e-5 … 5325.464176318087 38046.264454973796], [1.7884384433739497, 2.0078691637258808, 1.3336730045136205, 0.27314880593905083, 0.11717217357332622, 0.7608730047237832, 1.418507139298858, 1.5047548465666285, 1.2232513340114586, 0.5870425288598827  …  -0.5090363917647711, 0.4159266785825978, 1.142011701120123, 0.3991566630081422, -0.03828048002571067, 0.45909901400871095, 0.30936063082589194, 0.003078292861633498, 0.07752886898789478, 0.4972970076652429], [0.251311327227621, 0.4888737351135423, 0.8867746410121203, 1.086589015474623, 1.359889255567608, 1.1663408298950968, 0.6742235065052071, 0.43683503392133255, 0.50347745629217, 0.28420664577352606  …  -1.3913449608047943, -1.4820787109705316, -1.0020681941677192, 0.015795349003787432, 0.354779181378556, -0.12879054552524927, -1.0416735978677272, -1.3667321599783797, -0.24280530297126435, 0.585944704826394]), VComp([50273.02467875962 7041.105164396358 … 0.00010581479818065806 -2.8731305946311186e-5; 7041.105164396358 82880.84002711735 … -0.0003844802500373811 0.0001058147981981566; … ; 0.00010581479818065806 -0.0003844802500373811 … 82880.84002670436 7041.105164390399; -2.8731305946311186e-5 0.0001058147981981566 … 7041.105164390399 50273.02467860508], [-0.4836135038178731, -0.988678978737391, -1.3229272605434894, -0.32482102645182676, 0.7706544694426579, 0.8325822141583343, 0.5987229795637262, -0.5241990119238615, -1.9107531261540993, -2.4820153872991875  …  -0.6464292997148956, -0.5574036888828474, -0.3351650246270802, 0.1112275857600006, 0.32323019350882154, 0.8085350087723989, 1.5858869985684616, 1.6837522737163038, 1.20160003294185, 0.8592494110733625], [-0.31784821189198365, 0.1986421274795545, 0.07395072617986106, -0.16580134440202543, -0.22344252696654085, 0.1827130996591008, 1.0423472580033715, 1.6508726669189218, 1.8051465608388193, 1.3977140239824084  …  0.12167854454672755, -0.11185061677671573, -0.10965042590176122, 0.19013484987004253, 0.8374248207113852, 1.2895245916656173, 1.2016737602722332, 0.9172989551111528, 0.31352000354086257, -0.07552767910043261]), VComp([47555.96685127765 6659.851611491078 … 0.000100092703670424 -2.717768427064152e-5; 6659.851611491078 78402.25664982485 … -0.0003636877711588421 0.00010009270368697667; … ; 0.000100092703670424 -0.0003636877711588421 … 78402.25664943419 6659.851611485441; -2.717768427064152e-5 0.00010009270368697667 … 6659.851611485441 47555.96685113146], [0.4066548669398225, 0.6527573699038731, 0.5816931561575486, 0.39017907478916863, 0.9103386003926413, 0.8994363236523513, 0.40972139186328504, -0.26309126872025157, -1.0235610420699035, -1.2045025137506382  …  2.3605910456886523, 1.6760297054286803, 0.8655616471928317, -0.3470911540061117, -0.7543098221945449, 0.12175262357027786, 1.4032123448577127, 1.7844745477706296, 1.5926862118501055, 1.5854845568338483], [0.776847463305056, 1.38569216887199, 1.6662121732024364, 1.641756303077532, 1.5222001502924467, 0.9846669635716546, 0.8003230315970344, 0.39959120493513467, 0.302546033373993, 0.6610598848670328  …  -1.9937429281585854, -2.5432907386447496, -2.606678025607028, -1.921237570472245, -1.0458519688106935, -0.4689228809106339, 0.07851774467505346, 0.4218121455453776, 0.6476437583638489, 0.3185399115368512])])\n\n\nWe can visualize how well the mixtures were recovered during inference by plotting a circle at our GP predictions over time. The radius of each circle will be the standard deviation of our posterior uncertainty at that point. I will draw the true synthetically generated functions as well for comparison.\n\nfunction plot_example(o, kern, c_U, true_f)\n    f = Figure()\n    ax = Axis(f[1, 1])\n    x_N = [LinRange(0, 1, 100)]\n    _, c = kernel_mats(kern, x_N, c_U)\n    for on in filter(!isnothing, o)\n        μ = Point2d.(c[1].A * on.m1, c[1].A * on.m2)\n        σ = sqrt.(diag(inv(c[1].Λ)) .+ invquad(on.S_inv, c[1].A'))\n        poly!(ax, Circle.(μ, σ), alpha=0.2)\n    end\n    for i in 1:3\n        lines!(ax, true_f[i][:, 1], true_f[i][:, 2])\n    end\n    f\nend\n\nplot_example (generic function with 1 method)\n\n\n\nplot_example(o, kern, c_U, true_f)"
  },
  {
    "objectID": "posts/books.html",
    "href": "posts/books.html",
    "title": "So You Want to Learn [X]",
    "section": "",
    "text": "In the style of Susan Rigetti’s classic “So You Want to Learn Physics”, this post lists some of my favorite resources for learning stuff."
  },
  {
    "objectID": "posts/books.html#statistics",
    "href": "posts/books.html#statistics",
    "title": "So You Want to Learn [X]",
    "section": "Statistics",
    "text": "Statistics\n\nStatistical Modeling: A Fresh Approach (Daniel Kaplan) is a great introduction to frequentist statistics with as little math is possible.\nRegression and Other Stories (Andrew Gelman et al) is similar, but uses a Bayesian approach.\nStatistical Inference (George Casella and Roger L. Berger) is great for learning about estimation and hypothesis testing.\nPlane Analysis to Complex Questions (George E. Forsythe and Cleve B. Moler) is great for learning about analysis of variance if you already know linear algebra.\nGeneralized Linear Models with Examples in R (Peter Dunn et al) tells you all about exponential dispersion models and how the residuals we know from linear regression get generalized as “deviances” in GLMs.\nStatistical Rethinking (Richard McElreath) is a great introduction to Bayesian statistics with a focus on modeling.\nBayesian Data Analysis (Andrew Gelman et al) goes into more detail. It’s the definitive text on Bayesian methods.\nAll of Nonparametric Statistics (Larry Wasserman) gives an accessible introduction to nonparametric methods (bootstrap, jackknife, kernel smoothing) and why they work (influence functions, Hadamard differentiability).\nComplex Surveys: A guide to Analysis (Thomas Lumley) talks about estimating statistics of finite populations, which requires different kinds of estimators than those we’ve seen so far.\nSurvival Analysis: Techniques for Censored and Truncated Data (John P. Klein and Melvin L. Moeschberger) got me up to speed on survival analysis.\nCausal Inference: The Mixtape (Scott Cunningham) is a great resource for observational studies, with plenty of worked examples.\nHandbook of Markov Chain Monte Carlo is great for understanding and diagnosing failures of fancier MCMC techniques.\nElements of Sequential Monte Carlo (Christian Naesseth el al) is the simplest tutorial on sequential Monte Carlo methods I’ve seen."
  },
  {
    "objectID": "posts/books.html#machine-learning",
    "href": "posts/books.html#machine-learning",
    "title": "So You Want to Learn [X]",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nPattern Recognition and Machine Learning (Christopher M. Bishop) is my go-to reference for machine learning fundamentals.\nGaussian Processes for Machine Learning (Carl Rasmussen and Christopher K. I. Williams) is the classic text on Gaussian processes.\nReinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto) is a very accessible introduction to reinforcement learning.\nBandit Algorithms (Tor Lattimore and Csaba Szepesvári) is a comprehensive guide to multi-armed bandit problems.\nOnline Learning and Online Convex Optimization (Shai Shalev-Shwartz and Yoram Singer) is a short introduction to online learning.\nConvex Optimization (Stephen Boyd and Lieven Vandenberghe) is the canonical reference for convex optimization.\nUnderstanding Machine Learning: From Theory to Algorithms (Shai Shalev-Shwartz and Shai Ben-David) gives you a taste of learning theory."
  },
  {
    "objectID": "posts/Diffusion-is-a-fancy-VAE.html",
    "href": "posts/Diffusion-is-a-fancy-VAE.html",
    "title": "Generative ODE Models are VAEs",
    "section": "",
    "text": "Generative image models based on ordinary differential equations can be seen as forms of variational auto-encoders with a partially deterministic inference network. \\(\\newcommand{\\coloneqq}{\\mathrel{\\vcenter{:}}=}\\)\n\nVariational Auto-encoders\nGiven a dataset of samples \\(y\\) from an unknown distribution \\(\\pi_1\\), generative models attempt to find parameters \\(\\theta\\) for a fixed family that maximizes the likelihood \\(p_\\theta(y)\\). When the family implicitly marginalizes over a latent variable \\(z\\) so that \\(\\log p_\\theta(y) = \\log E_{z \\sim p} p_\\theta(y \\vert  z)\\), maximum likelihood estimation becomes intractable in general. Variational auto-encoders address this problem by maximizing a lower bound on the likelihood: \\[\n\\begin{align*}\n\\log p_\\theta(y) &= \\log E_{z \\sim p} p_\\theta(y \\vert  z) \\\\\n&=\\log E_{q(z \\vert  y)} \\frac{p(y\\vert z) p(z)}{q(z\\vert y)} &\\text{ (importance sampling)}\\\\\n& \\geq E_{q(z \\vert  y)} \\log \\frac{p(y\\vert z) p(z)}{q(z)} &\\text{ (Jensen's inequality)}\n\\end{align*}\n\\]\nThis quantity \\(\\mathcal{L} \\coloneqq E_{q(z \\vert  y)} \\log p(y,z) - \\log q(z)\\) is known as the evidence lower bound or ELBO. The distribution \\(q(z\\vert y)\\) used for importance sampling is usually parameterized by a neural net and is known as the inference network. In what follows, we will implicitly assume that \\(q\\) is a function of neural net parameters \\(\\phi\\).\n\n\nFrom Variational Auto-encoders to Rectified Flows\nAssume that the model’s prior over \\(z\\) factors as \\(p(z) = p(z_0)\\prod_t p(z_{t+\\Delta} \\vert  z_t)\\), where \\(t\\) ranges from \\(0\\) to \\(1\\) in increments of \\(\\Delta\\) and \\(z_1\\) is observed (taking the place of \\(y\\) in the discussion so far). Instead of fixing the model \\(p\\) and learning parameters for a distribution \\(q\\) to approximate its posterior, with diffusion-based approaches, we do the opposite.. Fix the posterior approximation \\(q(z_t \\vert  z_1, z_0)\\) to be a Delta distribution at \\(tz_1 + (1 - t)z_0\\) (linearly interpolating between observations). Let \\(p(z_{t+\\Delta} \\vert  z_t)\\) be distributed as \\(\\mathcal{N}(z_t + \\Delta f_\\theta(z_t, t), \\sigma^2/\\Delta^2)\\), where \\(f_\\theta\\) is a neural network parameterized by \\(\\theta\\). Under this linear interpolation scheme, \\(z_{t + \\Delta} - z_t = \\Delta (z_1 - z_0)\\). This means that \\[\n\\begin{align*}\n    &E\\left[\\log p((z_{t+\\Delta} \\vert  z_t)\\right] \\\\\n    &= E\\left[-\\frac{1}{2} \\frac{(z_{t + \\Delta} - [z_t + \\Delta f_\\theta (z_t, t)])^2}{\\sigma^2 \\Delta^2}\\right] \\\\\n    &= E\\left[\\frac{-1}{2\\sigma^2 \\Delta} ((z_1 - z_0) - f_\\theta (z_t, t))^2\\right]\n\\end{align*}\n\\]\nWe can plug this simplification into the full ELBO, noting that the sum over time can be written as an expectation.\n\\[\nE_{z_0} \\left[ \\log p(z_0) - \\log q(z_0\\vert z_1) + \\sum_t \\log p(z_{t+\\Delta} \\vert  z_t) \\right] = \\\\\n-KL[q(z_0\\vert z_1) \\vert p(z_0)] + \\frac{-1}{2\\sigma^2} E_{t, z_0} \\left[ ((z_1 - z_0) - f_\\theta (z_t, t))^2 \\right]\n\\]\nIt breaks into two parts: a KL divergence keeping \\(q(z_0\\vert z_1)\\) close to \\(p(z_0)\\), and a stochastic squared loss in which both time and latent code are sampled. As \\(\\Delta \\to 0\\), the distribution over times approaches uniform. This is precisely the loss for the Rectified Flow model for the case of a fixed \\(q(z_0 \\vert  z_1)\\). Originally, the authors interpreted this loss as a way to learn an ODE \\(\\frac{\\partial z}{\\partial t} = f(z, t)\\) which transports samples \\(z_0\\) to observations \\(z_1\\). By casting the loss in the language of variational auto-encoders, however, we can learn a posterior distribution for \\(z_0\\) at the same time."
  },
  {
    "objectID": "posts/Krylov-Methods.html",
    "href": "posts/Krylov-Methods.html",
    "title": "Krylov Methods",
    "section": "",
    "text": "The \\(i\\)th Krylov subspace \\(\\mathcal{K}_i\\) for a symmetric matrix \\(A\\) starting from vector \\(b\\) is the subspace spanned by the vectors \\(b, Ab, A^2b, \\dotsc A^{i-1}b\\). Algorithms that use these subspaces are called Krylov subspace methods. To see why this subspace can be useful, it helps to see some examples.\n\nThe Lanczos Algorithm\nThe Lanczos algorithm finds an orthonormal matrix \\(V\\) and tridiagonal matrix \\(T\\) so that \\(A = VTV^T\\). The columns of \\(V\\) are chosen from successive Krylov subspaces. \\(v_1\\) is chosen to be \\(b/ \\|b\\|\\). Afterwards, \\(v_i\\) is just \\(Av_{i-1}\\), but normalized and without any components parallel to the previous columns. Specifically \\[\n\\begin{align*}\nw_{i+1}' &= Av_i \\\\\nw_{i+1} &= w_{i+1}' - \\sum_{k=1}^i \\langle v_k, w_{i+1}'\\rangle v_k \\\\\nv_{i+1} &= w_{i+1} / \\|w_{i+1}\\|\n\\end{align*}\n\\] Note that this means \\(Av_k\\) is a linear combination of \\(v_1\\) through \\(v_{k+1}\\), so \\[\n\\begin{align*}\n\\langle v_k , w_{i+1}' \\rangle &= \\langle v_k , A v_i \\rangle \\\\\n&= \\langle Av_k, v_i \\rangle \\\\\n&= \\left \\langle \\sum_{j=1}^{k+1} c_j v_j , v_i \\right\\rangle\n\\end{align*}\n\\] for some constants \\(c_j\\). As the \\(v_i\\) are chosen to be orthogonal, the inner product will be zero whenever \\(k+1&lt;i\\). This means that we only need to orthogonalize each \\(w_{i+1}’\\) with respect to \\(v_i\\) and \\(v_{i-1}\\); the rest will already be orthogonal. This property shows that \\(T = V^TAV\\) (with entry \\((i,j)\\) of the form \\(v_i^TAv_j\\)) must be tridiagonal as required.\nFinally, note that \\[\n\\begin{align*}\n\\langle v_{i-1} , w_{i+1}' \\rangle &= \\langle Av_{i-1}, v_i \\rangle \\\\\n&= \\langle w_i', v_i \\rangle \\\\\n&= \\langle w_i + c_1 v_{i-1} + c_2 v_{i-2}, v_i \\rangle \\\\\n&= \\langle w_i , v_i \\rangle \\\\\n&= \\langle w_i , w_i \\rangle / \\|w_i\\| = \\|w_i\\|\n\\end{align*}\n\\]\nWe already calculated this quantity when normalizing to get \\(v_i\\), so this saves us an extra multiplication at each step.\n\n\nThe Conjugate Gradients Algorithm\nThe conjugate gradients algorithm computes \\(A^{-1}b\\) using a similar strategy. While the Lanczos algorithm makes \\(v_i\\) orthogonal to the previous \\(v_j\\), the conjugate gradients algorithm makes it \\(A\\)-orthogonal. We say that \\(v_i\\) and \\(v_j\\) are \\(A\\)-orthogonal when \\(v_i^TAv_j = 0\\). I will also write \\(v_i^TAv_j\\) as \\(\\langle v_i, v_j \\rangle_A\\).\nSay there was a matrix where \\(u = \\sum_t c_t v_t\\) for \\(A\\)-orthogonal \\(v_t\\). Then \\(\\langle v_i,u \\rangle_A = c_i \\langle v_i, v_i \\rangle_A\\), so \\(c_i = \\frac{\\langle v_i u \\rangle_A}{\\langle v_i, v_i \\rangle_A}\\). This means we can make \\(u\\) \\(A\\)-orthogonal to \\(v_i\\) by subtracting \\(\\frac{\\langle v_i u \\rangle_A}{\\langle v_i, v_i \\rangle_A} v_i\\).\nThis lets conjugate gradients compute the vectors \\(v_i\\) as follows: \\[\n\\begin{align*}\nw_{i+1}' &= Av_i \\\\\nw_{i+1} &= w_{i+1}' - \\sum_{k=1}^i \\frac{\\langle v_k, w_{i+1}'\\rangle_A}{\\langle v_k, v_k\\rangle_A}v_k \\\\\nv_{i+1} &= w_{i+1}/\\|w_{i+1}\\|\n\\end{align*}\n\\] This is exactly the same as the computation of the \\(v_i\\) in the Lanczos algorithm except that the inner products use the \\(A\\) metric.\nOnce we have an \\(A\\)-orthogonal basis like this, we can find an \\(x\\) to minimize \\(\\|x - A^{-1}b\\|_A\\) (which is equivalent to finding \\(A^{-1}b\\) if it exists). This is the same as minimizing \\(x^TAx -2x^Tb\\). Expressing \\(x\\) as \\(\\sum_i \\alpha_i v_i\\) and using A-orthogonality gives \\[\n\\sum_i (\\alpha_i)^2 v_i^TAv_i - 2\\alpha_iv_i^Tb\n\\] The derivative is zero when \\(\\alpha_i = \\frac{v_i^Tb}{v_i^TAv_i}\\). This lets us solve \\(A^{-1}b\\) in a linear number of matrix vector multiplications. If multiplication by \\(A\\) is \\(O(n^2)\\), then this isn’t any better than \\(O(n^3)\\) Gaussian elimination. But it’s often the case that multiplication by \\(A\\) is much faster than that, especially if it’s sparse or Toeplitz or comes from a Kronecker product."
  },
  {
    "objectID": "posts/hoplists/HopLists.html",
    "href": "posts/hoplists/HopLists.html",
    "title": "Hop Lists",
    "section": "",
    "text": "Introduction\nHop Lists are a novel retroactive set data-structure that allow for a branching timeline. Each hop list node \\(h_t\\) is associated with a specific time \\(t\\) and a randomly chosen height \\(L_t\\). The interface consists of three methods:\n\n\\(\\text{current}(h_t)\\) gets the set of elements we would see at time \\(t\\).\n\\(\\text{advance}(h_t)\\) creates a new node \\(h_{t+1}\\) allowing queries about the set at time \\(t+1\\).\n\\(\\text{push}(h_t, v)\\) pushes the value \\(v\\) into the set at time \\(t\\). This value will now appear in the sets associated with all future times \\(t' &gt;t\\).\n\nHop lists nodes store four fields: a set of underlying type \\(S\\), a pointer to a predecessor node with heigh at least \\(L_t\\), a list of the most recent nodes at each height, and a list of pointers to specific future nodes.\n@kwdef struct HopNode{S}\n    \"The new elements since `pred`.\"\n    set::S = S()\n    \"The most recent node at the same height as this one or higher\"\n    pred::Union{Nothing,HopNode{S}} = nothing\n    \"(Node, height) pairs sorted by height giving the most recent node at that height\"\n    levels::LinkedList{Pair{HopNode{S}, Int}} = nil(Pair{HopNode{S}, Int})\n    \"Nodes to update when this node gets updated\"\n    succs::Vector{HopNode{S}} = HopNode{S}[]\nend\nHop lists maintain the Predecessor Property: If hop node \\(h_t\\) has predecessor \\(h_s\\), then \\(h_t\\)’s set must store all the elements pushed at times in the interval \\((s, t]\\).\nThis means we can find \\(\\text{current}(h_t)\\) by taking the union of \\(h_t\\)’s ancestors.\n# Iteration jumps back through `pred` edges\nBase.iterate(h::HopNode, s::HopNode=h) = (s, s.pred)\nBase.iterate(h::HopNode, ::Nothing) = nothing\nBase.IteratorSize(::Type{HopNode{S}}) where {S} = Base.SizeUnknown()\nBase.eltype(::Type{HopNode{S}}) where {S} = HopNode{S}\n\ncurrent(h::HopNode) = mapreduce(a -&gt; a.set, union, h)\nWhen we create a new hop node \\(h_{t'} = \\text{advance}(h_t)\\), we will set the predecessor to be h_t.levels[l] where \\(l = L_{t'} \\sim \\text{Geom}(0.5)\\). To ensure that we maintain the predecessor property, we must take the the union of all the predecessor sets we find this way and store them in the new node’s set. The new levels list should remove all entries below \\(L_t\\) and add \\(h_t\\).\nfunction advance(h::HopNode)\n    n = 1 + rand(Geometric(0.5))\n    pred = getpred(h.levels, n)\n    itr = takewhile(x-&gt;x!=pred, h)\n    result = HopNode(;pred, set=mapreduce(a-&gt;a.set, union, itr))\n    result.levels = cons(result=&gt;n, listdrop(h.levels, n))\n    result\nend\nThis uses the utility functions listdrop and getpred\nfunction listdrop(l::LinkedList{Pair{A,Int}}, k::Int) where {A}\n    while !isempty(l)\n        (_,a) = l.head\n        a &gt; k && return l\n        l = l.tail\n    end\n    l\nend\n\nfunction getpred(l::LinkedList{Pair{A,Int}}, n::Int) where {A}\n    pred = nothing\n    for (p, height) in l\n        if height &gt;= n\n            pred = p\n            break\n        end\n    end\n    pred\nend\nFor example, if we inserted 1 at time 1, 2 at time 2, and so on up to 6, we might get a HopNode structure that looks like this The black arrows here correspond to pred pointers, the x axis corresponds to time, and the \\(y\\) axis gives the height \\(L_t\\) of each node \\(h_t\\).\n\n\n\nexample\n\n\nThe tricky part is handling \\(\\text{push}\\). We need to give each node \\(h_s\\) pointers to all future nodes \\(h_t\\) for which h_s.set \\(\\subseteq\\) h_t.set That way, when we push into \\(h_s\\), we know to push into \\(h_t\\) as well. This list of pointers will be our succs vector. The idea results in the following code.\nfunction Base.push!(t::HopNode{S}, v) where {S}\n    q = HopNode{S}[t]\n    while !isempty(q)\n        t = pop!(q)\n        push!(t.set, v)\n        append!(q, t.succs)\n    end\nend\nWe still need to create these succs pointers in the first place. Each node should have an element of succs pointing to the closest future node with a higher height if one exists.\nTo fit these requirements, we can modify the advance method as follows:\nfunction advance(h::HopNode)\n    n = 1 + rand(Geometric(0.5))\n    pred = getpred(h.levels, n)\n    itr = takewhile(x-&gt;x!=pred, h)\n    result = HopNode(set=mapreduce(a-&gt;a.set, union, itr))\n    for t in itr\n        push!(t.succs, result)\n    end\n    result.levels = cons(result=&gt;n, listdrop(h.levels, n))\n    result\nend\nWith the succs pointers visualized in red, the previous example looks as follows:\n\n\n\nexample2\n\n\nNote that advance can be called twice on the same node \\(h_t\\), producing a branching timeline. Updates to \\(h_t\\) will be propagated to both possible futures. This is why we need succs to be a vector rather than simply an optional pointer.\n\n\nAverage Time and Space Complexity\nIf the size of our timeline is \\(n\\), we’ll have on average \\(n\\) nodes with height \\(\\geq 1\\), \\(n/2\\) nodes with height \\(\\geq 2\\), and so on up to \\(1\\) node with height \\(\\log n\\). If we perform a current query from a node at height \\(1\\), it takes on average \\(2\\) hops through predecessor nodes to get to a node with height \\(\\geq 2\\). This means that after at most \\(2 \\log n\\) hops on average we should be at the node with height \\(\\log n\\) which has no predecessors. Therefore, the average number of sets we must union to answer a current query is \\(O(\\log n)\\) in expectation.\nThe average time complexity for push can be found analogously. The push operation follows succ pointers, where the successor to a node is the closest future node with a higher height, if one exists. As traversing each succ pointer takes us to a higher height, the time complexity of push is just the largest height of any node in our timeline, which on average is also \\(O(\\log n)\\).\nThe same logic allows us to find space complexity. Say we store at most \\(c\\) elements in each time-slot. We know that the set associated with any time \\(t\\) will be replicated at most \\(\\log n\\) times. So we use at most \\(cn \\log n = O(n \\log n)\\) space for the set fields. For the levels field, each HopNode creates a single linked list node for its levels list, so this contributes \\(O(n)\\) space. Each node’s succs field will contain at most one element if the timeline does not branch, so once again we get a linear space contribution. This gives total space complexity \\(O(n \\log n)\\).\n\n\nConcentration Bounds\nWe know from the previous section that the time it takes to insert an element is at most the maximum height of any node in the timeline. The probability that the maximum height of any node in a timeline is above \\(k\\) is \\[\n\\begin{align*}\n&1 - \\prod_{i=1}^n P(h_i \\text{ has height below $k$}) \\\\\n&= 1 - (1 - 2^{-k})^n\n\\end{align*}\n\\] For \\(k=2\\log_2 n\\), we get \\[\n1 - \\left(1 - \\frac{1}{n^2}\\right)^n\n\\] But \\(\\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n^2}\\right)^n = 1\\). So the probability of insertion being any worse than \\(2\\log_2 n\\) goes to zero.\nTo bound the number of backward hops taken by current queries, we can find the probability it takes \\(\\leq k\\) hops to iterate backwards from a node \\(h_n\\) with height \\(1\\). We can lower bound this by the probability that it takes \\(\\leq k/L\\) hops to get to a node with height 2, times the probability it takes \\(\\leq k/L\\) hops to get to a node with height 3, and so on up to the maximum height \\(L\\). This is \\[\n(1 - 2^{-k/L})^L\n\\] For \\(k = 2L\\log_2 L\\), we get the probability \\[\n\\left(1 - \\frac{1}{L^2}\\right)^L\n\\] As \\(L \\to \\infty\\) this converges to \\(1\\), meaning that the probability a current query takes more than \\(2 L \\log_2L\\) time falls to zero.\n\n\nHeight-Free Variant\nWe can construct a variant of the data structure described that does not use a levels list. Instead, when we create a new hop node \\(h_{t'} = \\text{advance}(h_t)\\), we will set the predecessor by sampling \\(n \\sim \\text{Geom}(0.5)\\) and then taking \\(n\\) predecessor hops back from \\(h_t\\). Specifically, we would have\nfunction advance(t::HopList2)\n    n = rand(Geometric(0.5))\n    itr = Iterators.drop(Iterators.take(t, 1 + n), 1)\n    result = HopList2()\n    set, pred = reduce(itr; init=(nothing, t)) do (s, p), a\n        push!(p.succs, result)\n        (s ∪ p.set, a)\n    end\n    result.set = set\n    result.pred = pred\n    result\nend\nAnalysis of this variant is more difficult. Let the number of hops back to the start of time from node \\(h_t\\) be given by \\(X_t\\). It’s easy to see that \\[\n\\begin{align*}\nX_0 &= 0 \\\\\nX_t &= \\max(0, X_{t-1} + 1 - G_t)\n\\end{align*}\n\\] where \\(G_t \\sim \\text{Geom}(0.5)\\). Simulating samples from this stochastic process seems to indicate that \\(X_t\\) scales as \\(\\sqrt{t}\\) rather than \\(\\log t\\) as in the original structure. But insertions into the height-free variant seem to be much faster than those into the original structure in practice. Thorough analysis of why this is the case remains to be done.\n\n\nExtensions\nWhile I have introduced these datastructures as retroactive set, they can compute partial sums of arbitrary monoids. For example, you can use them to compute prefix sums of a changing list of numbers."
  },
  {
    "objectID": "posts/synth_control/SyntheticControl.html",
    "href": "posts/synth_control/SyntheticControl.html",
    "title": "Synthetic Controls for Texas Prison Data",
    "section": "",
    "text": "This post uses a synthetic control design to study whether Texas’s prison building boom in 1993 resulted in them incarcerating more prisoners than they would have if their rate of prison building had continued as normal. The analysis will build off the one in the book Causal Inference: The Mixtable, but will make use of techniques from Gelman’s Bayesian workflow.\nAs in the Mixtape book, we’ll express Texas’s incarceration rates over time as a mixture of those of other states with comparable poverty, alcoholiosm and incarceration rates in ’93. Then, we’ll use this mixture of other states to construct a “synthetic” counterfactual Texas which, like the states in the mixture, didn’t have a prison building boom after ’93. We can compare the difference between the true and synthetic Texas over time.\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch import nn\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.kernels import ScaleKernel, RBFKernel\nfrom gpytorch.distributions import MultivariateNormal\nfrom torch.distributions import Normal\nfrom gpytorch.likelihoods.noise_models import HomoskedasticNoise\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import zscore\n\n\ntexas = pd.read_stata(\"https://github.com/scunning1975/mixtape/raw/master/texas.dta\")\ntexas = texas[texas.state != 'District of Columbia']\n\nOur endogenous variable \\(y\\) will be the black male incarceration rate. I’ll use a zscore for ease of training hyperparameters.\n\ntexas['incarceration_rate'] = texas.bmprison / texas.bmpop\ntexas['incarceration_z'] = texas.groupby('year').incarceration_rate.transform(zscore)\ntexas['poverty'] = texas['poverty'] / 100\ntexas = texas.sort_values([\"state\", \"year\"])\ndata = np.reshape(texas[[\"incarceration_z\", \"poverty\", \"alcohol\"]].values, (50, 16,-1))\n\nLooking at the data shows a pretty sizeable increase in Texas’s incarceration rate after ’93 compared to other states, which lends credence to the hypothesis.\n\nyears = pd.unique(texas.year)\nstates = pd.unique(texas.state)\ntexix = list(states).index(\"Texas\")\n\nfor ix, r in enumerate(data):\n    if ix == texix:\n        plt.plot(years, r[:,0], color=\"C0\")\n    else:\n        plt.plot(years, r[:,0], color=\"C1\" , alpha=0.1)\n\n\n\n\n\n\n\n\nTo construct our synthetic version of Texas, we’ll fit a Gaussian process model relating state characteristics in 1993 (incarceration rate, poverty and alcoholism levels) to incarceration rates for other years. Then we’ll apply the fitted model to Texas’s characeristics to predict the counterfactual incarceration rate.\n\nX = torch.from_numpy(data[:,7,:]).float()\nY = torch.from_numpy(data[:,:7,0]).float()\n\nclass SynthControlModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=3))\n        self.noise = HomoskedasticNoise()\n\n    def forward(self, x):\n        covar_x = self.noise(x) + self.covar_module(x)\n        return MultivariateNormal(torch.zeros(covar_x.shape[0]), covar_x)\n\n\nmodel = SynthControlModel()\n\nThe following code fits the noise and lengthscale parameters by maximum likelihood estimation.\n\nopt = torch.optim.Adam(model.parameters(), lr=0.1)\n\nfor i in range(200):\n    opt.zero_grad()\n    loss = -model(X).log_prob(Y.T).sum()\n    loss.backward()\n    if i % 20 == 0:\n        print(loss.item())\n    opt.step()\n\n390.9499206542969\n221.37319946289062\n212.59107971191406\n211.1005401611328\n210.900634765625\n210.87908935546875\n210.87689208984375\n210.87579345703125\n210.87570190429688\n210.87567138671875\n\n\nThe fitted noise and lengthscales seem reasonable.\n\nmodel.noise.noise.detach()\n\ntensor([0.0906])\n\n\n\nmodel.covar_module.base_kernel.lengthscale.detach()\n\ntensor([[1.6206, 0.9185, 1.6307]])\n\n\nPrediction with a Gaussian process is just a matter of cranking through the Gaussian conditioning equations. I’m avoiding gpytorch’s high level interface here, as we’ll need the additional flexibility that comes from doing this manually.\n\ndef predict(X, test_X, Y):\n    with torch.no_grad():\n        prior_x = model(X)\n        prior_t = model(test_X)\n        X_tk = model.covar_module.forward(test_X, X)\n        mu = X_tk @ torch.linalg.solve(prior_x.covariance_matrix, Y)\n        S = prior_t.covariance_matrix - X_tk @ torch.linalg.solve(prior_x.covariance_matrix, X_tk.T)\n        return MultivariateNormal(mu.T, S)\n\nHere, then, is our counterfactual prediction for Texas’s incarceration rate based on the training data from all the other states. There’s a lot of uncertainty shown by the blue shading, but we can guess the general trend.\n\nnon_texas = np.delete(np.arange(50), texix)\nX_no_texas = torch.from_numpy(data[non_texas, 7]).float()\ny_no_texas = torch.from_numpy(data[non_texas,:,0]).float()\nX_texas = torch.from_numpy(data[None,texix, 7]).float()\n\npredictions = predict(X_no_texas, X_texas, y_no_texas)\nlower, upper = predictions.confidence_region()\n\nplt.fill_between(years, lower[:,0].numpy(), upper[:,0].numpy(), alpha=0.5)\nplt.plot(years, data[texix,:,0]);\nplt.axvline(x=1993, c='red');\n\n\n\n\n\n\n\n\nTexas fits the model just fine pre-1993, but quickly deviates after its building boom. We can show just how atypical this lack of fit is using a randomization test. We’ll collect the log likelihoods of each state’s post ’93 observations in the posterior predictive we obtained from fitting on the other states. The fraction of states with likelihoods below that of Texas gives us a p-value: the probability of seeing a fit as bad as Texas’s if the treatment effect were zero.\n\npost_93 = torch.from_numpy(data[:,8:,0]).float()\nstate_probs = []\nfor i in range(50):\n    ixs = np.delete(np.arange(50), i)\n    post_prob = predict(X[ixs], X[i:i+1], post_93[ixs]).log_prob(post_93[i:i+1].T)\n    state_probs.append(post_prob.sum())\nstate_probs = torch.stack(state_probs, axis=0)\n(list(torch.argsort(state_probs)).index(texix) + 1) / len(state_probs)\n\n0.02\n\n\nIf we’re okay with a 2% false positive rate, we can reject the null hypothesis that the treatment effect is zero.\nWe can examine the contribution of top states towards our synthetic version of texas as follows:\n\ndef state_contrib(model):\n    X_kk = model.covar_module.forward(X_no_texas, X_no_texas)\n    X_kt = model.covar_module.forward(X_no_texas, X_texas)\n    return torch.linalg.solve(X_kk, X_kt)[:,0]\n\ncs = state_contrib(model)\nixs = torch.argsort(-cs)[:3]\nlist(zip(states[non_texas][ixs], cs[ixs].detach()))\n\n[('South Dakota', tensor(2.1369)),\n ('Virginia', tensor(1.6360)),\n ('Missouri', tensor(1.5237))]\n\n\n\nsimilar_states = data[non_texas][ixs,:,0]\n\nWe can overlay the trajectory of the similar states themselves below:\n\nfor r in similar_states:\n    plt.plot(years, r, color=\"C1\", alpha=0.3)\nplt.fill_between(years, lower[:,0].numpy(), upper[:,0].numpy(), alpha=0.5)\nplt.plot(years, data[texix,:,0]);\nplt.axvline(x=1993, c='red');\n\n\n\n\n\n\n\n\nJust because the Gaussian process model fits Texas worse than any other state, however, doesn’t mean it’s actually fitting the other states all that well.\nTo see how well the model fits the other states, we can do some leave-one-out cross validation. Specifically, we can check that the observed incarceration distribution for past and future years is similar to that of samples from the leave-one-out posterior predictive distribution. Below, the orange line shows the observed distribution of incarceration rate z scores, while the blue lines are leave-one-out posterior predictive samples.\n\nresults = []\nfor i in range(49):\n    ixs = np.delete(np.arange(49), i)\n    post = predict(X_no_texas[ixs], X_no_texas[i:i+1], y_no_texas[ixs]).sample(torch.Size([20]))\n    results.append(post)\nresults = torch.concat(results, axis=-1)\n\nfor i in range(20):\n    sns.kdeplot(results[i].ravel(), color='C0', alpha=0.2)\nsns.kdeplot(y_no_texas.ravel(), color='C1');\n\n\n\n\n\n\n\n\nThe fit is pretty good, but the wiggle on the right side of the orange plot indicates that our data gives somewhat higher incarceration rates than the predictive posterior would suggest. Let’s break this down by year.\n\nfig, axs = plt.subplots(4, 2)\nfor (dist, obs, ax) in zip(results.permute((1, 0, 2)), y_no_texas[:,8:].T, axs.ravel()):\n    for i in range(20):\n        sns.kdeplot(dist[i], color='C0', alpha=0.2, ax=ax)\n    sns.kdeplot(obs, color='C1', ax=ax);\nplt.tight_layout()\n\n\n\n\n\n\n\n\nIn earlier years, everything seems okay. But by ’98, that extra little hump starts appearing on the right. While other states weren’t increasing their rates as fast as Texas was, it does seem like the model isn’t quite describing them correctly.\nWe can validate this lack of fit further by checking probability-integral-transform values. The fraction of the leave-one-out posterior predictive distribution below each observation would be uniform under perfect caliration.\n\ndef to_univariate(post):\n    return Normal(post.mean, post.covariance_matrix.ravel())\n\nresults = []\nfor i in range(49):\n    ixs = np.delete(np.arange(49), i)\n    post = predict(X_no_texas[ixs], X_no_texas[i:i+1], y_no_texas[ixs])\n    results.append(to_univariate(post).cdf(y_no_texas[i:i+1]))\nresults = torch.concat(results, axis=-1)\n\nsns.displot(results.ravel())\n\n\n\n\n\n\n\n\nThe posterior predictive distributions seem to be quite under-dispersed relative to the observations. We need a better model! But I’ll leave that to a future post."
  },
  {
    "objectID": "posts/Tooling Guide.html",
    "href": "posts/Tooling Guide.html",
    "title": "An Opinionated Tooling Guide",
    "section": "",
    "text": "Statistics and Data Analysis: Overall: use R. Its has the largest ecosystem of statistical packages.\n\nFor Bayesian modeling, Stan has bindings for everything, but it’s easiest to use from R, and the rest of the toolkit (brms, loo, bayesplot, etc) doesn’t have an equivalent outside of R. Turing.jl and PyMC3 are substantially slower, and Numpyro is less mature (although it can be faster on a GPU).\nFor visualization, R has ggplot2 and ggpairs, Julia has AlgebraOfGraphics and Python has Seaborn. They’re all about as good as each other.\nThe data manipulation, R has the tidyverse, Python has polars and Julia has Dataframes.jl. I actually find Dataframes.jl to be the nicest of the bunch (with DataframesMeta). Polars makes you learn a whole separate DSL instead of using the base language, and the rest of the Python ecosystem uses Pandas, so you constantly have to convert back and forth. Pandas itself its a nonstarter - generally slow, verbose, and hard to use.\nFor geographic queries, R’s sf and Julia’s GeoStats are lovely. Python’s GeoPandas, being based on Pandas, is much more annoying to use.\nFor frequentist statistics, Python’s statsmodels and Julia’s GLMs, MixedModels and Survival packages give you much of base R and its lmer and survival packages, but neither Julia nor Python have anything like survey.\nFor Gaussian Process modeling, Python’s GPytorch is far faster and more feature-complete than anything else. Julia’s AbstractGPs has a nicer interface and is easier to extend, but it’s slower and lacks the approximate inference algorithms necessary for large datasets.\n\nNeural Nets\n\nThere’s a plethora of SAAS companies that promise easier experiment tracking. But in my experience, the easiest option is just to use git worktrees. Each experiment should live in a separate branch with a README documenting exactly what’s being tested. This makes it easy to fork and cherry-pick pieces of different experiments. Use git-lfs for weights and training data.\nFor training, use Jax with equinox.\nFor visualization, use Tensorboard.\n\nOther Programming\n\nUse Julia for anything numerical\nUse Python for scripting\nUse Rust if you want to be super-efficient.\nOtherwise, use Haskell.\n\nText Editors: Currently, I use Zed on a Mac. It’s very fast, and has excellent support for language servers and LLM assisted editing. When I used arch linux with xmonad, I favored kakoune (which has an order of magnitude smaller binary size and memory use) but kakoune really needs a tiling window manager to work well.\nNotebooks: I use Pluto for Julia, Marimo for Python, and Quarto (in RStudio) for R. It would be nice if there was a unified tool for this kind of thing- maybe Positron will get there eventually.\nStatic Site Generation: Use Quarto. Compared to Jekyll, niceties like latex math, computational notebooks and bibliography management are built in.\nDatabases - For OLTP, use postgres with pgvector & pgvectorscale for vector search and postGIS for spatial data. - For analytical workloads, use DuckDB - For local RAGs, use ChromaDB.\nDocuments - If you’re the only one editing it, use Typora. - If you need to collaborate with others, use Google Docs.\nOther - Zulip is much nicer than Slack. - Ollama is great for local LLMs. - Pixi is essential for package management across languages."
  },
  {
    "objectID": "posts/nearest_neighbor_gps/nngp.html",
    "href": "posts/nearest_neighbor_gps/nngp.html",
    "title": "Nearest Neighbor Gaussian Processes",
    "section": "",
    "text": "using KernelFunctions, LinearAlgebra, SparseArrays, AbstractGPs\nusing CairoMakie, AbstractGPsMakie\nCairoMakie.enable_only_mime!(\"png\")\nusing ParameterHandling, Optim, Zygote\nIn a \\(k\\)-Nearest Neighbor Gaussian Process, we assume that the input points \\(x\\) are ordered in such a way that \\(f(x_i)\\) is independent of \\(f(x_j)\\) whenever \\(i &gt; j + k\\). When \\(k=2\\), for example, this means we can generate the sequence of process values by sampling the first value, then sampling the second given the first, then the third given the first two, then the fourth given the second and third, and so on.\n\\[\n\\begin{align*}\nf_1 &\\sim p(f_1) \\\\\nf_2 &\\sim p(f_2 | f_1) \\\\\nf_3 &\\sim p(f_3 | f_1, f_2) \\\\\nf_4 &\\sim p(f_4 | f_2, f_3) \\\\\n\\dotsc\n\\end{align*}\n\\]\nThe conditional distribution for \\(f_i\\) with \\(k\\)-predecessors in the set \\(S\\) has mean \\(K_{i, S}K_{S,S}^{-1} f_S\\) and variance \\(K_{i, i} - K_{i, S}K_{S,S}^{-1} K_{i,S}\\). This means we can write the generation procedure as\n\\[\n\\begin{align*}\nf_1 &= \\eta_1 \\\\\nf_2 &= K_{2, 1}K_{1,1}^{-1}f_1 + \\eta_2 \\\\\nf_3 &= K_{3, (2,1)}K_{(2,1),(2,1)}^{-1}f_{(2,1)} + \\eta_3 \\\\\nf_4 &= K_{4, (3,2)}K_{(3,2), (3,2)}^{-1}f_{(3,2)} + \\eta_3 \\\\\n\\dotsc\n\\end{align*}\n\\] where \\(\\eta_i \\sim \\mathcal{N}(0, K_{i, i} - K_{i, S}K_{S,S}^{-1} K_{i,S})\\). In matrix form, this means \\[\n\\begin{align*}\nf &= Bf + \\eta\nf &= (I - B)^{-1}\\eta\n\\end{align*}\n\\] where \\(B\\) is the strictly lower triangular matrix that comes from stacking zero-padded rows of the form \\(K_{i, S}K_{SS}^{-1}\\). This shows that \\(f\\) has a precision matrix of the form \\(UU^T\\) where \\(L=(I - B)^TF^{-1/2}\\) and \\(F\\) is diagonal."
  },
  {
    "objectID": "posts/nearest_neighbor_gps/nngp.html#implementation",
    "href": "posts/nearest_neighbor_gps/nngp.html#implementation",
    "title": "Nearest Neighbor Gaussian Processes",
    "section": "Implementation",
    "text": "Implementation\nWe assume that pts are in order, and that each point is independent of all the previous ones given the previous k.\n\nfunction make_B(pts::AbstractVector{T}, k::Int, kern::Kernel) where {T}\n    n = length(pts)\n    js = Int[]\n    is = Int[]\n    vals = T[]\n    for i in 1:n\n        if i == 1\n            ns = T[]\n        else\n            ns = pts[max(1, i - k):i-1]\n        end\n        row = kernelmatrix(kern, ns) \\ kern.(ns, pts[i])\n        start_ix = max(i - k, 1)\n        col_ixs = start_ix:(start_ix+length(row)-1)\n        append!(js, col_ixs)\n        append!(is, fill(i, length(col_ixs)))\n        append!(vals, row)\n    end\n    sparse(is, js, vals, n, n)\nend\n\nmake_B (generic function with 1 method)\n\n\nTo the understand the form of the B matrix described above more clearly, consider its form in a 2-nearest neighbor Gaussian Process.\n\npts = [1.0, 2.0, 3.5, 4.2, 5.9, 8.0]\n\n6-element Vector{Float64}:\n 1.0\n 2.0\n 3.5\n 4.2\n 5.9\n 8.0\n\n\n\nkern = SqExponentialKernel()\n\nSquared Exponential Kernel (metric = Distances.Euclidean(0.0))\n\n\n\nB = make_B(pts, 2, kern)\n\n6×6 SparseMatrixCSC{Float64, Int64} with 9 stored entries:\n   ⋅          ⋅          ⋅          ⋅          ⋅         ⋅ \n  0.606531    ⋅          ⋅          ⋅          ⋅         ⋅ \n -0.242002   0.471434    ⋅          ⋅          ⋅         ⋅ \n   ⋅        -0.184647   0.842651    ⋅          ⋅         ⋅ \n   ⋅          ⋅        -0.331424   0.495153    ⋅         ⋅ \n   ⋅          ⋅          ⋅        -0.0267458  0.116556   ⋅ \n\n\nThe \\(F\\) matrix can be constructed analogously.\n\nfunction make_F(pts::AbstractVector{T}, k::Int, kern::Kernel) where {T}\n    n = length(pts)\n    vals = T[]\n    for i in 1:n\n        prior = kern(pts[i], pts[i])\n        if i == 1\n            push!(vals, prior)\n        else\n            ns = pts[max(1, i - k):i-1]\n            ki = kern.(ns, pts[i])\n            push!(vals, prior - dot(ki, kernelmatrix(kern, ns) \\ ki))\n        end\n    end\n    Diagonal(vals)\nend\n\nmake_F (generic function with 1 method)\n\n\n\nF = make_F(pts, 2, kern)\n\n6×6 Diagonal{Float64, Vector{Float64}}:\n 1.0   ⋅         ⋅         ⋅         ⋅         ⋅ \n  ⋅   0.632121   ⋅         ⋅         ⋅         ⋅ \n  ⋅    ⋅        0.857581   ⋅         ⋅         ⋅ \n  ⋅    ⋅         ⋅        0.356873   ⋅         ⋅ \n  ⋅    ⋅         ⋅         ⋅        0.901874   ⋅ \n  ⋅    ⋅         ⋅         ⋅         ⋅        0.987169\n\n\nThe associated covariance matrix has the form \\((I-B)^{-1}F(I-B)^{-1}\\). We can compare this approximation to the full (non nearest-neighbor) covariance matrix.\n\nL = (I - B) \\ sqrt(F)\n\n6×6 Matrix{Float64}:\n  1.0          0.0          0.0        0.0        0.0      0.0\n  0.606531     0.79506      0.0        0.0        0.0      0.0\n  0.0439369    0.374819     0.926056   0.0        0.0      0.0\n -0.0749706    0.169036     0.780342   0.597388   0.0      0.0\n -0.0516836   -0.0405252    0.0794716  0.295798   0.94967  0.0\n -0.00401888  -0.00924444  -0.011608   0.0184994  0.11069  0.993564\n\n\n\nL * L'\n\n6×6 Matrix{Float64}:\n  1.0          0.606531     0.0439369  -0.0749706    -0.0516836  -0.00401888\n  0.606531     1.0          0.324652    0.0889216    -0.0635677  -0.00978746\n  0.0439369    0.324652     1.0         0.782705      0.0561348  -0.0143912\n -0.0749706    0.0889216    0.782705    1.0           0.235746    0.000731802\n -0.0516836   -0.0635677    0.0561348   0.235746      1.0         0.110251\n -0.00401888  -0.00978746  -0.0143912   0.000731802   0.110251    1.0\n\n\n\nkernelmatrix(kern, pts)\n\n6×6 Matrix{Float64}:\n 1.0          0.606531     0.0439369   0.00597602   6.11357e-6   2.28973e-11\n 0.606531     1.0          0.324652    0.0889216    0.000497955  1.523e-8\n 0.0439369    0.324652     1.0         0.782705     0.0561348    4.00653e-5\n 0.00597602   0.0889216    0.782705    1.0          0.235746     0.000731802\n 6.11357e-6   0.000497955  0.0561348   0.235746     1.0          0.110251\n 2.28973e-11  1.523e-8     4.00653e-5  0.000731802  0.110251     1.0\n\n\nThe two are pretty close!"
  },
  {
    "objectID": "posts/nearest_neighbor_gps/nngp.html#interface",
    "href": "posts/nearest_neighbor_gps/nngp.html#interface",
    "title": "Nearest Neighbor Gaussian Processes",
    "section": "Interface",
    "text": "Interface\nTo make this usable with Julia’s AbstractGPs library, we’ll add a new method for the posterior function.\n\nstruct NearestNeighbors\n    k::Int\nend\n\n\nstruct InvRoot{A}\n    U::A\nend\n\n\nAbstractGPs.diag_Xt_invA_X(A::InvRoot, X::AbstractVecOrMat) = AbstractGPs.diag_At_A(A.U' * X)\n\n\nfunction AbstractGPs.posterior(nn::NearestNeighbors, fx::AbstractGPs.FiniteGP, y::AbstractVector)\n    kern = fx.f.kernel\n    F = make_F(fx.x, nn.k, kern)\n    B = make_B(fx.x, nn.k, kern)\n    U = UpperTriangular((I - B)' * inv(sqrt(F)))\n    δ = y - mean(fx)\n    α = U * (U' * δ)\n    C = InvRoot(U)\n    AbstractGPs.PosteriorGP(fx.f, (α=α, C=C, x=fx.x, δ=δ))\nend\n\nHere’s how we use it:\n\ny = sin.(pts)\n\n6-element Vector{Float64}:\n  0.8414709848078965\n  0.9092974268256817\n -0.35078322768961984\n -0.8715757724135882\n -0.373876664830236\n  0.9893582466233818\n\n\n\nfx = GP(kern)(pts, 0.0)\n\nAbstractGPs.FiniteGP{AbstractGPs.GP{AbstractGPs.ZeroMean{Float64}, KernelFunctions.SqExponentialKernel{Distances.Euclidean}}, Vector{Float64}, LinearAlgebra.Diagonal{Float64, FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}}}}(\nf: GP{ZeroMean{Float64}, SqExponentialKernel{Distances.Euclidean}}(ZeroMean{Float64}(), Squared Exponential Kernel (metric = Distances.Euclidean(0.0)))\nx: [1.0, 2.0, 3.5, 4.2, 5.9, 8.0]\nΣy: Diagonal(Fill(0.0, 6))\n)\n\n\nNote that the nearest neighbor approximation requires a noise-free GP.\n\npost = posterior(NearestNeighbors(5), fx, y)\n\nAbstractGPs.PosteriorGP{GP{ZeroMean{Float64}, SqExponentialKernel{Distances.Euclidean}}, @NamedTuple{α::Vector{Float64}, C::InvRoot{UpperTriangular{Float64, SparseMatrixCSC{Float64, Int64}}}, x::Vector{Float64}, δ::Vector{Float64}}}(GP{ZeroMean{Float64}, SqExponentialKernel{Distances.Euclidean}}(ZeroMean{Float64}(), Squared Exponential Kernel (metric = Distances.Euclidean(0.0))), (α = [0.44959487401574766, 0.634378019439147, 0.31422639172541256, -1.1208602827978673, -0.23967552424377783, 1.0165902480943727], C = InvRoot{UpperTriangular{Float64, SparseMatrixCSC{Float64, Int64}}}([1.0 -0.7628739783668902 … 0.06024322535476907 -0.006784618661573238; 0.0 1.2577665549971213 … -0.12457526770687782 0.0140347496063293; … ; 0.0 0.0 … 1.0570415005635936 -0.12359893384397007; 0.0 0.0 … 0.0 1.0068137091164393]), x = [1.0, 2.0, 3.5, 4.2, 5.9, 8.0], δ = [0.8414709848078965, 0.9092974268256817, -0.35078322768961984, -0.8715757724135882, -0.373876664830236, 0.9893582466233818]))\n\n\n\nplot(1:0.01:8, post)"
  },
  {
    "objectID": "posts/nearest_neighbor_gps/nngp.html#optimizing-hyperparameters",
    "href": "posts/nearest_neighbor_gps/nngp.html#optimizing-hyperparameters",
    "title": "Nearest Neighbor Gaussian Processes",
    "section": "Optimizing Hyperparameters",
    "text": "Optimizing Hyperparameters\nThe last thing necessary to make this technique usable in practice is to ensure it works with autodifferentiation so that we can optimize hyperparameters like lengthscales.\n\ninitial_params = (var=positive(1.0), lengthscale=positive(1.0))\n\n(var = ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8), lengthscale = ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8))\n\n\n\nflat_initial_params, unflatten = ParameterHandling.value_flatten(initial_params)\n\n([-1.490116130486996e-8, -1.490116130486996e-8], ParameterHandling.value ∘ ParameterHandling.var\"#unflatten_to_NamedTuple#flatten##13\"{Float64, @NamedTuple{var::ParameterHandling.Positive{Float64, typeof(exp), Float64}, lengthscale::ParameterHandling.Positive{Float64, typeof(exp), Float64}}, ParameterHandling.var\"#unflatten_to_Tuple#flatten##11\"{Float64, Int64, Int64, ParameterHandling.var\"#unflatten_to_Tuple#flatten##11\"{Float64, Int64, Int64, ParameterHandling.var\"#unflatten_to_empty_Tuple#flatten##12\"{Float64, Tuple{}}, ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}}, ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}}}((var = ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8), lengthscale = ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8)), ParameterHandling.var\"#unflatten_to_Tuple#flatten##11\"{Float64, Int64, Int64, ParameterHandling.var\"#unflatten_to_Tuple#flatten##11\"{Float64, Int64, Int64, ParameterHandling.var\"#unflatten_to_empty_Tuple#flatten##12\"{Float64, Tuple{}}, ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}}, ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}}(1, 1, ParameterHandling.var\"#unflatten_to_Tuple#flatten##11\"{Float64, Int64, Int64, ParameterHandling.var\"#unflatten_to_empty_Tuple#flatten##12\"{Float64, Tuple{}}, ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}}(0, 1, ParameterHandling.var\"#unflatten_to_empty_Tuple#flatten##12\"{Float64, Tuple{}}(()), ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}(ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8), ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}())), ParameterHandling.var\"#unflatten_Positive#flatten##19\"{Float64, ParameterHandling.Positive{Float64, typeof(exp), Float64}, ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}}(ParameterHandling.Positive{Float64, typeof(exp), Float64}(-1.490116130486996e-8, exp, 1.4901161193847656e-8), ParameterHandling.var\"#unflatten_to_Real#flatten##2\"{Float64, Float64}()))))\n\n\n\nfunction build_gp(params)\n    k2 = params.var * with_lengthscale(kern, params.lengthscale)\n    GP(k2)(pts, 0.0)\nend\n\nbuild_gp (generic function with 1 method)\n\n\n\nfunction objective(flat_params)\n    params = unflatten(flat_params)\n    fx = build_gp(params)\n    -logpdf(fx, y)\nend\n\nobjective (generic function with 1 method)\n\n\n\ntraining_results = Optim.optimize(\n    objective,\n    θ -&gt; only(Zygote.gradient(objective, θ)),\n    flat_initial_params,\n    BFGS(\n        alphaguess=Optim.LineSearches.InitialStatic(scaled=true),\n        linesearch=Optim.LineSearches.BackTracking(),\n    ),\n    inplace=false,\n)\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     4.130829e+00\n\n * Found with\n    Algorithm:     BFGS\n\n * Convergence measures\n    |x - x'|               = 2.86e-07 ≰ 0.0e+00\n    |x - x'|/|x'|          = 4.80e-07 ≰ 0.0e+00\n    |f(x) - f(x')|         = 2.19e-12 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 5.31e-13 ≰ 0.0e+00\n    |g(x)|                 = 4.43e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    11\n    f(x) calls:    18\n    ∇f(x) calls:   12\n\n\nWith optimized parameters, we get much less uncertainty in our predictions.\n\nplot(1:0.01:8, posterior(NearestNeighbors(5),\n    build_gp(unflatten(training_results.minimizer)), y))"
  },
  {
    "objectID": "posts/coffee/coffee.html",
    "href": "posts/coffee/coffee.html",
    "title": "Analyzing Coffee Yields",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\nThis is rstanarm version 2.32.2\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n  options(mc.cores = parallel::detectCores())\n\noptions(mc.cores = 4)\n\nThis post demonstrates working with generalized linear mixed models in the context of coffee bean yield data. Each row in the following dataset is an observation of coffee bean yield from a single farm over the past year. Note that this dataset comes from cluster sampling. First, a random sample of supply units were chosen. Then, within each supply unit, a random sample of farms in the same cluster is taken. The number of farms sampled from each sampling unit is proportional to the size of the sampling unit, making sample means unbiased estimates of population means.\n\ndata &lt;- read_csv(\"coffee.csv\")\n\nNew names:\nRows: 1168 Columns: 19\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(7): supply_unit, region, obs_shade, ipm_methods, last_replanted, last_... dbl\n(10): ...1, farm_altitude_meters, coffee_farm_size_ha, main_stems_count,... lgl\n(2): had_recent_soil_or_leaf_test, used_test_results\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\ndata |&gt; select(ipm_methods, region, supply_unit, obs_shade)\n\n# A tibble: 1,168 × 4\n   ipm_methods                                      region supply_unit obs_shade\n   &lt;chr&gt;                                            &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;    \n 1 replanting maintain_field_hygiene biological_co… sabana sabana_nor… light_sh…\n 2 replanting maintain_field_hygiene biological_co… sabana sabana_nor… light_sh…\n 3 maintain_field_hygiene                           sabana sabana_nor… medium_s…\n 4 replanting maintain_field_hygiene                sabana sabana_nor… medium_s…\n 5 maintain_field_hygiene                           sabana sabana_nor… medium_s…\n 6 maintain_field_hygiene                           sabana sabana_nor… light_sh…\n 7 maintain_field_hygiene                           sabana sabana_nor… medium_s…\n 8 maintain_field_hygiene                           sabana sabana_nor… medium_s…\n 9 replanting maintain_field_hygiene                sabana sabana_nor… medium_s…\n10 maintain_field_hygiene                           sabana sabana_nor… minimal_…\n# ℹ 1,158 more rows\n\n\nWe’ll need to do a little pre-processing first. The column listing pest management techniques (ipm_methods) can contain multiple treatment names, separated by a space. We’ll expand these into binary categories.\n\nextract_categories &lt;- function(data, s) {\n  data |&gt; separate_longer_delim(cols = {{ s }}, delim=\" \") |&gt; mutate(value = 1) |&gt;\n  pivot_wider(\n    names_from = {{ s }},\n    values_from = value,\n    values_fill = 0\n  )\n}\n\nWe’ll also convert the obs_shade and last_replanted columns to factors.\n\ndata &lt;- data |&gt;\n  mutate(\n    obs_shade=strtoi(str_match(obs_shade, \"_(\\\\d+)_percent\")[,2]),\n    last_replanted=factor(last_replanted,\n                          levels=c(\"more_than_5_years_ago\", \"2_5_years_ago\", \"within_the_last_two_years\"),\n                          ordered=TRUE),\n    obs_shade=factor(obs_shade, ordered = TRUE)) |&gt;\n  extract_categories(ipm_methods)\n\nNow, on to the main question: does shade decrease yield? The following plot certainly suggests it does.\n\ndata |&gt; ggplot(aes(obs_shade, yield_kg_ha_green)) + geom_boxplot()\n\n\n\n\n\n\n\n\nBut there’s likely confounders here. Higher altitude farms might both get less shade and have soils less amenable to high yields.\n\ndata |&gt; ggplot(aes(scale(farm_altitude_meters), log(yield_kg_ha_green))) + geom_point() + facet_wrap(~obs_shade) + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s try modeling this more carefully.\n\nfit &lt;- lmer(log(yield_kg_ha_green) ~ obs_shade + (1|region/supply_unit) + scale(farm_altitude_meters), data=data)\ntdat &lt;- tibble(resid=residuals(fit))\ntdat |&gt; ggplot(aes(resid)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nA log linear model seems a little skewed.\n\nggplot(tdat,aes(sample=resid)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\n\n\nA GLM with a Gamma family works better here.\n\nfit2 &lt;- glmer(yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) + (1|region/supply_unit), data=data, family=Gamma(link=\"log\"))\nplot(fit2)\n\n\n\n\n\n\n\n\n\ntibble(resid=residuals(fit2)) |&gt; ggplot(aes(sample=resid)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\n\n\n\nsummary(fit2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Gamma  ( log )\nFormula: yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) +  \n    (1 | region/supply_unit)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n 17386.2  17431.8  -8684.1  17368.2     1159 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4288 -0.7563 -0.2222  0.5141  5.3585 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n supply_unit:region (Intercept) 0.02685  0.1639  \n region             (Intercept) 0.01471  0.1213  \n Residual                       0.45488  0.6744  \nNumber of obs: 1168, groups:  supply_unit:region, 20; region, 8\n\nFixed effects:\n                             Estimate Std. Error t value Pr(&gt;|z|)    \n(Intercept)                  6.485595   0.092593  70.044   &lt;2e-16 ***\nobs_shade.L                 -0.233903   0.106632  -2.194   0.0283 *  \nobs_shade.Q                  0.020627   0.088579   0.233   0.8159    \nobs_shade.C                  0.045673   0.059993   0.761   0.4465    \nobs_shade^4                  0.009596   0.039745   0.241   0.8092    \nscale(farm_altitude_meters) -0.037419   0.022113  -1.692   0.0906 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) obs_.L obs_.Q obs_.C obs_^4\nobs_shade.L  0.135                            \nobs_shade.Q  0.289  0.415                     \nobs_shade.C  0.122  0.692  0.362              \nobs_shade^4  0.143  0.162  0.403  0.230       \nscl(frm_l_)  0.002  0.146 -0.033  0.028  0.019\n\n\nWe see a pretty clear negative linear relationship between shade coverage and yield. To decrease the variance of our estimate, we can try controlling for other factors. Let’s factor in fertilizer use.\n\ndata |&gt; ggplot(aes(scale(average_fertilizer_per_hectare), log(yield_kg_ha_green))) + geom_point() + geom_smooth(span=0.2)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIt seems like fertilizer has a highly nonlinear relationship with yield. Let’s just look at quantiles.\n\ndata &lt;- data |&gt; mutate(fertilizer_quantile = ntile(average_fertilizer_per_hectare, 4))\n\n\ndata |&gt; ggplot(aes(fertilizer_quantile, log(yield_kg_ha_green))) + geom_bar(stat=\"identity\")\n\n\n\n\n\n\n\n\n\nfit3 &lt;- glmer(yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) + fertilizer_quantile + (1|region/supply_unit), data=data, family=Gamma(link=\"log\"))\nsummary(fit3)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Gamma  ( log )\nFormula: yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) +  \n    fertilizer_quantile + (1 | region/supply_unit)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n 17220.0  17270.6  -8600.0  17200.0     1158 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4805 -0.6998 -0.1895  0.4592  6.5325 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n supply_unit:region (Intercept) 0.023739 0.15408 \n region             (Intercept) 0.006918 0.08318 \n Residual                       0.417381 0.64605 \nNumber of obs: 1168, groups:  supply_unit:region, 20; region, 8\n\nFixed effects:\n                            Estimate Std. Error t value Pr(&gt;|z|)    \n(Intercept)                  5.88611    0.08835  66.625   &lt;2e-16 ***\nobs_shade.L                 -0.24805    0.09991  -2.483   0.0130 *  \nobs_shade.Q                  0.00149    0.08341   0.018   0.9857    \nobs_shade.C                  0.01520    0.05654   0.269   0.7881    \nobs_shade^4                  0.02244    0.03741   0.600   0.5485    \nscale(farm_altitude_meters) -0.03483    0.02060  -1.691   0.0909 .  \nfertilizer_quantile          0.23374    0.01742  13.418   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) obs_.L obs_.Q obs_.C obs_^4 sc(__)\nobs_shade.L  0.136                                   \nobs_shade.Q  0.293  0.414                            \nobs_shade.C  0.140  0.691  0.364                     \nobs_shade^4  0.127  0.161  0.402  0.227              \nscl(frm_l_) -0.010  0.151 -0.043  0.013  0.019       \nfrtlzr_qntl -0.483 -0.008 -0.020 -0.039  0.027  0.010\n\n\nThe AIC and residual variance decreased, and the standard error for shade’s negative linear effect shrunk even smaller.\n\nplot(fit3)\n\n\n\n\n\n\n\n\nFinally, we can add in factors for the pest control efforts we extracted initially.\n\nfit4 &lt;- glmer(yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) + fertilizer_quantile + use_insect_traps + biological_control + (1|region/supply_unit), data=data, family=Gamma(link=\"log\"))\nsummary(fit4)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Gamma  ( log )\nFormula: yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) +  \n    fertilizer_quantile + use_insect_traps + biological_control +  \n    (1 | region/supply_unit)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n 17217.0  17277.8  -8596.5  17193.0     1156 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4889 -0.6999 -0.1899  0.4687  6.6757 \n\nRandom effects:\n Groups             Name        Variance Std.Dev.\n supply_unit:region (Intercept) 0.022751 0.15084 \n region             (Intercept) 0.007078 0.08413 \n Residual                       0.411851 0.64176 \nNumber of obs: 1168, groups:  supply_unit:region, 20; region, 8\n\nFixed effects:\n                             Estimate Std. Error t value Pr(&gt;|z|)    \n(Intercept)                  5.877484   0.088270  66.585   &lt;2e-16 ***\nobs_shade.L                 -0.250834   0.099695  -2.516   0.0119 *  \nobs_shade.Q                  0.005447   0.083375   0.065   0.9479    \nobs_shade.C                  0.020206   0.056476   0.358   0.7205    \nobs_shade^4                  0.021293   0.037376   0.570   0.5689    \nscale(farm_altitude_meters) -0.031682   0.020572  -1.540   0.1236    \nfertilizer_quantile          0.231400   0.017450  13.260   &lt;2e-16 ***\nuse_insect_traps             0.186580   0.093333   1.999   0.0456 *  \nbiological_control           0.121985   0.086255   1.414   0.1573    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) obs_.L obs_.Q obs_.C obs_^4 sc(__) frtlz_ us_ns_\nobs_shade.L  0.135                                                 \nobs_shade.Q  0.294  0.414                                          \nobs_shade.C  0.140  0.690  0.365                                   \nobs_shade^4  0.128  0.163  0.404  0.228                            \nscl(frm_l_) -0.012  0.151 -0.041  0.015  0.020                     \nfrtlzr_qntl -0.476 -0.003 -0.016 -0.039  0.032  0.006              \nus_nsct_trp -0.012  0.012  0.053  0.045  0.027  0.040  0.003       \nblgcl_cntrl -0.039 -0.033 -0.045 -0.009 -0.052  0.035 -0.084 -0.099\n\n\nInterestingly, controlling for pest mitigation methods makes the effect of altitude seem much less significant. An analysis of deviance lets us compare the nested models.\n\nanova(fit2, fit3, fit4)\n\nData: data\nModels:\nfit2: yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) + (1 | region/supply_unit)\nfit3: yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) + fertilizer_quantile + (1 | region/supply_unit)\nfit4: yield_kg_ha_green ~ obs_shade + scale(farm_altitude_meters) + fertilizer_quantile + use_insect_traps + biological_control + (1 | region/supply_unit)\n     npar   AIC   BIC  logLik deviance    Chisq Df Pr(&gt;Chisq)    \nfit2    9 17386 17432 -8684.1    17368                           \nfit3   10 17220 17271 -8600.0    17200 168.2306  1     &lt;2e-16 ***\nfit4   12 17217 17278 -8596.5    17193   6.9538  2     0.0309 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe difference in deviances (likelihood ratio) for model 4 is still significant."
  },
  {
    "objectID": "posts/conjugate_computation/Conjugate-Computation.html",
    "href": "posts/conjugate_computation/Conjugate-Computation.html",
    "title": "Conjugate Computation",
    "section": "",
    "text": "This post is about a technique that allows us to use variational message passing on models where the likelihood doesn’t have a conjugate prior. There will be a lot of Jax code snippets to make everything as concrete as possible."
  },
  {
    "objectID": "posts/conjugate_computation/Conjugate-Computation.html#the-math",
    "href": "posts/conjugate_computation/Conjugate-Computation.html#the-math",
    "title": "Conjugate Computation",
    "section": "The Math",
    "text": "The Math\nSay \\(X\\) comes from a distribution with density \\(q(X;\\theta)\\), and we want to find \\(\\theta\\) to maximize \\(E[g(X)]\\). While in gradient descent we let \\(\\theta_{t+1}=\\theta_t + \\alpha \\nabla_\\theta E[g(X)]\\), the natural gradient update is \\(\\theta_{t+1}=\\theta_t + \\alpha F^{-1}\\nabla_\\theta E[g(X)]\\), where \\(F\\) is the Fisher Information Matrix. When \\(q\\) is the density for an exponential family (which is the only form of \\(q\\) I will consider here), \\(F\\) is the Hessian of the log normalizer \\(A(\\theta)\\).\nVariational inference seeks a \\(\\theta\\) that maximizes the ELBO \\(\\mathcal{L} = f(\\theta) + E_{q_\\theta}[\\log p(X) - \\log q(X)]\\) where the expected likelihood \\(f(\\theta) = E_{q_\\theta} \\log p(y \\vert  X)\\). When \\(p\\) and \\(q\\) come from an exponential family with statistic \\(T(X)\\) and natural parameters \\(\\eta\\) and \\(\\theta\\) respectively, taking the gradient of the ELBO gives\n\\[\n\\begin{aligned}\n\\nabla_\\theta L &= \\nabla_\\theta f + \\nabla_\\theta \\langle E[T(X)], \\eta - \\theta \\rangle + \\nabla_{\\theta} A(\\theta) \\\\\n&= \\nabla_\\theta f + \\nabla_\\theta \\langle \\nabla A(\\theta), \\eta - \\theta \\rangle + \\nabla_{\\theta} A(\\theta) \\\\\n&= \\nabla_\\theta f + \\nabla_\\theta^2A(\\theta)(\\eta - \\theta) \\\\\n\\end{aligned}\n\\]\nThis means that the natural gradient is \\(F^{-1} \\nabla_\\theta f + (\\eta - \\theta)\\).\nWhen the likelihood is conjugate to the prior, \\(\\log p(y\\vert x)\\) can be written as \\(\\langle T(X), \\phi(y) \\rangle + c\\), where \\(c\\) does not depend on \\(x\\) and \\(\\phi\\) is a transformation of the observations. This means that \\(\\log p(y\\vert x) + \\log p(x) = \\langle T(x), \\phi(y) + \\eta \\rangle + c\\). Taking the gradient as above, we get \\[\n\\begin{align*}\n\\nabla_\\theta L &= \\nabla_\\theta \\langle E[T(X)], \\phi(y) + \\eta - \\theta \\rangle + \\nabla_{\\theta} A(\\theta) \\\\\n&= \\nabla_\\theta^2A(\\theta)(\\phi(y) + \\eta - \\theta) \\\\\n\\end{align*}\n\\]\nThis means that the natural gradient is \\(\\phi(y) + \\eta - \\theta\\); a unit length step along the natural gradient recovers the conjugate update.\nWhen \\(q\\) factors as \\(q(X_1; \\theta_1)q(X_2; \\theta_2)\\) and non-leaf children in \\(p\\) are always conjugate to their parents, we can update each component separately. Say \\(f\\) is a function of \\(X_1\\) alone, \\(\\eta_1\\) is a function of \\(X_2\\), and \\(\\eta_2\\) is a constant. The gradient of the ELBO with respect to \\(\\theta_1\\) becomes \\[\n\\begin{align*}\n\\nabla_{\\theta_1} L &= \\nabla_{\\theta_1} f + \\nabla_{\\theta_1} \\langle E[T(X_1)], E[\\eta_1(X_2)] - \\theta_1 \\rangle + \\nabla_{\\theta_1} A(\\theta_1) \\\\\n&= \\nabla_{\\theta_1} f + \\nabla_{\\theta_1}^2A(\\theta_1)( E[\\eta_1(X_2)] - \\theta_1) \\\\\n\\end{align*}\n\\]\nSimilarly, the gradient for \\(\\theta_2\\) is $ _{_2}^2A(_2)(E[(X_1)] + _2 - _2)$. This coordinate-wise natural gradient update is called “variational message passing”.\nWe can find \\(F^{-1} \\nabla_\\theta f\\) by using the “conjugate computation” trick. Let \\(\\hat{\\theta}\\) be the expected-statistic parameterization corresponding to natural parameters \\(\\theta\\). Observe that, by the chain rule:\n\\[\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial \\hat{\\theta}}\\frac{\\partial \\hat{\\theta}}{\\partial \\theta}\\]\nAs \\(\\hat{\\theta} = \\nabla_\\theta A(\\theta)\\), this simiplies to \\[\\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial \\hat{\\theta}} \\nabla_\\theta^2 A(\\theta) \\] So \\[ \\nabla_\\theta^{-2} A(\\theta) \\frac{\\partial f}{\\partial \\theta} = \\frac{\\partial f}{\\partial \\hat{\\theta}} \\] That is, the natural gradient with respect to the natural parameterization is just the ordinary gradient with respect to the mean parameterization."
  },
  {
    "objectID": "posts/conjugate_computation/Conjugate-Computation.html#the-code",
    "href": "posts/conjugate_computation/Conjugate-Computation.html#the-code",
    "title": "Conjugate Computation",
    "section": "The Code",
    "text": "The Code\nTo see how this works in practice, consider doing variational inference for a hierarchical latent Gaussian mixture model. Let our observations \\(y_i\\) have log density \\(f(y_i; \\mu_{c_i})\\) where \\(f\\) is some complicated function like a neural net and \\(c_i\\) is the known mixture component responsible for \\(z_i\\). The \\(\\mu_j \\sim \\mathcal{N}(m_{d_j}, \\tau_2)\\), where \\(d_j\\) is the known mixture component responsible for \\(\\mu_j\\), and \\(m_k \\sim \\mathcal{N}(0, \\tau_3)\\). In our variational approximation, we will assume the \\(\\mu_j\\) and \\(m_k\\) are all independently Gaussian distributed with natural parameters \\(\\theta_j\\) and \\(\\theta_k\\) respectively. Then the natural gradient update for \\(\\mu_j\\) is \\[F^{-1} \\nabla_{\\theta_j} f + \\begin{bmatrix} \\tau_2 m_{d_j} \\\\ -\\tau_2/2 \\end{bmatrix} - \\theta_j\\] The update for \\(m_k\\) is \\[\\sum_j \\begin{bmatrix} \\tau_2 E[\\mu_j] \\\\ -\\tau_2/2 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -\\tau_3/2 \\end{bmatrix} - \\theta_k\\]\nIf the number of observations \\(N\\) is large, it may be hard to find \\(\\nabla_{\\theta_j} f = \\sum_i^N \\log p(y_i \\vert  \\mu_{c_i})\\) exactly. In this case, we can take a Monte Carlo estimate \\(N \\nabla_{\\theta_j} E_{i \\in [N]} \\log p(y_i \\vert  \\mu_{c_i})\\). Alternately, we can divide the learning rate by \\(N\\), so that the natural gradient is \\[\nF^{-1}_i \\nabla_{\\theta_i} \\log p(y_i \\vert  \\mu_{c_i}) + \\frac{1}{N} \\left( \\begin{bmatrix} \\tau_2 m_{d_j} \\\\ -\\tau_2/2 \\end{bmatrix} - \\theta_j \\right)\n\\] The same goes for the sums involved in the variational message passing updates.\nIn Jax, we’ll represent our posterior distributions in vectorized form; each Normal will actually represent a plate of \\(n\\) normal distributions. Plates of distributions will have methods for their means and natural gradient updates.\nfrom typing import NamedTuple\nfrom jaxtyping import Array, Float, Int\nimport jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jt, jax.nn\nimport matplotlib.pyplot as plt, numpy as np\nfrom einops import repeat\nimport equinox as eqx, equinox.nn as nn, optax, diffrax as dfx\nimport functools as ft\nclass Normal(NamedTuple):\n    nat_params: Float[Array, \"n ... 2\"]\n    n_obs: Int[Array, \"n\"]\n    lr: jnp.float32\n    prior_mean: \"Distribution\"\n    prior_prec: jnp.float32\n    parent_ixs: Int[Array, \"n\"]\n\n    def mean(self, ix):\n        return -0.5 * self.nat_params[ix, ..., 0] / self.nat_params[ix, ..., 1]\n\n    def mean_params(self, ix):\n        prec = -2 * self.nat_params[ix, ..., 1]\n        mu = self.mean(ix)\n        return jnp.stack((mu, (1 / prec) + jnp.square(mu)), axis=-1)\n\n    def update(self, df, ix):\n        mu = self.prior_mean.mean(ix)\n        ones = jnp.ones(mu.shape)\n        prior_msg = jnp.stack((self.prior_prec * mu,\n                               -self.prior_prec * ones / 2), axis=-1)\n        onestr = \"1 \" * len(df.shape[1:])\n        n_obs = repeat(self.n_obs, \"n -&gt; n \" + onestr)\n        nat_grad = df + (prior_msg - self.nat_params[ix]) / n_obs[ix]\n        self = self._replace(nat_params=self.nat_params.at[ix].add(self.lr * nat_grad))\n        parent_df = jnp.stack((self.prior_prec * self.mean(ix), - ones * self.prior_prec / 2), axis=-1)\n        prior_mean = self.prior_mean.update(parent_df, self.parent_ixs[ix])\n        return self._replace(prior_mean=prior_mean)\nWe’ll make a helper function to initialize one of these variational posterior plates given a plate of prior distributions.\ndef normal(mean_dist, prec, n_obs, parent_ixs, lr, key):\n    mean = mean_dist.mean(parent_ixs)\n    post_mu = mean + jnp.sqrt(1/prec) * jr.normal(key, mean.shape)\n    return Normal(normal_nats(post_mu, prec), n_obs, lr, mean_dist, prec, parent_ixs)\ndef normal_nats(mu, prec):\n    ones = jnp.ones(mu.shape)\n    return jnp.stack([prec * mu, -0.5 * prec * ones], axis=-1)\nThe prior mean for \\(m_k\\) is a delta distribution, so we’ll need a representation of this as well.\nclass Delta(NamedTuple):\n    val: Float[Array, \"k\"]\n\n    def mean(self, ix):\n        return repeat(self.val, '... -&gt; i ...', i=ix.shape[0])\n\n    def update(self, df, ix):\n        return self\ncolon = slice(None,None,None)"
  },
  {
    "objectID": "posts/conjugate_computation/Conjugate-Computation.html#example-simple-nonlinearity",
    "href": "posts/conjugate_computation/Conjugate-Computation.html#example-simple-nonlinearity",
    "title": "Conjugate Computation",
    "section": "Example: Simple Nonlinearity",
    "text": "Example: Simple Nonlinearity\nLet the expected likelihood \\(f\\) be a monte carlo estimate of a normal distribution around a transformation of \\(z\\) with precision \\(\\tau_1\\). We’ll have two \\(m\\)s, two \\(\\mu\\)s for each \\(m\\), and 20 \\(z\\)s for each \\(\\mu\\).\nTAU3 = 0.1\nTAU2 = 1\nTAU1 = 5\ndef init_posteriors(n_obs_ms, n_obs_mus, mu_parents, key):\n    key, key2 = jr.split(key, 2)\n    m = normal(Delta(jnp.float32(0)), jnp.float32(TAU3), n_obs_ms,\n        jnp.zeros(len(n_obs_mus), jnp.int32), 1e-3, key)\n    mu = normal(m, jnp.float32(TAU2),\n        n_obs_mus, mu_parents, 1e-3, key2)\n    return mu\ndata_key, post_key, obs_key, key = jr.split(jr.PRNGKey(1), 4)\nn_obs_ms = jnp.full(2, 2, jnp.int32)\nn_obs_mus = jnp.full(4, 20, jnp.int32)\nmu_parents = repeat(jnp.arange(2), 'a -&gt; (a b)', b=2)\ntrue_mu_dist = init_posteriors(n_obs_ms, n_obs_mus, mu_parents, data_key)\ntrue_z = true_mu_dist.mean(colon)[:,None] + jnp.sqrt(1 / TAU1) * jr.normal(obs_key, (4, 20))\ntrue_z = jnp.ravel(true_z)\nmu = init_posteriors(n_obs_ms, n_obs_mus, mu_parents, post_key)\ninit_mu = init_posteriors(n_obs_ms, n_obs_mus, mu_parents, post_key)\nplt.hist(list(jnp.reshape(true_z, (4,20))), stacked=True);\nplt.legend(labels=['A', 'B', 'C', 'D']);\nWe’ll keep the nonlinear log likelihood simple for now:\ndef trans_val(z):\n    return jnp.square(z)\n@jax.value_and_grad\ndef f(moments, y, key):\n    key, key2 = jr.split(key, 2)\n    std = jnp.sqrt(moments[1] - jnp.square(moments[0]))\n    mu_sample = moments[0] + std * jr.normal(key, std.shape)\n    noise = jnp.sqrt(1 / TAU1) * jr.normal(key2, std.shape)\n    z_sample = mu_sample + noise\n    obs_lik = -0.5 * jnp.sum(jnp.square(y - trans_val(z_sample)))\n    return obs_lik\ny = trans_val(true_z)\nplt.hist(list(jnp.reshape(y, (4, 20))), stacked=True);\nAlthough it’s unnecessary for this toy example, we’ll process the observations in batches to demonstrate how to handle a big-data setting.\norig_ixs = repeat(jnp.arange(4), 'a -&gt; (a b)', b=20)\npermkey, key = jr.split(key, 2)\nixs = jnp.reshape(jr.permutation(permkey, 80), (4, 20))\n@jax.jit\ndef step(mu, y, noise_key, ix):\n    keys = jr.split(noise_key, len(ix))\n    oix = orig_ixs[ix]\n    logprob, ng = jax.vmap(f)(mu.mean_params(oix), y[ix], keys)\n    return -jnp.mean(logprob), mu.update(ng, oix)\nfor epoch in range(100):\n    for ix in ixs:\n        noise_key, key = jr.split(key, 2)\n        loss, mu = step(mu, y, noise_key, ix)\n    if epoch % 10 == 0:\n        print(\"Loss\", loss)\nLoss 100.12155\nLoss 23.347706\nLoss 15.448761\nLoss 9.154531\nLoss 6.0349517\nLoss 8.66155\nLoss 6.0191417\nLoss 6.167668\nLoss 3.9419441\nLoss 4.059013\ntrue_mu_dist.mean(colon)\nArray([ 0.93337744, -0.959161  , -2.7729065 , -2.4498918 ], dtype=float32)\nmu.mean(colon)\nArray([-1.8313473, -1.6707655,  2.7105517,  2.4046152], dtype=float32)\nInference got pretty close to the true values (up to sign, which isn’t identifiable here). Compare this to where we started:\ninit_mu.mean(colon)\nArray([-7.319654  , -6.7709327 ,  1.2107476 ,  0.45153332], dtype=float32)"
  },
  {
    "objectID": "posts/conjugate_computation/Conjugate-Computation.html#example-simple-neural-nonlinearity",
    "href": "posts/conjugate_computation/Conjugate-Computation.html#example-simple-neural-nonlinearity",
    "title": "Conjugate Computation",
    "section": "Example: Simple Neural Nonlinearity",
    "text": "Example: Simple Neural Nonlinearity\nFor a slightly more sophisticated example, let’s try to generate some related sin-curves from noise. Our linlinearity will come from a neural net.\njax.clear_caches()\nRES = 64\ninputs = jnp.linspace(0, 6, RES)\npost_key, net_key, data_key, key = jr.split(jr.PRNGKey(1), 4)\nw_base = jnp.array([0.9, 1.15])[None, :]\nw_resid = jnp.array([0.4, -0.4])[:, None]\nw = w_base + w_base * w_resid\ny = jnp.sin(inputs[None] * (w[...,None,None] + 0.02 * (jr.normal(data_key, (2,2,20))[...,None])))\ny = jnp.reshape(y, (80, -1))\nfor ix, yi in enumerate(jnp.reshape(y, (4, 20, 64))):\n    for yj in yi:\n        plt.plot(inputs, yj, f\"C{ix}\")\nfor ix, yi in enumerate(jnp.reshape(y, (2, 40, 64))):\n    for yj in yi:\n        plt.plot(inputs, yj, f\"C{ix}\")\ndef init_posteriors(n_obs_ms, n_obs_mus, mu_parents, key):\n    key, key2 = jr.split(key, 2)\n    m = normal(Delta(jnp.zeros(RES)), jnp.float32(TAU3), n_obs_ms,\n        jnp.zeros(len(n_obs_mus), jnp.int32), 1e-2, key)\n    mu = normal(m, jnp.float32(TAU2),\n        n_obs_mus, mu_parents, 1e-3, key2)\n    return mu\n@ft.partial(eqx.filter_vmap, in_axes=((0, None), 0, 0))\n@eqx.filter_value_and_grad\ndef f(args, y, key):\n    moments, net = args\n    key, key2 = jr.split(key, 2)\n    std = jnp.sqrt(moments[:,1] - jnp.square(moments[:, 0]))\n    mu_sample = moments[:,0] + std * jr.normal(key, std.shape)\n    noise = jnp.sqrt(1 / TAU1) * jr.normal(key2, std.shape)\n    z_sample = mu_sample + noise\n    obs_lik = -0.5 * jnp.sum(jnp.square(y - net(z_sample)))\n    return -obs_lik\n@eqx.filter_jit\ndef step(mu, net, y, noise_key, ix, opt_state, opt_update):\n    keys = jr.split(noise_key, len(ix))\n    oix = orig_ixs[ix]\n    params = mu.mean_params(oix)\n    neg_logprob, grads = f((params, net), y[ix], keys)\n    ng, net_grads = grads\n    net_grad = jt.tree_map(lambda g: jnp.mean(g, axis=0), net_grads)\n    updates, opt_state = opt_update(net_grad, opt_state)\n    net = eqx.apply_updates(net, updates)\n    return jnp.mean(neg_logprob), mu.update(-ng, oix), net, opt_state\nmu = init_posteriors(n_obs_ms, n_obs_mus, mu_parents, post_key)\nnet = nn.MLP(RES, RES, 512, 3, activation=jax.nn.swish, key=net_key)\nlr=1e-3\nopt = optax.adabelief(learning_rate=lr)\nopt_state = opt.init(eqx.filter(net, eqx.is_inexact_array))\nfor epoch in range(500):\n    for ix in ixs:\n        noise_key, key = jr.split(key, 2)\n        loss, mu, net, opt_state = step(mu, net, y, noise_key, ix, opt_state, opt.update)\n    if epoch % 100 == 0:\n        print(\"Loss\", loss)\nparams = mu.mean(colon)\nfor p in params:\n    plt.plot(inputs, net(p))\nThe distances between \\(\\mu\\) values correspond to what we’d assume given the prior.\nplt.imshow(jnp.sum(jnp.square(params[:, None] - params[None, :]), axis=2))\nplt.colorbar()"
  },
  {
    "objectID": "posts/conjugate_computation/Conjugate-Computation.html#example-diffusion-nonlinearity",
    "href": "posts/conjugate_computation/Conjugate-Computation.html#example-diffusion-nonlinearity",
    "title": "Conjugate Computation",
    "section": "Example: Diffusion Nonlinearity",
    "text": "Example: Diffusion Nonlinearity\nWe can try to accomplish the same task using a ODE-based model. We learn a neural network describing a vector field. Integrating along this vector field maps from our latent space to the space of observations.\njax.clear_caches()\n@ft.partial(eqx.filter_vmap, in_axes=((0, None), 0, 0))\n@eqx.filter_value_and_grad\ndef f(args, y, key):\n    moments, net = args\n    key, key2, key3 = jr.split(key, 3)\n    std = jnp.sqrt(moments[:,1] - jnp.square(moments[:, 0]))\n    mu_sample = moments[:,0] + std * jr.normal(key, std.shape)\n    noise = jnp.sqrt(1 / TAU1) * jr.normal(key2, std.shape)\n    z_sample = mu_sample + noise\n    t = jr.uniform(key3)\n    pos = t * y + (1 - t) * z_sample\n    target = y - z_sample\n    obs_lik = -0.5 * jnp.sum(jnp.square(target - net(t, pos)))\n    return -obs_lik\nclass Net(eqx.Module):\n    net: nn.MLP\n    def __init__(self, key):\n        super().__init__()\n        self.net = nn.MLP(RES + 1, RES, 256, 3, activation=jax.nn.swish, key=key)\n    def __call__(self, t, y, *args):\n        t = repeat(t, \"-&gt; 1\")\n        return self.net(jnp.concatenate([y, t]))\nmu = init_posteriors(n_obs_ms, n_obs_mus, mu_parents, post_key)\nlr=1e-3\nnet = Net(net_key)\nopt = optax.adabelief(learning_rate=lr)\nopt_state = opt.init(eqx.filter(net, eqx.is_inexact_array))\nlosses = []\nfor epoch in range(3000):\n    for ix in ixs:\n        noise_key, key = jr.split(key, 2)\n        loss, mu, net, opt_state = step(mu, net, y, noise_key, ix, opt_state, opt.update)\n    if epoch % 100 == 0:\n        losses.append(loss)\nplt.plot(losses)\nplt.yscale('log');\nparams = mu.mean(colon)\n@ft.partial(jax.vmap, in_axes=(None, 0))\ndef forwardflow(net, z):\n    term = dfx.ODETerm(net)\n    solver = dfx.ReversibleHeun()\n    sol = dfx.diffeqsolve(term, solver, 0, 1, 1e-3, z)\n    return sol.ys[0]\nresults = forwardflow(net, params)\nfor p in results:\n    plt.plot(inputs, p)\nplt.imshow(jnp.sum(jnp.square(params[:, None] - params[None, :]), axis=2))\nplt.colorbar()\nOne of the nice things about ODE models is that we can use them for prediction as well as generation.\n@ft.partial(jax.vmap, in_axes=(None, 0))\ndef backflow(net, x):\n    term = dfx.ODETerm(net)\n    solver = dfx.ReversibleHeun()\n    sol = dfx.diffeqsolve(term, solver, 1, 0, -1e-3, x)\n    return sol.ys[0]\nWe’ll take one sample from each class.\nry = jnp.reshape(y, (4, 20, 64))\ntopy = ry[:,0,:]\nfor yi in topy:\n    plt.plot(inputs, yi)\nWe can integrate backwards through time to get the latent code associated with each sample.\nzs = backflow(net, topy)\nThe latent codes are well separated in Euclidean space.\nplt.imshow(jnp.sum(jnp.square(zs[:, None] - params[None, :]), axis=2))\nplt.colorbar();\nWe can visualize the embeddings with PCA.\nfrom numpy.linalg import eig\npmu = jnp.mean(params, axis=0)\nXbar = params - pmu[None,:]\nS = Xbar.T @ Xbar\nres = eig(S)\nproj = Xbar @ res[1][:, :2]\nplt.scatter(proj[:2, 0], proj[:2, 1])\nplt.scatter(proj[2:, 0], proj[2:, 1]);"
  },
  {
    "objectID": "posts/finite_particle_approx.html",
    "href": "posts/finite_particle_approx.html",
    "title": "Finite Particle Approximations",
    "section": "",
    "text": "Say you have a discrete distribution \\(\\pi\\) that you want to approximate with a small number of weighted particles. Intuitively, it seems like the the best choice of particles would be the outputs of highest probability under \\(\\pi\\), and that the relative weights of these particles should be the same under our approximation as they were under \\(\\pi\\). This actually isn’t hard to prove!\n\nMinimizing the KL Divergence\nLet \\(q\\) be our approximation: a version of \\(\\pi\\) with support restricted to \\(b\\) outcomes. We’ll try to minimize \\(KL[q, \\pi]\\). (Note that \\(KL[\\pi, q]\\) will always be infinite as we do not have $ q$, so using this distance isn’t an option). Treat \\(\\pi\\) and \\(q\\) as vectors in \\(\\mathbb{R}^n\\), and assume without loss of generality that \\(\\pi_1 \\geq \\pi_2 \\geq \\pi_3 \\dotsc\\).\nClaim 1: \\(KL[q, \\pi]\\) is minimized when the nonzero components of \\(q\\) have \\(q \\propto \\pi\\).\nProof: Use Lagrange multipliers. Let \\(S\\) be the support of \\(q\\). \\(L(\\lambda, q) = \\sum_{i \\in S} q_i \\log (q_i / \\pi_i) + \\lambda (1 - \\langle q, 1 \\rangle)\\). The minimum will be at a fixed point of the Lagrangian. Differentiating, we find that \\(\\log (q_i/\\pi_i) + (q_i/q_i) + \\lambda = 0\\) for all \\(i\\) in \\(q\\)’s support, or \\(\\pi_i / q_i =e^{\\lambda + 1}\\). As this proportionality constant is the same for all \\(i\\), this confirms that minimizer is proportional to \\(\\pi\\) whenever it is nonzero. \\(\\square\\)\nClaim 2: If \\(q \\propto m \\odot \\pi\\) where \\(\\odot\\) indicates point-wise multiplication and \\(m\\) is a binary mask with \\(\\|m\\|_0 = b\\) picking out the support of \\(q\\), then \\(\\arg \\min KL[q, \\pi]\\) is obtained when \\(m_i = 1\\) when \\(i \\leq b\\) and \\(m_i = 0\\) otherwise.\nProof: Let \\(P = m^T\\pi\\). The KL divergence simplifies as follows \\[\n\\begin{align*}\nKL[q, \\pi] &= \\frac{1}{P}\\sum_{i=1}^n \\pi_i m_i \\left( \\log (\\pi_i / P) - \\log \\pi_i\\right) \\\\\n&= -\\log(P)\n\\end{align*}\n\\] The \\(m\\) maximizing \\(m^T\\pi\\) is clearly the one with its mass on the \\(b\\) highest outcomes in \\(\\pi\\). \\(\\square\\)\n\n\nMinimizing Maximum Mean Discrepancy\nThe KL divergence isn’t the only way to measure distance between distributions. In fact, it’s not a particularly flexible way, as it only lets us compare our discrete distribution \\(q\\) with other discrete distributions \\(\\pi\\). Instead, we can minimize the “maximum mean discrepancy” or MMD. The big idea is to choose \\(q\\) to make \\(E_{X \\sim q}f(X)\\) as close as possible to \\(E_{X \\sim \\pi} f(X)\\) for all functions \\(f\\) in some space \\(\\mathcal{H}\\). \\[\n\\text{MMD} = \\sup_{f \\in \\mathcal{H}, \\|f\\| \\leq 1} (E_{X \\sim \\pi} f(X) - E_{X \\sim q} f(X))^2\n\\] If we assume that \\(\\mathcal{H}\\) is closed under negation, squaring this difference won’t change the optimal distribution \\(q^*\\) minimizing the expression, so we can equivalently write this as \\[\n\\sup_{f \\in \\mathcal{H}, \\|f\\| \\leq 1} (E_{X \\sim \\pi} f(X) - E_{X \\sim q} f(X))\n\\] If we let \\(\\mathcal{H}\\) be a reproducing kernel Hilbert space with kernel \\(k\\), function evaluation \\(f(x)\\) is the same as taking the inner product \\(\\langle f, k(x, \\cdot) \\rangle\\), and we can write the result as \\[\n\\sup_{f \\in \\mathcal{H}, \\|f\\| \\leq 1} (E_{X \\sim \\pi} \\langle f, k(X, \\cdot) \\rangle - E_{X \\sim q} \\langle f, k(X, \\cdot) \\rangle)\n\\] You can read more about kernel mean embeddings like this here.\nLet’s assume that \\(\\pi\\) is discrete, and we can use \\(\\mathbb{R}^n\\) as this Hilbert space. Letting \\(\\pi\\) and \\(q\\) also indicate vectors of component probabilities, the expression simplifies as \\[\n\\sup_{f \\in \\mathbb{R}^n, \\|f\\| \\leq 1} \\langle f, E_{X \\sim \\pi}[X], - E_{X \\sim q}[X] \\rangle = \\|\\pi - q\\|_2^2 \\\\\n\\] Our problem, therefore, is to project \\(\\pi\\) onto the space of all \\(q\\) for which \\(\\|q\\|_1 = 1\\) (the sum of components is 1) and \\(\\|q\\|_0 = b\\) (the total number of non-zero components is \\(b\\)).\nThis space is not convex, but we can get around this by fixing a binary mask vector \\(m\\) with 1-norm \\(b\\) and minimizing \\(\\|\\pi - m \\odot q\\|_2^2\\) over all unit \\(q\\), where \\(\\odot\\) indicates the Hadamard product. This gives us the Lagrangian \\[\nL(\\lambda, q) = \\|m \\odot q\\|^2 - 2 \\langle m \\odot \\pi, q \\rangle + \\lambda (\\langle q, m \\rangle - 1).\n\\] The derivative with respect to \\(q_i\\) is zero when \\(2m_i q_i - 2m_i \\pi_i + \\lambda m_i = 0\\) from which we can conclude that \\(q_i = \\pi_i + c\\) for some normalizing constant \\(c\\) shared by all components in the support.\nWe can plug in this result about the optimal \\(q_i\\) and solve to find the optimal \\(m_i\\). Let \\(S\\) be the support of \\(q\\) we get from \\(m\\) and \\(1\\) be the all ones vector. We want to minimize \\[\n\\begin{align*}\n\\text{MMD} &= \\|m\\pi + \\frac{\\langle 1-m, \\pi \\rangle}{b}m - \\pi\\|^2 \\\\\n&= \\sum_i \\left( 1_{i \\notin S} \\pi_i + 1_{i \\in S}\\frac{\\langle 1-m, \\pi \\rangle}{b}\\right)^2 \\\\\n&= (1 - b) + \\sum_{i \\notin S} \\pi_i^2\n\\end{align*}\n\\] Clearly, this quantity is minimized by choosing \\(S\\) to contain the \\(b\\) most likely components of \\(\\pi\\).\n\n\nUnbiased MMD Minimization\nThe approximation schemes we’ve looked at so far have been deterministic: to make a \\(b\\) particle approximation \\(q\\) to a more complicated distribution \\(\\pi\\), we always choose \\(q\\)’s support \\(S\\) to be the \\(b\\) most likely elements in \\(\\pi\\). But because of this determinism, these approximations are biased. By biased here, I mean that it is not generally the case that \\(E[\\sum_{i=1}^n q_i f(x_i)] \\neq E_{X \\sim \\pi} f(X)\\) for any function \\(f\\) unless \\(b\\) is large enough to capture the full support of \\(\\pi\\).\nIf this property (unbiased-ness) was more important to us than truly minimizing the maximum mean discrepancy, we could instead try to choose \\(S\\) stochastically. Specifically, we could do the following: - Assign each outcome \\(i\\) in \\(\\pi\\)’s support a weight \\(w_i\\). - Sample an outcome with probability proportional to the remaining weights. - Put that outcome in \\(S\\) and prevent it from being sampled again. - Repeat \\(b\\) times.\nWith this scheme, the number of times outcome \\(i\\) is included in \\(S\\) is given by Wallenius’ non-central hypergeometric distribution with success-outcomes parameter \\(m_1 =1\\), total outcomes parameter \\(N\\) being the size of \\(\\pi\\)’s support, draws parameter \\(n=b\\), and odds parameter \\(\\theta_i = \\frac{(n-1)w_i}{1 - w_i}\\). If we know the odds parameter \\(\\theta_i\\), we can get back \\(w_i\\) using \\(\\sigma(\\log(\\theta_i / (n-1)))\\) where \\(\\sigma\\) is the standard logistic function.\nOnce we choose the support set \\(S\\), we can deterministically assign probabilities to each outcome in \\(q\\) to minimize the maximum mean discrepancy from our target distribution \\(\\pi\\). As shown above, the optimal choice is to set \\(q_i = \\pi_i + c\\) where \\(c\\) is the equally distributed un-allocated probability mass \\(\\frac{1}{b} \\sum_{j \\notin S} \\pi_j\\).\nNow, we can choose the \\(w_i\\) for each outcome in such a way that \\(\\pi_i = E[q_i]\\), making \\(q\\) unbiased unlike the deterministic approximation discussed earlier. Let \\(s_i = P(i \\in S)\\) and \\(r_i = 1 - s_i\\). \\[\n\\begin{align*}\n\\pi_i &= E[1_{i \\in S}(\\pi_i + \\frac{1}{b}\\sum_j 1_{j \\notin S} P_j)] \\\\\n&= s_i(\\pi_i + \\frac{1}{b}\\sum_j \\pi_j (1 - s_j)) \\\\\nb(1-s_i)\\pi_i &= \\sum_j \\pi_j (1 - s_j) \\\\\n(b - 1)r_i \\pi_i &= \\sum_{j \\neq i} \\pi_j r_i\n\\end{align*}\n\\] This holds for all \\(i\\) simultaneously, so letting \\(P\\) be the matrix with \\(\\pi\\) along its diagonal and \\(1\\) be the all ones matrix, we can write the above equation in vectorized form as \\[\n\\begin{align*}\n(b-1)Pr &= (1-I)Pr \\\\\n((b-1)I - 1 + I)Pr &= 0 \\\\\n(bI - 1)Pr &= 0 \\\\\n\\end{align*}\n\\] We can see that if we find an eigenvector of \\((bI - 1)P\\) with eigenvalue zero, the entries will indicate the probability mass functions of our our non-central hypergeometric distributions at zero.\nHowever, as \\((bI - 1)\\) is nonsingular for nonzero \\(b\\), no such eigenvalue exists! This means this whole approach is actually impossible. We can’t randomly choose a support and then optimally pick the weights if we want the result to be unbiased. We can either choose optimal weights, or we can have an unbiased approximation. Having both at the same time is fundamentally impossible.\n\n\nQuasi Monte Carlo Approximation\nIf we’d prefer to have unbiased-ness to optimal weights, there are other finite particle approximation strategies we could use besides standard Monte Carlo. One simple approach is to use randomized lattice Quasi Monte Carlo. The basic idea goes like this: - Let \\(u_i = i/b\\) for \\(b\\) different particles. - Choose a random number \\(s\\) between 0 and 1 and add it to all the \\(u_i\\) mod 1. These are unbiased, evenly dispersed samples from a uniform distribution. - Now apply the inverse CDF \\(\\Phi\\) of \\(\\pi\\) to each of these samples. Here, we’re assuming an ordering among the possible outcomes; if \\(\\Phi(0.3) = 3\\), for example, that would mean that 30% of \\(\\pi\\)’s probability mass is for outcomes below 3. This is known as inverse transform sampling, and guarantees we’ll get unbiased samples from \\(\\pi\\)! We’ll set all the weights to be \\(1/b\\), just as in the standard Monte Carlo case.\nSome connections to think about: when the \\(\\pi\\) we’re trying to approximate is a the empirical distribution of particles during a round of sequential Monte Carlo, this approximation is known as systematic resampling. If, as is often the case in probabilistic programming, we’re not trying to sample from a single categorical distribution \\(\\pi\\), but rather a sequence of dependent distributions \\(\\pi^1, \\pi^2, \\dotsc\\), we can sample our uniform grid in the unit hypercube rather than along the unit interval and apply the inverse CDF of \\(\\pi^i\\) to the \\(i\\)th coordinates. In multiple dimensions, however, a uniform grid might not be the best idea. If we try to pick the lattice structure in a way that minimizes MMD between \\(q\\) and \\(\\pi\\), the best choice of lattice will depend on the class of functions \\(\\mathcal{H}\\) we’re considering for the MMD. For example, one common RKHS for multidimensional spaces has a kernel that is just the product of the kernels for each dimension. If the kernel for one of the dimensions has a lower bandwidth, it will make sense to pack our grid points more tightly in this dimension."
  },
  {
    "objectID": "posts/congestion_pricing/Notebook.html",
    "href": "posts/congestion_pricing/Notebook.html",
    "title": "Air Quality and Congestion Pricing",
    "section": "",
    "text": "This post dives into the data of a recent paper quantifying the effects of Manhattan’s recent congestion pricing scheme (Fraser et al. (2025)). The authors argue that congestion pricing decreased the average daily maximum PM 2.5 concentration by 3 μg/m^3. Obviously, this isn’t the kind of treatment effect one can show experimentally; they first had to estimate what particulate concentration would have been if congestion pricing hadn’t been put into place, and then take the difference between what was observed and what they predicted. I’ll be adapting their methodology to use a Bayesian workflow.\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(rstanarm)\nlibrary(\"bayesplot\")\noptions(mc.cores = parallel::detectCores())\nThe data below comes from air quality sensors throughout the NYC metropolitan area. For each day from January 1 2024 through June 1 2025, we observe the maximum PM 2.5 concentration at that sensor, along with environmental factors near that sensor that might influence the reading (e.g. distance from a highway, temperature, precipitation, population density, median income, etc). Sensors come from three different “areas”: the congestion reduction zone (abbreviated CRZ) where, starting in January 2025, congestion pricing was implemented, the five boroughs of New York (abbreviated NYC), and the rest of the greater metropolitan area (abbreviated CBSA).\ndata = read_rds(\"panel_daily_nyc.rds\") |&gt;\n  select(week, within, area, date, bgmean, value, treated, temp, precip,\n         pop_density, median_income) |&gt;\n  mutate(\n    date_trunc=floor_date(date, unit = \"weeks\", week_start = 1),\n    treated_crz = treated & within) |&gt;\n  drop_na()\nBelow, we’ll plot bands of 25% to 75% quantiles of weekly PM 2.5 concentration over time. The vertical bar indicates the start of the treatment period.\nquants &lt;- \\(spec, prefix) list(setNames(\n  quantile(c_across({{spec}}), c(0.25, 0.5, 0.75)),\n  paste(prefix, c(\"lo\", \"median\", \"hi\"), sep=\"_\")))\n\ntdate &lt;- min(filter(data, treated)$date_trunc)\n\ndata |&gt; group_by(area, date_trunc) |&gt;\n  summarize(q=quants(value, \"val\"), .groups = \"drop\") |&gt; unnest_wider(q) |&gt;\n  ggplot(aes(x=date_trunc, ymin=val_lo, ymax=val_hi, fill=area)) +\n    geom_ribbon(alpha=0.3) + geom_vline(xintercept = tdate)\nLooking over the graph gives us some important intuitions:"
  },
  {
    "objectID": "posts/congestion_pricing/Notebook.html#a-simple-difference-in-differences-model",
    "href": "posts/congestion_pricing/Notebook.html#a-simple-difference-in-differences-model",
    "title": "Air Quality and Congestion Pricing",
    "section": "A Simple Difference in Differences Model",
    "text": "A Simple Difference in Differences Model\nWith these intuitions in mind, we’ll start with the following simple model using the following assumptions:\n\nThere’s an overall time-trend affecting all three regions.\nThe effect of congestion pricing is homogeneous in time and affects only the CRZ.\n\n\nfit1 &lt;- stan_glm(sqrt(value) ~ factor(date_trunc) + treated_crz + area, data=data)\n\nLet’s compare this model’s predictions of what sensor readings without congestion pricing would have been like to what we observed in practice.\n\npreds &lt;- t(posterior_predict(fit1, mutate(data, treated_crz=FALSE))[sample.int(4000, 200),])^2\ncolnames(preds) &lt;- paste(\"pred\", 1:200, sep=\"\")\nbind_cols(data, as_tibble(preds)) |&gt;\n  group_by(area, date_trunc) |&gt;\n  summarize(across(value | starts_with(\"pred\"), mean)) |&gt;\n  rowwise() |&gt; mutate(q=quants(starts_with(\"pred\"), \"pred\")) |&gt; unnest_wider(q) |&gt;\n  ggplot(aes(date_trunc, value, ymin=pred_lo, ymax=pred_hi)) +\n    geom_line(aes(color=\"true value\")) + geom_ribbon(alpha=0.3, aes(fill=\"predicted HDI\")) +\n  geom_vline(xintercept = tdate) + facet_wrap(~area, nrow=3) + \n  scale_fill_manual(values = c(\"predicted HDI\" = \"blue\")) +\n  scale_color_manual(values = c(\"true value\" = \"red\"))\n\n\n\n\n\n\n\n\nThe predicted sensor readings generally match the true values before the treatment period, but are higher than what we observed after the treatment period within the CRZ. This suggests that congestion pricing did cause lower emissions. We can also see this in the posterior distribution of the treatment coefficient, with support almost entirely below zero.\n\nmcmc_hist(fit1, \"treated_crzTRUE\")"
  },
  {
    "objectID": "posts/congestion_pricing/Notebook.html#effects-beyond-the-crz",
    "href": "posts/congestion_pricing/Notebook.html#effects-beyond-the-crz",
    "title": "Air Quality and Congestion Pricing",
    "section": "Effects Beyond the CRZ",
    "text": "Effects Beyond the CRZ\nSo far, we’ve assumed that any treatment effect is localized to the CRZ area. But this may not be the case: perhaps emissions from vehicles driven in the CRZ carry over into other regions as well. If we can’t use areas outside the CRZ as controls, however, we’ll need to choose other sensor locations.\nThe original paper identified a handful of sensor locations known to be upwind of the CRZ. The average PM 2.5 levels of these sensor locations were identified as “background levels”. These levels are shown below.\n\ndata |&gt; group_by(area, date_trunc) |&gt;\n  summarize(bgmean=mean(bgmean)) |&gt;\n  ggplot(aes(date_trunc, bgmean)) + geom_line() + geom_vline(xintercept = tdate)\n\n\n\n\n\n\n\n\nIn our next model, we’ll let all three regions have treatment effects deviating from the linear trend identified by this background PM 2.5 level. We’ll also control for environmental effects.\n\nfit2 &lt;- stan_glm(sqrt(value) ~ bgmean + treated:area + area +\n                   log(temp) + precip + log(pop_density + 1 ) + log(median_income + 1),\n                 data=data)\n\n\nmcmc_intervals(fit2)\n\n\n\n\n\n\n\n\nFrom the posterior plot above, it doesn’t seem like areas outside the CRZ experienced treatment effects after all. Once again, we can plot the counterfactual particle levels from this model.\n\npreds &lt;- t(posterior_predict(fit2, mutate(data, treated=FALSE))[sample.int(4000, 1000),])^2\ncolnames(preds) &lt;- paste(\"pred\", 1:1000, sep=\"\")\npred_data &lt;- bind_cols(data, as_tibble(preds))\npred_data |&gt;\n  group_by(area, date_trunc) |&gt;\n  summarize(across(value | starts_with(\"pred\"), mean)) |&gt;\n  rowwise() |&gt; mutate(q=quants(starts_with(\"pred\"), \"pred\")) |&gt; unnest_wider(q) |&gt;\n  ggplot(aes(date_trunc, value, ymin=pred_lo, ymax=pred_hi)) +\n    geom_line() + geom_ribbon(alpha=0.3) + geom_vline(xintercept = tdate) + facet_wrap(~area, nrow=3)\n\n\n\n\n\n\n\n\nInterestingly, it seems from the plot above like the treatment effect decreases with time: initial PM 2.5 levels dropped in 2025, but now they seem almost level with our counterfactual predictions. We’ll come back to this idea of time-varying treatment effects.\nFor now, let’s try to calculate the posterior on the treatment effect itself. Because we’re modeling the square root of the PM 2.5 concentration rather than the concentration itself, we can’t just look at posterior of the treated coefficient. Instead, we have to manually calculate the difference between actual observations and couterfactual predictions.\n\npred_data |&gt;\n  filter(treated_crz) |&gt;\n  mutate(across(starts_with(\"pred\"), \\(x) value - x)) |&gt;\n  summarize(across(starts_with(\"pred\"), mean)) |&gt; \n  reframe(effect=c_across(starts_with(\"pred\"))) |&gt; select(effect) |&gt;\n  ggplot(aes(effect)) + geom_histogram()"
  },
  {
    "objectID": "posts/likelihood_ratios.html",
    "href": "posts/likelihood_ratios.html",
    "title": "Fun with Likelihood Ratios",
    "section": "",
    "text": "Say you’re trying to maximize a likelihood \\(p_{\\theta}(x)\\), but you only have an unnormalized version \\(\\hat{p_{\\theta}}\\) for which \\(p_{\\theta}(x) = \\frac{\\hat{p_\\theta}(x)}{N_\\theta}\\). How do you pick \\(\\theta\\)? Well, you can rely on the magic of self normalized importance sampling.\n\\[\n\\int \\hat{p_{\\theta}}(x)dx = N_\\theta \\\\\n\\int \\frac{q(x)}{q(x)} \\hat{p_{\\theta}}(x)dx = N_\\theta \\\\\nE_{q(x)}\\frac{\\hat{p_{\\theta}}(x)}{q(x)}=N_\\theta\n\\]\nTake a Monte Carlo estimate of the expectation, and you’re good to go. Specifically, you can maximize\n\\[\n\\log \\frac{\\hat{p_{\\theta}}(x)}{N_\\theta} = \\log \\hat{p_{\\theta}}(x) - \\log E_{q(x)}\\frac{\\hat{p_{\\theta}}(x)}{q(x)}\n\\]\nA special case is when \\(q(x)\\) is uniform, where this simplifies to \\(\\log \\hat{p_{\\theta}}(x) - b\\log E_{q(x)}\\hat{p_{\\theta}}(x)\\) for constant \\(b\\). This is just the negative sampling rule from Mikolov’s famous skip-gram paper!"
  },
  {
    "objectID": "posts/likelihood_ratios.html#maximizing-implicit-likelihoods",
    "href": "posts/likelihood_ratios.html#maximizing-implicit-likelihoods",
    "title": "Fun with Likelihood Ratios",
    "section": "Maximizing Implicit Likelihoods",
    "text": "Maximizing Implicit Likelihoods\nOkay, that’s cool, but what if we don’t even have an unnormalized \\(\\hat{p_{\\theta}}(x)\\) and \\(q(x)\\)? What if we just had a simulator \\(q(x)\\) that can spit out samples, but doesn’t know anything about densities?\nWe’d like to minimize the KL divergence of between whatever density our sampler \\(q\\) is capturing and the true data distribution \\(p\\).\n\\[\n-E_{q(x)}\\log \\frac{p(x)}{q(x)}\n\\]\nWell, that expression looks familiar. Once again, we need to maximize a likelihood ratio! Only this time, we can’t evaluate \\(q(x)\\). Instead, we can notice that \\(\\log p(x)/q(x)\\) is just the log odds of a sample \\(x\\) coming from \\(p\\) rather than \\(q\\). And estimating the log odds of some event occurring is easy: just build a discriminative binary classifier! Specifically, let \\(u(x)=p(x)\\) if \\(y=1\\) and \\(u(x)=q(x)\\) if \\(y=0\\). Then\n\\[\n\\begin{align*}\n\\frac{p(x)}{q(x)} &= \\frac{u(x \\vert y=1)}{u(x \\vert y=0)} \\\\\n&= \\frac{u(y=1, x)}{u(y=0, x)} \\frac{u(y=0)}{u(y=1)} \\\\\n&= \\frac{u(y=1 \\vert x)}{u(y=0 \\vert x)} \\frac{u(y=0)}{u(y=1)}\n\\end{align*}\n\\]\nSo now we have two objectives to minimize: the one for the classifier \\(u(y=1 \\vert x)\\) and the one for the implicit model \\(q(x)\\). That’s just a GAN!"
  },
  {
    "objectID": "posts/sparse_variational_gp/Sparse-Variational-Gaussian-Processes.html",
    "href": "posts/sparse_variational_gp/Sparse-Variational-Gaussian-Processes.html",
    "title": "Sparse Variational Gaussian Processes",
    "section": "",
    "text": "This notebook introduces Fully Independent Training Conditional (FITC) sparse variational Gaussian process model. You shouldn’t need any prior knowledge about Gaussian processes- it’s enough to know how to condition and marginalize finite dimensional Gaussian distributions. I’ll assume you know about variational inference and Pyro, though.\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom pyro.infer import SVI, Trace_ELBO, Predictive\nfrom torch.distributions.transforms import LowerCholeskyTransform\nimport gpytorch as gp\nSay we observe some data \\(x_1, x_2, \\dotsc\\).\nxs = torch.linspace(-10, 10, 50)\nAssume there’s an unknown function \\(f\\) that maps each data point \\(x_i\\) to an unknown value \\(f_i\\).\nfs = 2*torch.sin(xs)\nAnd each \\(f_i\\) is associated with an observed noisy version \\(y_i\\).\nys = fs + 0.4*torch.randn(xs.shape)\nplt.plot(xs, ys);\nSay we have some additional inputs \\(x_1^\\ast, x_2^\\ast, \\dotsc\\) and we want to estimate the associated \\(f^\\ast_1, f^\\ast_2, \\dotsc\\). We’ll assume that the \\(f_i\\) and \\(f^\\ast\\), along with a latent vector \\(u\\) of outouts at known inputs \\(z\\), are all jointly Gaussian. The \\(u_i\\) are known as inducing points. We’ll ensure that the conditional covariance structure is sparse, however: \\(f_i\\) will be conditionally independent given \\(u\\). This will keep the computation of the posterior tractable, even when we have a large number of training points \\(f\\).\nSpecifically, say \\[\n\\begin{bmatrix} u \\\\ f \\\\ f^* \\end{bmatrix} \\sim \\mathcal{N}\\left(0, \\begin{bmatrix} K_{uu}  & K_{uf} & K_{u*} \\\\ K_{fu} & D_{ff} & K_{fu}K_{uu}^{-1}K_{u*} \\\\ K_{*u} & K_{*u}K_{uu}^{-1}K_{uf} & K_{**} \\end{bmatrix} \\right)\n\\]\nThe expressions for conditionally independent covariances keep popping up, so We’ll abbreviate \\(K_{au}K_{uu}^{-1}K_{ub}\\) as \\(Q_{ab}\\). Using the standard Gaussian conditioning formula, we find that\n\\[\n\\begin{bmatrix} f \\\\ f^* \\end{bmatrix} \\,  \\bigg \\vert \\, u \\sim \\mathcal{N} \\left(\n\\begin{bmatrix} K_{fu}K_{uu}^{-1}u \\\\ K_{*u}K_{uu}^{-1}u \\end{bmatrix}\n, \\begin{bmatrix}  D_{ff} - Q_{ff} & Q_{f*} \\\\ Q_{*f} & K_{**} - Q_{**}\\end{bmatrix} \\right)\n\\]\nWe’ll choose \\(D_{ff}\\) so that \\(D_{ff} - Q_{ff}\\) is diagonal. Specifically, we’ll let \\(D_{ff} = Q_{ff} + \\text{Diag}(I - Q_{ff})\\).\nIt remains to choose the dense covariances \\(K_{uu}\\). We’ll choose a covariance structure that makes \\(u_i\\) and \\(u_j\\) close when \\(z_i\\) and \\(z_j\\) are.\ndef kernel(a,b):\n    return torch.exp(-0.5*((a[:,None] - b[None,:])/2)**2)\nz = torch.linspace(-8, 8, 10)\nk_uu = kernel(z, z)\nk_uu_chol = torch.linalg.cholesky(k_uu)\nk_uu_inv = torch.cholesky_inverse(k_uu_chol)\nk_fu = kernel(xs, z)\nk_ff_given_u = torch.diag(torch.eye(fs.shape[0]) - (k_fu @ k_uu_inv @ k_fu.T)) + 1e-5\nconditioner = k_fu @ k_uu_inv\nThis gives us a fully generative prior for the function values \\(f\\) and inducing points \\(u\\).\ndef model(obs):\n    u = pyro.sample(\"u\", dist.MultivariateNormal(torch.zeros(k_uu_inv.shape[0]), precision_matrix= k_uu_inv))\n    with pyro.plate(\"data\"):\n        f = pyro.sample(\"f\", dist.Normal(conditioner @ u, k_ff_given_u))\n        return pyro.sample(\"obs\", dist.Normal(f, 0.16), obs=obs)\nWe’ll assume that the posterior over \\(u\\) given our observations \\(y\\) is Gaussian as well.\nlower_cholesky = LowerCholeskyTransform()\ndef guide(obs):\n    M = k_uu_inv.shape[0]\n    m = pyro.param(\"m\", torch.randn(M))\n    S = lower_cholesky(pyro.param(\"S\", k_uu_chol))\n    return pyro.sample(\"u\", dist.MultivariateNormal(m, scale_tril=S))\nThis guide only covers \\(u\\), not \\(f\\). The conditional distribution of \\(f\\) given \\(u\\) will be the same as in the prior because it’s independent of \\(y\\). To let the model know that the associated guide has the same conditional distribution for \\(f\\), we use Pyro’s block function. As \\(y\\) here is Normally distributed about \\(f\\), we could analytically marginalize out \\(f\\). But we’ll keep things simple and use samples of \\(f\\) instead.\nmarginalized_model = poutine.block(model, hide=\"f\")"
  },
  {
    "objectID": "posts/sparse_variational_gp/Sparse-Variational-Gaussian-Processes.html#training",
    "href": "posts/sparse_variational_gp/Sparse-Variational-Gaussian-Processes.html#training",
    "title": "Sparse Variational Gaussian Processes",
    "section": "Training",
    "text": "Training\nWe can fit the parameters in our variational approximation to maximize the ELBO using a standard Pyro training loop.\n\nadam = pyro.optim.Adam({\"lr\": 0.03})\nsvi = SVI(marginalized_model, guide, adam, loss=Trace_ELBO())\npyro.clear_param_store()\nfor j in range(1500):\n    loss = svi.step(ys)\n    if j % 100 == 0:\n        print(loss)\n\n7876.168869018555\n1096.6914720535278\n678.0399866104126\n410.22321701049805\n347.35849380493164\n295.4183578491211\n338.96053409576416\n299.43280029296875\n245.88098907470703\n397.6497116088867\n272.0316352844238\n280.7838191986084\n233.17931365966797\n235.1186876296997\n250.67036628723145\n\n\n\npred = Predictive(model, guide=guide, num_samples=100)\nsamples = pred(None)['f'].numpy()\nplt.figure(figsize=(15,10))\nplt.plot(xs, samples.T, alpha=0.1);\nplt.scatter(z, pyro.param(\"m\").detach().numpy())\nplt.scatter(xs.numpy(), ys.numpy());"
  }
]