<!DOCTYPE html>
<html lang="en">
        <head>
                        <meta charset="utf-8" />
                        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
                        <meta name="generator" content="Pelican" />
                        <title>Finite Particle Approximations</title>
                        <link rel="stylesheet" href="/theme/css/main.css" />
    <meta name="description" content="Say you have a discrete distribution \(\pi\) that you want to approximate with a small number of weighted particles. Intuitively, it seems like..." />
        </head>

        <body id="index" class="home">
                <header id="banner" class="body">
                        <h1><a href="/">Sam's Blog</a></h1>
                        <nav><ul>
                                                <li><a href="/category/algorithms.html">algorithms</a></li>
                                                <li class="active"><a href="/category/machine_learning.html">machine_learning</a></li>
                                                <li><a href="/category/math.html">math</a></li>
                                                <li><a href="/category/slam.html">slam</a></li>
                                                <li><a href="/category/statistics.html">statistics</a></li>
                                                <li><a href="/category/tools.html">tools</a></li>
                        </ul></nav>
                </header><!-- /#banner -->
  <section id="content" class="body">
    <article>
      <header>
        <h1 class="entry-title">
          <a href="/finite-particle-approximations.html" rel="bookmark"
             title="Permalink to Finite Particle Approximations">Finite Particle Approximations</a></h1>
      </header>

      <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-04-02T00:00:00-05:00">
                Published: Tue 02 April 2024
        </abbr>

                <address class="vcard author">
                        By                                 <a class="url fn" href="/author/sam-anklesaria.html">Sam Anklesaria</a>
                </address>
        <p>In <a href="/category/machine_learning.html">machine_learning</a>.</p>
        
</footer><!-- /.post-info -->        <p>Say you have a discrete distribution <span class="math">\(\pi\)</span> that you want to approximate with a small number of weighted particles. Intuitively, it seems like the the best choice of particles would be the outputs of highest probability under <span class="math">\(\pi\)</span>, and that the relative weights of these particles should be the same under our approximation as they were under <span class="math">\(\pi\)</span>. This actually isn’t hard to prove!</p>
<h3>Minimizing the KL Divergence</h3>
<p>Let <span class="math">\(q\)</span> be our approximation: a version of <span class="math">\(\pi\)</span> with support restricted to <span class="math">\(b\)</span> outcomes. We’ll try to minimize <span class="math">\(KL[q, \pi]\)</span>. (Note that <span class="math">\(KL[\pi, q]\)</span> will always be infinite as we do not have <span class="math">\( \pi \ll q\)</span>, so using this distance isn't an option). Treat <span class="math">\(\pi\)</span> and <span class="math">\(q\)</span> as vectors in <span class="math">\(\mathbb{R}^n\)</span>, and assume without loss of generality that <span class="math">\(\pi_1 \geq \pi_2 \geq \pi_3 \dotsc\)</span>.</p>
<p><strong>Claim 1: <span class="math">\(KL[q, \pi]\)</span> is minimized when the nonzero components of <span class="math">\(q\)</span> have <span class="math">\(q \propto \pi\)</span>.</strong></p>
<p><em>Proof:</em>
Use Lagrange multipliers. Let <span class="math">\(S\)</span> be the support of <span class="math">\(q\)</span>.  <span class="math">\(L(\lambda, q) = \sum_{i \in S} q_i \log (q_i / \pi_i) + \lambda (1 - \langle q, 1 \rangle)\)</span>. The minimum will be at a fixed point of the Lagrangian. Differentiating, we find that <span class="math">\(\log (q_i/\pi_i) + (q_i/q_i) + \lambda = 0\)</span> for all <span class="math">\(i\)</span> in <span class="math">\(q\)</span>'s support, or <span class="math">\(\pi_i / q_i =e^{\lambda + 1}\)</span>. As this proportionality constant is the same for all <span class="math">\(i\)</span>, this confirms that minimizer is proportional to <span class="math">\(\pi\)</span> whenever it is nonzero. <span class="math">\(\square\)</span></p>
<p><strong>Claim 2: If <span class="math">\(q \propto m \odot \pi\)</span> where <span class="math">\(\odot\)</span> indicates point-wise multiplication and <span class="math">\(m\)</span> is a binary mask with <span class="math">\(\|m\|_0 = b\)</span> picking out the support of <span class="math">\(q\)</span>, then <span class="math">\(\arg \min KL[q, \pi]\)</span> is obtained when <span class="math">\(m_i = 1\)</span> when <span class="math">\(i \leq b\)</span> and <span class="math">\(m_i = 0\)</span> otherwise.</strong></p>
<p><em>Proof</em>:
Let <span class="math">\(P = m^T\pi\)</span>. The KL divergence simplifies as follows
</p>
<div class="math">$$
\begin{align*}
KL[q, \pi] &amp;= \frac{1}{P}\sum_{i=1}^n \pi_i m_i \left( \log (\pi_i / P) - \log \pi_i\right) \\
&amp;= -\log(P)
\end{align*}
$$</div>
<p>
 The <span class="math">\(m\)</span> maximizing <span class="math">\(m^T\pi\)</span> is clearly the one with its mass on the <span class="math">\(b\)</span> highest outcomes in <span class="math">\(\pi\)</span>. <span class="math">\(\square\)</span></p>
<h3>Minimizing Maximum Mean Discrepancy</h3>
<p>The KL divergence isn’t the only way to measure distance between distributions. In fact, it’s not a particularly flexible way, as it only lets us compare our discrete distribution <span class="math">\(q\)</span> with other discrete distributions <span class="math">\(\pi\)</span>. Instead, we can minimize the “maximum mean discrepancy” or MMD. The big idea is to choose <span class="math">\(q\)</span> to make <span class="math">\(E_{X \sim q}f(X)\)</span> as close as possible to <span class="math">\(E_{X \sim \pi} f(X)\)</span> for all functions <span class="math">\(f\)</span> in some space <span class="math">\(\mathcal{H}\)</span>.
</p>
<div class="math">$$
\text{MMD} = \sup_{f \in \mathcal{H}, \|f\| \leq 1} (E_{X \sim \pi} f(X) - E_{X \sim q} f(X))^2
$$</div>
<p>
If we assume that <span class="math">\(\mathcal{H}\)</span> is closed under negation, squaring this difference won't change the optimal distribution <span class="math">\(q^*\)</span> minimizing the expression, so we can equivalently write this as
</p>
<div class="math">$$
\sup_{f \in \mathcal{H}, \|f\| \leq 1} (E_{X \sim \pi} f(X) - E_{X \sim q} f(X))
$$</div>
<p>
If we let <span class="math">\(\mathcal{H}\)</span> be a <a href="https://web.archive.org/web/https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space</a> with kernel <span class="math">\(k\)</span>, function evaluation <span class="math">\(f(x)\)</span> is the same as taking the inner product <span class="math">\(\langle f, k(x, \cdot) \rangle\)</span>, and we can write the result as
</p>
<div class="math">$$
\sup_{f \in \mathcal{H}, \|f\| \leq 1} (E_{X \sim \pi} \langle f, k(X, \cdot) \rangle - E_{X \sim q} \langle f, k(X, \cdot) \rangle)
$$</div>
<p>
You can read more about kernel mean embeddings like this <a href="https://web.archive.org/web/https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions">here</a>.</p>
<p>Let's assume that <span class="math">\(\pi\)</span> is discrete, and we can use <span class="math">\(\mathbb{R}^n\)</span> as this Hilbert space.
Letting <span class="math">\(\pi\)</span> and <span class="math">\(q\)</span> also indicate vectors of component probabilities, the expression simplifies as
</p>
<div class="math">$$
\sup_{f \in \mathbb{R}^n, \|f\| \leq 1} \langle f, E_{X \sim \pi}[X], - E_{X \sim q}[X] \rangle = \|\pi - q\|_2^2 \\
$$</div>
<p>
Our problem, therefore, is to project <span class="math">\(\pi\)</span> onto the space of all <span class="math">\(q\)</span> for which <span class="math">\(\|q\|_1 = 1\)</span> (the sum of components is 1) and <span class="math">\(\|q\|_0 = b\)</span> (the total number of non-zero components is <span class="math">\(b\)</span>).</p>
<p>This space is not convex, but we can get around this by fixing a binary mask vector <span class="math">\(m\)</span> with 1-norm <span class="math">\(b\)</span> and minimizing <span class="math">\(\|\pi - m \odot q\|_2^2\)</span> over all unit <span class="math">\(q\)</span>, where <span class="math">\(\odot\)</span> indicates the Hadamard product. This gives us the Lagrangian
</p>
<div class="math">$$
L(\lambda, q) = \|m \odot q\|^2 - 2 \langle m \odot \pi, q \rangle + \lambda (\langle q, m \rangle - 1).
$$</div>
<p>
The derivative with respect to <span class="math">\(q_i\)</span> is zero when <span class="math">\(2m_i q_i - 2m_i \pi_i + \lambda m_i = 0\)</span> from which we can conclude that <span class="math">\(q_i = \pi_i + c\)</span> for some normalizing constant <span class="math">\(c\)</span> shared by all components in the support.</p>
<p>We can plug in this result about the optimal <span class="math">\(q_i\)</span> and solve to find the optimal <span class="math">\(m_i\)</span>.
Let <span class="math">\(S\)</span> be the support of <span class="math">\(q\)</span> we get from <span class="math">\(m\)</span> and <span class="math">\(1\)</span> be the all ones vector. We want to minimize
</p>
<div class="math">$$
\begin{align*}
\text{MMD} &amp;= \|m\pi + \frac{\langle 1-m, \pi \rangle}{b}m - \pi\|^2 \\
&amp;= \sum_i \left( 1_{i \notin S} \pi_i + 1_{i \in S}\frac{\langle 1-m, \pi \rangle}{b}\right)^2 \\
&amp;= (1 - b) + \sum_{i \notin S} \pi_i^2
\end{align*}
$$</div>
<p>
Clearly, this quantity is minimized by choosing <span class="math">\(S\)</span> to contain the <span class="math">\(b\)</span> most likely components of <span class="math">\(\pi\)</span>.</p>
<h3>Unbiased MMD Minimization</h3>
<p>The approximation schemes we've looked at so far have been deterministic: to make a <span class="math">\(b\)</span> particle approximation <span class="math">\(q\)</span> to a more complicated distribution <span class="math">\(\pi\)</span>, we always choose <span class="math">\(q\)</span>'s support <span class="math">\(S\)</span> to be the <span class="math">\(b\)</span> most likely elements in <span class="math">\(\pi\)</span>.
But because of this determinism, these approximations are biased. By <em>biased</em> here, I mean that it is not generally the case that <span class="math">\(E[\sum_{i=1}^n q_i f(x_i)] \neq E_{X \sim \pi} f(X)\)</span> for any function <span class="math">\(f\)</span> unless <span class="math">\(b\)</span> is large enough to capture the full support of <span class="math">\(\pi\)</span>.</p>
<p>If this property (unbiased-ness) was more important to us than truly minimizing the maximum mean discrepancy, we could instead try to choose <span class="math">\(S\)</span> stochastically. Specifically, we could do the following:
- Assign each outcome <span class="math">\(i\)</span> in <span class="math">\(\pi\)</span>'s support a weight <span class="math">\(w_i\)</span>.
- Sample an outcome with probability proportional to the remaining weights.
- Put that outcome in <span class="math">\(S\)</span> and prevent it from being sampled again.
- Repeat <span class="math">\(b\)</span> times.</p>
<p>With this scheme, the number of times outcome <span class="math">\(i\)</span> is included in <span class="math">\(S\)</span> is given by <a href="https://web.archive.org/web/https://en.wikipedia.org/wiki/Wallenius%27_noncentral_hypergeometric_distribution">Wallenius' non-central hypergeometric distribution</a> with success-outcomes parameter <span class="math">\(m_1 =1\)</span>, total outcomes parameter <span class="math">\(N\)</span> being the size of <span class="math">\(\pi\)</span>'s support, draws parameter <span class="math">\(n=b\)</span>, and odds parameter <span class="math">\(\theta_i = \frac{(n-1)w_i}{1 - w_i}\)</span>.
If we know the odds parameter <span class="math">\(\theta_i\)</span>, we can get back <span class="math">\(w_i\)</span> using <span class="math">\(\sigma(\log(\theta_i / (n-1)))\)</span> where <span class="math">\(\sigma\)</span> is the standard logistic function.</p>
<p>Once we choose the support set <span class="math">\(S\)</span>, we can deterministically assign probabilities to each outcome in <span class="math">\(q\)</span> to minimize the maximum mean discrepancy from our target distribution <span class="math">\(\pi\)</span>.
As shown above, the optimal choice is to set <span class="math">\(q_i = \pi_i + c\)</span> where <span class="math">\(c\)</span> is the equally distributed un-allocated probability mass <span class="math">\(\frac{1}{b} \sum_{j \notin S} \pi_j\)</span>.</p>
<p>Now, we can choose the  <span class="math">\(w_i\)</span> for each outcome in such a way that <span class="math">\(\pi_i = E[q_i]\)</span>, making <span class="math">\(q\)</span> <em>unbiased</em> unlike the deterministic approximation discussed earlier. Let <span class="math">\(s_i = P(i \in S)\)</span> and <span class="math">\(r_i = 1 - s_i\)</span>.
</p>
<div class="math">$$
\begin{align*}
\pi_i &amp;= E[1_{i \in S}(\pi_i + \frac{1}{b}\sum_j 1_{j \notin S} P_j)] \\
&amp;= s_i(\pi_i + \frac{1}{b}\sum_j \pi_j (1 - s_j)) \\
b(1-s_i)\pi_i &amp;= \sum_j \pi_j (1 - s_j) \\
(b - 1)r_i \pi_i &amp;= \sum_{j \neq i} \pi_j r_i
\end{align*}
$$</div>
<p>
This holds for all <span class="math">\(i\)</span> simultaneously, so letting <span class="math">\(P\)</span> be the matrix with <span class="math">\(\pi\)</span> along its diagonal and <span class="math">\(1\)</span> be the all ones matrix, we can write the above equation in vectorized form as
</p>
<div class="math">$$
\begin{align*}
(b-1)Pr &amp;= (1-I)Pr \\
((b-1)I - 1 + I)Pr &amp;= 0 \\
(bI - 1)Pr &amp;= 0 \\
\end{align*}
$$</div>
<p>
We can see that if we find an eigenvector of <span class="math">\((bI - 1)P\)</span> with eigenvalue zero, the entries will indicate the probability mass functions of our our non-central hypergeometric distributions at zero.</p>
<p>However, as <span class="math">\((bI - 1)\)</span> is nonsingular for nonzero <span class="math">\(b\)</span>, no such eigenvalue exists!
This means this whole approach is actually impossible.  <strong>We can't randomly choose a support and then optimally pick the weights if we want the result to be unbiased.</strong> We can either choose optimal weights, or we can have an unbiased approximation. Having both at the same time is fundamentally impossible.</p>
<h3>Quasi Monte Carlo Approximation</h3>
<p>If we'd prefer to have unbiased-ness to optimal weights, there are other finite particle approximation strategies we could use besides standard Monte Carlo.
One simple approach is to use randomized lattice <a href="https://web.archive.org/web/https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method">Quasi Monte Carlo</a>. The basic idea goes like this:
- Let <span class="math">\(u_i = i/b\)</span> for <span class="math">\(b\)</span> different particles.
- Choose a random number <span class="math">\(s\)</span> between 0 and 1 and add it to all the <span class="math">\(u_i\)</span> mod 1. These are unbiased, evenly dispersed samples from a uniform distribution.
- Now apply the inverse CDF <span class="math">\(\Phi\)</span> of <span class="math">\(\pi\)</span> to each of these samples. Here, we're assuming an ordering among the possible outcomes; if <span class="math">\(\Phi(0.3) = 3\)</span>, for example, that would mean that 30% of <span class="math">\(\pi\)</span>'s probability mass is for outcomes below 3. This is known as inverse transform sampling, and guarantees we'll get unbiased samples from <span class="math">\(\pi\)</span>! We'll set all the weights to be <span class="math">\(1/b\)</span>, just as in the standard Monte Carlo case.</p>
<p>Some connections to think about: when the <span class="math">\(\pi\)</span> we're trying to approximate is a the empirical distribution of particles during a round of sequential Monte Carlo, this approximation is known as <em>systematic resampling</em>.
If, as is often the case in probabilistic programming, we're not trying to sample from a single categorical distribution <span class="math">\(\pi\)</span>, but rather a sequence of dependent distributions <span class="math">\(\pi^1, \pi^2, \dotsc\)</span>, we can sample our uniform grid in the unit hypercube rather than along the unit interval and apply the inverse CDF of <span class="math">\(\pi^i\)</span> to the <span class="math">\(i\)</span>th coordinates.
In multiple dimensions, however, a uniform grid might not be the best idea. If we try to pick the lattice structure in a way that minimizes MMD between <span class="math">\(q\)</span> and <span class="math">\(\pi\)</span>, the best choice of lattice will depend on the class of functions <span class="math">\(\mathcal{H}\)</span> we're considering for the MMD. For example, one common RKHS for multidimensional spaces has a kernel that is just the product of the kernels for each dimension. If the kernel for one of the dimensions has a lower bandwidth, it will make sense to pack our grid points more tightly in this dimension.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
      </div><!-- /.entry-content -->

    </article>
  </section>
                <section id="extras" class="body">
                </section><!-- /#extras -->

                <footer id="contentinfo" class="body">
                        <address id="about" class="vcard body">
                                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                        </address><!-- /#about -->

                        <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
                </footer><!-- /#contentinfo -->

        </body>
</html>