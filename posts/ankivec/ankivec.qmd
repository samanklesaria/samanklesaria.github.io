---
title: "AnkiVec: Vector Search for Anki"
date: 11/20/2025
categories: tools
---

I just released [AnkiVec](https://ankiweb.net/shared/info/2028823832?cb=1763666137047), an Anki addon that creates vector embeddings for cards using Ollama and enables hybrid semantic search with ChromaDB.
![screenshot](screenshot.png)

## Features

- **Vector Embeddings**: Generate embeddings for all cards using local Ollama models
- **Semantic Search**: Find cards by meaning, not just keywords
- **Fast Local Processing**: Uses lightweight embedding models (nomic-embed-text by default)
- **Persistent Storage**: ChromaDB stores embeddings for quick retrieval


## How to Use It

When you restart Anki after installing the add-on, a vector-database will be initialized for your deck. This process may take a few minute- you should see a loading dialog with a progress bar. From this point on, any changes to your deck will be automatically indexed.

To search the vector database, use the form "vec: [your query here]" in Anki's usual search bar. You can combine keyword searches with vector searches by putting the "vec" section at the end of the query: e.g. "keyword1 keyword2 vec: [natural language description]".

## Why ChromaDB?

I started trying to use [sqlite-vec](https://alexgarcia.xyz/sqlite-vec/installation.html), modifying Anki's internal sqlite database. Unfortunately, Anki doesn't like other processes modifying its database, and frequently warns mistakenly about corruption issues. Eventually, I figured it was easiest to keep the embedding database separate, and switched to ChromaDB.

## Why Ollama?

If I'm already using ChromaDB, why use Ollama? ChromaDB has its own embedding models built in! Unfortunately, they're super slow compared to Ollama's. For the `all-minilm` model that ChromaDB uses by default, Ollama's version is about 2.72x faster on my Mac. Even if you jump through the necessary hoops to get MPS acceleration for ChromaDB, Ollama is still over twice as fast. Yes, this makes installation a bit more complicated, but it's worth it, and makes it easier to swap in other embedding choices. You can run the benchmarks yourself from the [repository](https://github.com/samanklesaria/ankivec).

## How do dependencies get installed?

Anki's launcher uses a bundled version of `uv` to download its dependencies. I piggybacked on this to install my own dependencies (ollama and chromadb).

- When the addon is initialized, it checks where Anki's copy of `uv` is located. On a Mac, this is within the application bundle itself.
- Next, it `uv sync`s its own directory using this bundled `uv` instance.
- Finally, it adds the addon directory to the Python path before importing any modules.

This avoids the usual headaches of vendoring in all the transitive dependencies of an Anki addon. Just let `uv` handle it for you!
