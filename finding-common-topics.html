<!DOCTYPE html>
<html lang="en">
        <head>
                        <meta charset="utf-8" />
                        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
                        <meta name="generator" content="Pelican" />
                        <title>Finding Common Topics</title>
                        <link rel="stylesheet" href="/theme/css/main.css" />
    <meta name="description" content="How do you find thematic clusters in a large corpus of text documents? The techniques baked into sklearn (e.g. nonnegative matrix factorization,..." />
        </head>

        <body id="index" class="home">
                <header id="banner" class="body">
                        <h1><a href="/">Sam's Blog</a></h1>
                        <nav><ul>
                                                <li><a href="/category/algorithms.html">algorithms</a></li>
                                                <li class="active"><a href="/category/machine_learning.html">machine_learning</a></li>
                                                <li><a href="/category/math.html">math</a></li>
                                                <li><a href="/category/slam.html">slam</a></li>
                                                <li><a href="/category/statistics.html">statistics</a></li>
                                                <li><a href="/category/tools.html">tools</a></li>
                        </ul></nav>
                </header><!-- /#banner -->
  <section id="content" class="body">
    <article>
      <header>
        <h1 class="entry-title">
          <a href="/finding-common-topics.html" rel="bookmark"
             title="Permalink to Finding Common Topics">Finding Common Topics</a></h1>
      </header>

      <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2025-02-10T00:00:00-06:00">
                Published: Mon 10 February 2025
        </abbr>

                <address class="vcard author">
                        By                                 <a class="url fn" href="/author/sam-anklesaria.html">Sam Anklesaria</a>
                </address>
        <p>In <a href="/category/machine_learning.html">machine_learning</a>.</p>
        
</footer><!-- /.post-info -->        <p>How do you find thematic clusters in a large corpus of text documents? The techniques baked into <code>sklearn</code> (e.g. nonnegative matrix factorization, LDA) give you some intuition about common themes. But contemporary NLP has largely moved on from bag-of-words representations. We can do better with some transformer models!</p>
<p>For demonstration purposes, I'll use a few categories from the standard 20-newsgroups dataset. Ideally, we should be able to recover the four categories in the dataset (atheism, computer graphics, space and religion). </p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;alt.atheism&quot;</span><span class="p">,</span>
    <span class="s2">&quot;talk.religion.misc&quot;</span><span class="p">,</span>
    <span class="s2">&quot;comp.graphics&quot;</span><span class="p">,</span>
    <span class="s2">&quot;sci.space&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span>
    <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;headers&quot;</span><span class="p">,</span> <span class="s2">&quot;footers&quot;</span><span class="p">,</span> <span class="s2">&quot;quotes&quot;</span><span class="p">),</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<p>Some of the documents in the dataset are only a few words; I only want to deal with documents that are least a couple hundred characters. </p>
<div class="codehilite"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">200</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">))))</span>
</code></pre></div>

<p>First, I'll map each document to its embedding using the <em>all-MiniLM</em> BERT variant.</p>
<div class="codehilite"><pre><span></span><code><span class="n">minilm_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span><span class="p">)</span>
<span class="n">minilm</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">mean_pooling</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
    <span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#First element contains all embeddings</span>
    <span class="n">input_mask_expanded</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
        <span class="n">token_embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">token_embeddings</span> <span class="o">*</span> <span class="n">input_mask_expanded</span><span class="p">,</span>
                     <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">input_mask_expanded</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">toks</span> <span class="o">=</span> <span class="n">minilm_tokenizer</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">model_output</span> <span class="o">=</span> <span class="n">minilm</span><span class="p">(</span><span class="o">**</span><span class="n">toks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">))</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">mean_pooling</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span>
                                              <span class="n">toks</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]),</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</code></pre></div>

<p>Next, I'll cluster the embeddings with the standard k-means algorithm. There's far more sophisticated clustering techniques in <code>sklearn</code>, but this should be sufficient for the toy problem.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_clusters</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>
    <span class="n">neural_kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
    <span class="n">neural_kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">docs_per_label</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="n">neural_kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">})</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">neural_kmeans</span><span class="p">,</span> <span class="n">docs_per_label</span>
</code></pre></div>

<p>Finally, I'll take a random set of documents closest to the center of each cluster and ask Llama to find a title for the collection.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">top_per_cluster</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">kmeans</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
        <span class="n">X</span><span class="p">[</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="n">i</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(((</span><span class="n">embeddings</span><span class="p">[</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
                                                <span class="o">-</span> <span class="n">c</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))[:</span><span class="n">k</span><span class="p">]],</span> <span class="n">m</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">llama</span> <span class="o">=</span> <span class="n">ChatOllama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<p>I'll let the LLM contemplate common themes to itself before deciding on a title. We can require that the results get packaged together in a structured output format. </p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">SampleAnalysis</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">analysis</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Analysis of the texts.&#39;</span><span class="p">)</span>
    <span class="n">category</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Category of the cluster.&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">llama_summarize</span><span class="p">(</span><span class="n">strs</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Your task is to understand why the given documents were assigned to the same cluster.</span>
<span class="s2">- First analyze the documents in the cluster for common topics.</span>
<span class="s2">- Then, propose a category for the cluster containing these documents based on the analysis.&quot;&quot;&quot;</span><span class="p">)]</span>
    <span class="n">prompt</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">strs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">llama</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">SampleAnalysis</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div>

<p>Let's try it out!</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_topics</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">neural_kmeans</span><span class="p">,</span> <span class="n">docs_per_label</span> <span class="o">=</span> <span class="n">get_clusters</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">top_embeddings</span> <span class="o">=</span> <span class="n">top_per_cluster</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">neural_kmeans</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">llama_summarize</span><span class="p">([</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">t</span><span class="p">])</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">top_embeddings</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;category&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">category</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span>
        <span class="s1">&#39;n_docs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">docs_per_label</span><span class="p">[</span><span class="n">a</span><span class="p">])</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">docs_per_label</span><span class="p">))]</span>
    <span class="p">})</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;n_docs&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Markdown</span><span class="p">(</span><span class="n">get_topics</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">())</span>
</code></pre></div>

<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: left;">category</th>
<th style="text-align: right;">n_docs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">0</td>
<td style="text-align: left;">Computer Graphics</td>
<td style="text-align: right;">706</td>
</tr>
<tr>
<td style="text-align: right;">3</td>
<td style="text-align: left;">Space Exploration and Development</td>
<td style="text-align: right;">706</td>
</tr>
<tr>
<td style="text-align: right;">2</td>
<td style="text-align: left;">Debates about the existence of God and the nature of human reason, with a focus on criticizing Christian beliefs and practices.</td>
<td style="text-align: right;">632</td>
</tr>
<tr>
<td style="text-align: right;">1</td>
<td style="text-align: left;">Social Commentary/Philosophy</td>
<td style="text-align: right;">613</td>
</tr>
</tbody>
</table>
<p>Sounds about right!</p>
      </div><!-- /.entry-content -->

    </article>
  </section>
                <section id="extras" class="body">
                </section><!-- /#extras -->

                <footer id="contentinfo" class="body">
                        <address id="about" class="vcard body">
                                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                        </address><!-- /#about -->

                        <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
                </footer><!-- /#contentinfo -->

        </body>
</html>