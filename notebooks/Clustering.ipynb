{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fb5045-9f53-4033-8c74-cef96e2f0bb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "How do you find thematic clusters in a large corpus of text documents? The techniques baked into `sklearn` (e.g. nonnegative matrix factorization, LDA) give you some intuition about common themes. But contemporary NLP has largely moved on from bag-of-words representations. We can do better with some transformer models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866ae7d-1f8d-4a9f-a71c-80f4bcaef9f2",
   "metadata": {},
   "source": [
    "For demonstration purposes, I'll use a few categories from the standard 20-newsgroups dataset. Ideally, we should be able to recover the four categories in the dataset (atheism, computer graphics, space and religion). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d9871d-4fb1-4b2d-81a6-7db00b99b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import KMeans\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42523a0-c3ad-4141-809a-5c5a4488010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737efbeb-ae99-489a-95fd-c8d71cc5e1cf",
   "metadata": {},
   "source": [
    "Some of the documents in the dataset are only a few words; I only want to deal with documents that are least a couple hundred characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9408e814-575d-4a89-a75f-c5d21b12245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(list(filter(lambda x: len(x) > 200, (d.strip() for d in dataset.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90c220-7951-43e8-9e47-cd86ca6ced33",
   "metadata": {},
   "source": [
    "First, I'll map each document to its embedding using the *all-MiniLM* BERT variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d845e0-bbb8-4743-a07f-24b1a1e1c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "minilm = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5e1317-531a-4003-bedf-754c5ceaef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element contains all embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded,\n",
    "                     1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a96589-8d56-4b78-bd09-89a3a37862d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(X):\n",
    "    loader = DataLoader(X, batch_size=16)\n",
    "    embeddings = []\n",
    "    for batch in loader:\n",
    "        toks = minilm_tokenizer(batch, padding=True, truncation=True,\n",
    "                                return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = minilm(**toks.to('mps'))\n",
    "            result = F.normalize(mean_pooling(model_output,\n",
    "                                              toks['attention_mask']), p=2, dim=1)\n",
    "            embeddings.append(result.cpu())\n",
    "    return torch.cat(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948237e-54e9-4215-8e49-9adfa52fa5fe",
   "metadata": {},
   "source": [
    "Next, I'll cluster the embeddings with the standard k-means algorithm. There's far more sophisticated clustering techniques in `sklearn`, but this should be sufficient for the toy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ff45f9d-f752-4cef-8e2f-e1492484e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(embeddings):\n",
    "    neural_kmeans = KMeans(n_clusters=4, n_init=25)\n",
    "    neural_kmeans.fit(embeddings)\n",
    "    docs_per_label = pd.DataFrame({'labels': neural_kmeans.labels_}).value_counts()\n",
    "    return neural_kmeans, docs_per_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480cb30-62a9-4fa2-8471-72e7177b8332",
   "metadata": {},
   "source": [
    "Finally, I'll take a random set of documents closest to the center of each cluster and ask Llama to find a title for the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8fb5d3a8-2bb6-41b3-99d8-52f216e9b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_per_cluster(X, embeddings, kmeans, k=25, m=8):\n",
    "    return [np.random.choice(\n",
    "        X[kmeans.labels_ == i][np.argsort(((embeddings[kmeans.labels_ == i]\n",
    "                                                - c)**2).sum(axis=-1))[:k]], m)\n",
    "        for i, c in enumerate(kmeans.cluster_centers_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb6f569-d60d-4ac1-839a-c1db146a5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = ChatOllama(model=\"llama3\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472928e-1742-43f2-acc7-828af80a7f43",
   "metadata": {},
   "source": [
    "I'll let the LLM contemplate common themes to itself before deciding on a title. We can require that the results get packaged together in a structured output format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b076fb1b-2e31-47d4-aa1c-1a4655ae05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleAnalysis(BaseModel):\n",
    "    analysis: str = Field(description='Analysis of the texts.')\n",
    "    category: str = Field(description='Category of the cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5afe6cd-eac3-4650-95b5-5f3cf4a12b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_summarize(strs):\n",
    "    prompt = [SystemMessage(\"\"\"\n",
    "Your task is to understand why the given documents were assigned to the same cluster.\n",
    "- First analyze the documents in the cluster for common topics.\n",
    "- Then, propose a category for the cluster containing these documents based on the analysis.\"\"\")]\n",
    "    prompt.extend(strs)\n",
    "    return llama.with_structured_output(SampleAnalysis).invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080967b-3bb9-401a-a47d-766b99309de9",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c714e74-5a9c-4a14-aaac-e600870aa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(X):\n",
    "    embeddings = get_embeddings(X)\n",
    "    neural_kmeans, docs_per_label = get_clusters(embeddings)\n",
    "    top_embeddings = top_per_cluster(X, embeddings, neural_kmeans)\n",
    "    results = [llama_summarize([a for a in t]) for t in top_embeddings]\n",
    "    return pd.DataFrame({\n",
    "        'category': [r.category for r in results],\n",
    "        'n_docs': [int(docs_per_label[a]) for a in range(len(docs_per_label))]\n",
    "    }).sort_values(by='n_docs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9e0a939-2c93-49fb-a60c-3797f4074464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | category                                                                                                                        |   n_docs |\n",
       "|---:|:--------------------------------------------------------------------------------------------------------------------------------|---------:|\n",
       "|  0 | Computer Graphics                                                                                                               |      706 |\n",
       "|  3 | Space Exploration and Development                                                                                               |      706 |\n",
       "|  2 | Debates about the existence of God and the nature of human reason, with a focus on criticizing Christian beliefs and practices. |      632 |\n",
       "|  1 | Social Commentary/Philosophy                                                                                                    |      613 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(get_topics(X).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f082cc-1996-4b5e-8ddd-eb1fd21f4834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Sounds about right!"
   ]
  }
 ],
 "metadata": {
  "category": "machine_learning",
  "date": "02/10/25",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "title": "Finding Common Topics"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
